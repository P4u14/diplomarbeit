<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/SegmentationRunner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/SegmentationRunner.py" />
              <option name="originalContent" value="from atlas.refiner.color_patch_refiner import ColorPatchRefiner&#10;from atlas.selector.bmi_atlas_selector import BmiAtlasSelector&#10;from atlas.selector.similarity_atlas_selector import SimilarityAtlasSelector&#10;from atlas.voter.majority_voter import MajorityVoter&#10;from atlas.voter.weighted_majority_voter import WeightedMajorityVoter&#10;from preprocessing.blue_color_preprocessor import BlueColorPreprocessor&#10;from preprocessing.color_preprocessor import ColorPreprocessor&#10;from preprocessing.dimples_roi_preprocessor import DimplesRoiPreprocessor&#10;from preprocessing.torso_roi_preprocessor import TorsoRoiPreprocessor&#10;from segmenter.atlas_segmenter import AtlasSegmenter&#10;import time&#10;import os&#10;&#10;&#10;class AtlasSegmentationRunner:&#10;    def __init__(self, num_atlases_to_select, atlas_dir, preprocessing_steps, atlas_selector, segmentation_voter, segmentation_refiner, output_dir, target_images_dir):&#10;        self.segmenter = AtlasSegmenter(&#10;            num_atlases_to_select,&#10;            atlas_dir,&#10;            preprocessing_steps,&#10;            atlas_selector,&#10;            segmentation_voter,&#10;            segmentation_refiner,&#10;            output_dir&#10;        )&#10;        self.target_images_dir = target_images_dir&#10;&#10;    def run(self):&#10;        # start timing&#10;        start_time = time.time()&#10;        target_images = self.segmenter.load_target_images(self.target_images_dir)&#10;        segmented_images = self.segmenter.segment_images(target_images)&#10;        self.segmenter.save_segmentation(segmented_images)&#10;        # end timing and compute durations&#10;        end_time = time.time()&#10;        total_seconds = end_time - start_time&#10;        # format total duration h:m:s&#10;        hrs = int(total_seconds // 3600)&#10;        mins = int((total_seconds % 3600) // 60)&#10;        secs = int(total_seconds % 60)&#10;        duration_str = f&quot;{hrs:02d}:{mins:02d}:{secs:02d}&quot;&#10;        # average per image&#10;        num_images = len(target_images)&#10;        if num_images &gt; 0:&#10;            avg_seconds = total_seconds / num_images&#10;            avg_hrs = int(avg_seconds // 3600)&#10;            avg_mins = int((avg_seconds % 3600) // 60)&#10;            avg_secs = int(avg_seconds % 60)&#10;            avg_str = f&quot;{avg_hrs:02d}:{avg_mins:02d}:{avg_secs:02d}&quot;&#10;        else:&#10;            avg_str = &quot;00:00:00&quot;&#10;        # write durations to file in output_dir&#10;        duration_file = os.path.join(self.segmenter.output_dir, &quot;duration.txt&quot;)&#10;        with open(duration_file, &quot;w&quot;) as f:&#10;            f.write(f&quot;Total duration: {duration_str}\n&quot;)&#10;            f.write(f&quot;Average per image: {avg_str}\n&quot;)&#10;&#10;# Beispiel für die Ausführung:&#10;if __name__ == &quot;__main__&quot;:&#10;    # Hier müssen die passenden Objekte und Parameter übergeben werden&#10;    runner = AtlasSegmentationRunner(&#10;        num_atlases_to_select=13,&#10;        atlas_dir=&quot;data/Atlas_Data_BMI_Percentile&quot;,&#10;        # preprocessing_steps=[DimplesRoiPreprocessor(target_ratio=10/7) ,BlueColorPreprocessor()],  # Liste mit Preprocessing-Objekten&#10;        preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;        atlas_selector=BmiAtlasSelector(&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;, &quot;data/Info_Sheets/bmi_table_who.csv&quot;),      # AtlasSelector-Objekt&#10;        segmentation_voter=WeightedMajorityVoter(scheme=&quot;softmax&quot;, temperature=0.02, threshold=0.5),  # SegmentationVoter-Objekt&#10;        segmentation_refiner=ColorPatchRefiner(BlueColorPreprocessor()),&#10;        output_dir=&quot;data/Atlas_Experiment100&quot;,&#10;        target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    )&#10;    # runner = AtlasSegmentationRunner(&#10;    #     num_atlases_to_select=3,&#10;    #     atlas_dir=&quot;data/Atlas_Data&quot;,&#10;    #     preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;    #     atlas_selector=SimilarityAtlasSelector(),      # AtlasSelector-Objekt&#10;    #     segmentation_voter=MajorityVoter(),  # SegmentationVoter-Objekt&#10;    #     segmentation_refiner=None,&#10;    #     output_dir=&quot;data/Atlas_Experiment01&quot;,&#10;    #     target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    # )&#10;    runner.run()" />
              <option name="updatedContent" value="from atlas.refiner.color_patch_refiner import ColorPatchRefiner&#10;from atlas.selector.bmi_atlas_selector import BmiAtlasSelector&#10;from atlas.selector.similarity_atlas_selector import SimilarityAtlasSelector&#10;from atlas.voter.majority_voter import MajorityVoter&#10;from atlas.voter.weighted_majority_voter import WeightedMajorityVoter&#10;from preprocessing.blue_color_preprocessor import BlueColorPreprocessor&#10;from preprocessing.color_preprocessor import ColorPreprocessor&#10;from preprocessing.dimples_roi_preprocessor import DimplesRoiPreprocessor&#10;from preprocessing.torso_roi_preprocessor import TorsoRoiPreprocessor&#10;from segmenter.atlas_segmenter import AtlasSegmenter&#10;import time&#10;import os&#10;&#10;&#10;class AtlasSegmentationRunner:&#10;    def __init__(self, num_atlases_to_select, atlas_dir, preprocessing_steps, atlas_selector, segmentation_voter, segmentation_refiner, output_dir, target_images_dir):&#10;        self.segmenter = AtlasSegmenter(&#10;            num_atlases_to_select,&#10;            atlas_dir,&#10;            preprocessing_steps,&#10;            atlas_selector,&#10;            segmentation_voter,&#10;            segmentation_refiner,&#10;            output_dir&#10;        )&#10;        self.target_images_dir = target_images_dir&#10;&#10;    def run(self):&#10;        # start timing&#10;        start_time = time.time()&#10;        target_images = self.segmenter.load_target_images(self.target_images_dir)&#10;        segmented_images = self.segmenter.segment_images(target_images)&#10;        self.segmenter.save_segmentation(segmented_images)&#10;        # end timing and compute durations&#10;        end_time = time.time()&#10;        total_seconds = end_time - start_time&#10;        # format total duration h:m:s&#10;        hrs = int(total_seconds // 3600)&#10;        mins = int((total_seconds % 3600) // 60)&#10;        secs = int(total_seconds % 60)&#10;        duration_str = f&quot;{hrs:02d}:{mins:02d}:{secs:02d}&quot;&#10;        # average per image&#10;        num_images = len(target_images)&#10;        if num_images &gt; 0:&#10;            avg_seconds = total_seconds / num_images&#10;            avg_hrs = int(avg_seconds // 3600)&#10;            avg_mins = int((avg_seconds % 3600) // 60)&#10;            avg_secs = int(avg_seconds % 60)&#10;            avg_str = f&quot;{avg_hrs:02d}:{avg_mins:02d}:{avg_secs:02d}&quot;&#10;        else:&#10;            avg_str = &quot;00:00:00&quot;&#10;        # write durations to file in output_dir&#10;        duration_file = os.path.join(self.segmenter.output_dir, &quot;duration.txt&quot;)&#10;        with open(duration_file, &quot;w&quot;) as f:&#10;            f.write(f&quot;Total duration: {duration_str}\n&quot;)&#10;            f.write(f&quot;Average per image: {avg_str}\n&quot;)&#10;&#10;# Beispiel für die Ausführung:&#10;if __name__ == &quot;__main__&quot;:&#10;    # Hier müssen die passenden Objekte und Parameter übergeben werden&#10;    runner = AtlasSegmentationRunner(&#10;        num_atlases_to_select=13,&#10;        atlas_dir=&quot;data/Atlas_Data_BMI_Percentile&quot;,&#10;        # preprocessing_steps=[DimplesRoiPreprocessor(target_ratio=10/7) ,BlueColorPreprocessor()],  # Liste mit Preprocessing-Objekten&#10;        preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;        atlas_selector=BmiAtlasSelector(&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;, &quot;data/Info_Sheets/bmi_table_who.csv&quot;),      # AtlasSelector-Objekt&#10;        segmentation_voter=WeightedMajorityVoter(scheme=&quot;softmax&quot;, temperature=0.02, threshold=0.5),  # SegmentationVoter-Objekt&#10;        segmentation_refiner=ColorPatchRefiner(BlueColorPreprocessor()),&#10;        output_dir=&quot;data/Atlas_Experiment100&quot;,&#10;        target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    )&#10;    # runner = AtlasSegmentationRunner(&#10;    #     num_atlases_to_select=3,&#10;    #     atlas_dir=&quot;data/Atlas_Data&quot;,&#10;    #     preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;    #     atlas_selector=SimilarityAtlasSelector(),      # AtlasSelector-Objekt&#10;    #     segmentation_voter=MajorityVoter(),  # SegmentationVoter-Objekt&#10;    #     segmentation_refiner=None,&#10;    #     output_dir=&quot;data/Atlas_Experiment01&quot;,&#10;    #     target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    # )&#10;    runner.run()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/ssl/pretext/jigsaw_permutation/JigsawPermTrain.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ssl/pretext/jigsaw_permutation/JigsawPermTrain.py" />
              <option name="originalContent" value="# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;Created on Thu Sep 14 12:16:31 2017&#10;&#10;@author: Biagio Brattoli&#10;&quot;&quot;&quot;&#10;import argparse&#10;import datetime&#10;import os&#10;from time import time&#10;&#10;import numpy as np&#10;import torch&#10;from matplotlib import pyplot as plt&#10;from torch import nn&#10;from torch.autograd import Variable&#10;&#10;from ssl.dataset.JigsawImageLoader import DataLoader&#10;from ssl.pretext.jigsaw_permutation.JigsawPermNetwork import JigsawPermNetwork&#10;from ssl.pretext.jigsaw_permutation.utils.TrainingUtils import adjust_learning_rate, compute_accuracy&#10;&#10;parser = argparse.ArgumentParser(description='Train JigsawPuzzleSolver on Imagenet')&#10;parser.add_argument('data', type=str, help='Path to Imagenet folder')&#10;parser.add_argument('--model', default=None, type=str, help='Path to pretrained model')&#10;parser.add_argument('--classes', default=1000, type=int, help='Number of permutation to use')&#10;parser.add_argument('--gpu', default=0, type=int, help='gpu id')&#10;parser.add_argument('--epochs', default=70, type=int, help='number of total epochs for training')&#10;parser.add_argument('--iter_start', default=0, type=int, help='Starting iteration count')&#10;parser.add_argument('--batch', default=64, type=int, help='batch size')&#10;parser.add_argument('--checkpoint', default='data/SSL_Pretext/JigsawPerm', type=str, help='checkpoint folder')&#10;parser.add_argument('--lr', default=0.001, type=float, help='learning rate for SGD optimizer')&#10;parser.add_argument('--cores', default=0, type=int, help='number of CPU core for loading')&#10;parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',&#10;                    help='evaluate model on validation set, No training')&#10;args = parser.parse_args()&#10;&#10;&#10;def main():&#10;    total_start_time = time()&#10;    log_print('Start training: %s' % datetime.datetime.fromtimestamp(total_start_time).strftime('%Y-%m-%d %H:%M:%S'))&#10;&#10;    device = configure_device()&#10;    log_print('Process number: %d' % (os.getpid()))&#10;    os.makedirs(args.checkpoint,exist_ok=True)&#10;&#10;    train_data, train_loader = load_train_data()&#10;    val_data, val_loader = load_val_data()&#10;&#10;    iter_per_epoch = train_data.N / args.batch&#10;    log_print('Images: train %d, validation %d' % (train_data.N, val_data.N))&#10;&#10;    net = initialize_network(device)&#10;    load_network(net)&#10;&#10;    criterion = nn.CrossEntropyLoss()&#10;    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)&#10;&#10;    logger_test = None&#10;&#10;    ############## TESTING ###############&#10;    if args.evaluate:&#10;        test(net, criterion, None, val_loader, 0, device)&#10;        return&#10;&#10;    ############## TRAINING ###############&#10;    log_print(('Start training: lr %f, batch size %d, classes %d' % (args.lr, args.batch, args.classes)))&#10;    log_print(('Checkpoint: ' + args.checkpoint))&#10;&#10;    # Train the Model&#10;    steps, train_accs, train_losses, val_accs, val_losses = train_model(criterion, device, iter_per_epoch, logger_test, net, optimizer, train_loader, val_loader)&#10;    save_final_model(net)&#10;&#10;    # Print total training time&#10;    total_end_time = time()&#10;    log_print('End training: %s' % datetime.datetime.fromtimestamp(total_end_time).strftime('%Y-%m-%d %H:%M:%S'))&#10;    duration_seconds = total_end_time - total_start_time&#10;    m, s = divmod(duration_seconds, 60)&#10;    h, m = divmod(m, 60)&#10;    log_print(&quot;Training duration: %d hours, %d minutes und %d seconds&quot; % (h, m, s))&#10;&#10;    plot_train_and_val_metrics(train_accs, train_losses, val_accs, val_losses)&#10;&#10;    evaluate_best_model(criterion, device, net, steps, val_loader)&#10;&#10;&#10;def evaluate_best_model(criterion, device, net, steps, val_loader):&#10;    log_print(&quot;\nLoading best model for final evaluation...&quot;)&#10;    best_model_path = os.path.join(args.checkpoint, 'best_model.pth.tar')&#10;    if os.path.exists(best_model_path):&#10;        net.load_state_dict(torch.load(best_model_path))&#10;        log_print(&quot;Best model loaded. Evaluating on validation set...&quot;)&#10;        final_loss, final_acc = test(net, criterion, None, val_loader, steps, device)&#10;        log_print(f&quot;Best model performance - Loss: {final_loss:.4f}, Accuracy: {final_acc:.2f}%&quot;)&#10;    else:&#10;        log_print(&quot;No best model found to evaluate.&quot;)&#10;&#10;&#10;def plot_train_and_val_metrics(train_accs, train_losses, val_accs, val_losses):&#10;    epochs_range = range(1, len(train_losses) + 1)&#10;    plt.figure(figsize=(12, 5))&#10;    # Plot Loss&#10;    plt.subplot(1, 2, 1)&#10;    plt.plot(epochs_range, train_losses, label='Train Loss')&#10;    plt.plot(epochs_range, val_losses, label='Val Loss')&#10;    plt.title('Loss over Epochs')&#10;    plt.xlabel('Epochs')&#10;    plt.ylabel('Loss')&#10;    plt.legend()&#10;    # Plot Accuracy&#10;    plt.subplot(1, 2, 2)&#10;    plt.plot(epochs_range, train_accs, label='Train Accuracy')&#10;    plt.plot(epochs_range, val_accs, label='Val Accuracy')&#10;    plt.title('Accuracy over Epochs')&#10;    plt.xlabel('Epochs')&#10;    plt.ylabel('Accuracy (%)')&#10;    plt.legend()&#10;    plt.tight_layout()&#10;    plot_path = os.path.join(args.checkpoint, 'training_metrics_plot.png')&#10;    plt.savefig(plot_path)&#10;    log_print(f&quot;Training metrics plot saved to {plot_path}&quot;)&#10;    plt.close()&#10;&#10;&#10;def save_final_model(net):&#10;    final_model_path = os.path.join(args.checkpoint, 'final_model.pth.tar')&#10;    net.save(final_model_path)&#10;    log_print(f&quot;Final model saved to {final_model_path}&quot;)&#10;&#10;&#10;def train_model(criterion, device, iter_per_epoch, logger_test, net, optimizer, train_loader, val_loader):&#10;    batch_time, net_time = [], []&#10;    steps = args.iter_start&#10;    best_val_acc = 0.0&#10;    train_losses, train_accs, val_losses, val_accs = [], [], [], []&#10;    for epoch in range(int(args.iter_start / iter_per_epoch), args.epochs):&#10;        lr = adjust_learning_rate(optimizer, epoch, init_lr=args.lr, step=20, decay=0.1)&#10;&#10;        end = time()&#10;        epoch_train_loss = 0.0&#10;        epoch_train_acc = 0.0&#10;        num_batches = 0&#10;        for i, (images, labels, original) in enumerate(train_loader):&#10;            batch_time.append(time() - end)&#10;            if len(batch_time) &gt; 100:&#10;                del batch_time[0]&#10;&#10;            images = Variable(images)&#10;            labels = Variable(labels)&#10;            images = images.to(device)&#10;            labels = labels.to(device)&#10;&#10;            # Forward + Backward + Optimize&#10;            optimizer.zero_grad()&#10;            t = time()&#10;            outputs = net(images)&#10;            net_time.append(time() - t)&#10;            if len(net_time) &gt; 100:&#10;                del net_time[0]&#10;&#10;            prec1, prec5 = compute_accuracy(outputs.cpu().data, labels.cpu().data, topk=(1, 5))&#10;            acc = prec1.item()&#10;&#10;            loss = criterion(outputs, labels)&#10;            loss.backward()&#10;            optimizer.step()&#10;            loss_val = float(loss.cpu().data.numpy())&#10;&#10;            epoch_train_loss += loss_val&#10;            epoch_train_acc += acc&#10;            num_batches += 1&#10;&#10;            if steps % 20 == 0:&#10;                log_print(&#10;                    ('[%2d/%2d] %5d) [batch load % 2.3fsec, net %1.2fsec], LR %.5f, Loss: % 1.3f, Accuracy % 2.2f%%' % (&#10;                        epoch + 1, args.epochs, steps,&#10;                        np.mean(batch_time), np.mean(net_time),&#10;                        lr, loss_val, acc)))&#10;&#10;            if steps % 20 == 0:&#10;                original = [im[0] for im in original]&#10;                imgs = np.zeros([9, 75, 75, 3])&#10;                for ti, img in enumerate(original):&#10;                    img = img.numpy()&#10;                    imgs[ti] = np.stack([&#10;                        (im - im.min()) / (im.max() - im.min()) if im.max() &gt; im.min() else np.zeros_like(im)&#10;                        for im in img&#10;                    ], axis=2)&#10;&#10;            steps += 1&#10;&#10;            if steps % 1000 == 0:&#10;                filename = '%s/jps_%03i_%06d.pth.tar' % (args.checkpoint, epoch, steps)&#10;                net.save(filename)&#10;                log_print('Saved: ' + args.checkpoint)&#10;&#10;            end = time()&#10;&#10;        # Store average epoch metrics for training&#10;        train_losses.append(epoch_train_loss / num_batches)&#10;        train_accs.append(epoch_train_acc / num_batches)&#10;&#10;        val_loss, val_acc = test(net, criterion, logger_test, val_loader, steps, device)&#10;        val_losses.append(val_loss)&#10;        val_accs.append(val_acc)&#10;&#10;        if val_acc &gt; best_val_acc:&#10;            best_val_acc = val_acc&#10;            best_model_path = os.path.join(args.checkpoint, 'best_model.pth.tar')&#10;            net.save(best_model_path)&#10;            log_print(f&quot;New best model saved to {best_model_path} with validation accuracy: {val_acc:.2f}%&quot;)&#10;&#10;        if os.path.exists(args.checkpoint + '/stop.txt'):&#10;            # break without using CTRL+C&#10;            break&#10;    return steps, train_accs, train_losses, val_accs, val_losses&#10;&#10;&#10;def load_val_data():&#10;    val_path = args.data + '/ILSVRC2012_img_val'&#10;    if os.path.exists(val_path + '_255x255'):&#10;        val_path += '_255x255'&#10;    val_data = DataLoader(val_path, args.data + '/ilsvrc12_val.txt', classes=args.classes)&#10;    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=args.batch, shuffle=True,&#10;                                             num_workers=args.cores)&#10;    return val_data, val_loader&#10;&#10;&#10;def load_train_data():&#10;    train_path = args.data + '/ILSVRC2012_img_train'&#10;    if os.path.exists(train_path + '_255x255'):&#10;        train_path += '_255x255'&#10;    train_data = DataLoader(train_path, args.data + '/ilsvrc12_train.txt', classes=args.classes)&#10;    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=args.batch, shuffle=True,&#10;                                               num_workers=args.cores)&#10;    return train_data, train_loader&#10;&#10;&#10;def initialize_network(device):&#10;    net = JigsawPermNetwork(args.classes)&#10;    net.to(device)&#10;    log_print(&quot;Network has been moved to the selected device.&quot;)&#10;    return net&#10;&#10;&#10;def load_network(net):&#10;    # load from checkpoint if exists, otherwise from model&#10;    if os.path.exists(args.checkpoint):&#10;        files = [f for f in os.listdir(args.checkpoint) if 'pth' in f]&#10;        if len(files) &gt; 0:&#10;            files.sort()&#10;            # print files&#10;            ckp = files[-1]&#10;            net.load_state_dict(torch.load(args.checkpoint + '/' + ckp))&#10;            args.iter_start = int(ckp.split(&quot;.&quot;)[-3].split(&quot;_&quot;)[-1])&#10;            log_print('Starting from: ', ckp)&#10;        else:&#10;            if args.model is not None:&#10;                net.load(args.model)&#10;    else:&#10;        if args.model is not None:&#10;            net.load(args.model)&#10;&#10;&#10;def configure_device():&#10;    if args.gpu &gt;= 0 and torch.backends.mps.is_available():&#10;        device = torch.device(&quot;mps&quot;)&#10;        log_print(&quot;MPS available. Training runs on MPS.&quot;)&#10;    elif args.gpu &gt;= 0 and torch.cuda.is_available():&#10;        device = torch.device(&quot;cuda:%d&quot; % args.gpu)&#10;        log_print(('Using GPU %d' % args.gpu))&#10;        os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;&#10;        os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = str(args.gpu)&#10;    else:&#10;        device = torch.device(&quot;cpu&quot;)&#10;        log_print('Training läuft auf CPU.')&#10;    return device&#10;&#10;&#10;def test(net, criterion, logger, val_loader, steps, device):&#10;    log_print('Evaluating network.......')&#10;    accuracy = []&#10;    losses = []&#10;    net.eval()&#10;    for i, (images, labels, _) in enumerate(val_loader):&#10;        images = Variable(images)&#10;        labels = Variable(labels)&#10;        # if args.gpu &gt;= 0 and torch.cuda.is_available():&#10;        #     images = images.cuda()&#10;        images = images.to(device)&#10;        labels = labels.to(device)&#10;&#10;        # Forward + Backward + Optimize&#10;        outputs = net(images)&#10;        loss = criterion(outputs, labels)&#10;        losses.append(loss.item())&#10;&#10;        outputs = outputs.cpu().data&#10;        labels = labels.cpu().data&#10;&#10;        prec1, prec5 = compute_accuracy(outputs, labels, topk=(1, 5))&#10;        accuracy.append(prec1.item())&#10;&#10;    avg_loss = np.mean(losses)&#10;    avg_acc = np.mean(accuracy)&#10;&#10;    if logger is not None:&#10;        logger.scalar_summary('accuracy', avg_acc, steps)&#10;    log_print('TESTING: %d), Loss: %.3f, Accuracy %.2f%%' % (steps, avg_loss, avg_acc))&#10;    net.train()&#10;    return avg_loss, avg_acc&#10;&#10;&#10;def log_print(*args, sep=' ', end='\n', file=None, flush=False):&#10;    # standard output&#10;    print(*args, sep=sep, end=end, file=file, flush=flush)&#10;    # write in log file nur wenn args und args.checkpoint existieren&#10;    if 'args' in globals() and hasattr(args, 'checkpoint'):&#10;        log_dir = args.checkpoint&#10;        os.makedirs(log_dir, exist_ok=True)&#10;        log_path = os.path.join(log_dir, 'logs.txt')&#10;        with open(log_path, 'a', encoding='utf-8') as f:&#10;            print(*args, sep=sep, end=end, file=f)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
              <option name="updatedContent" value="# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;Created on Thu Sep 14 12:16:31 2017&#10;&#10;@author: Biagio Brattoli&#10;&quot;&quot;&quot;&#10;import argparse&#10;import datetime&#10;import os&#10;from time import time&#10;&#10;import numpy as np&#10;import torch&#10;from matplotlib import pyplot as plt&#10;from torch import nn&#10;from torch.autograd import Variable&#10;&#10;from ssl.dataset.JigsawImageLoader import DataLoader&#10;from ssl.pretext.jigsaw_permutation.JigsawPermNetwork import JigsawPermNetwork&#10;from ssl.pretext.jigsaw_permutation.utils.TrainingUtils import adjust_learning_rate, compute_accuracy&#10;&#10;parser = argparse.ArgumentParser(description='Train JigsawPuzzleSolver on Imagenet')&#10;parser.add_argument('data', type=str, help='Path to Imagenet folder')&#10;parser.add_argument('--model', default=None, type=str, help='Path to pretrained model')&#10;parser.add_argument('--classes', default=1000, type=int, help='Number of permutation to use')&#10;parser.add_argument('--gpu', default=0, type=int, help='gpu id')&#10;parser.add_argument('--epochs', default=70, type=int, help='number of total epochs for training')&#10;parser.add_argument('--iter_start', default=0, type=int, help='Starting iteration count')&#10;parser.add_argument('--batch', default=64, type=int, help='batch size')&#10;parser.add_argument('--checkpoint', default='data/SSL_Pretext/JigsawPerm', type=str, help='checkpoint folder')&#10;parser.add_argument('--lr', default=0.001, type=float, help='learning rate for SGD optimizer')&#10;parser.add_argument('--cores', default=0, type=int, help='number of CPU core for loading')&#10;parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',&#10;                    help='evaluate model on validation set, No training')&#10;args = parser.parse_args()&#10;&#10;&#10;def main():&#10;    total_start_time = time()&#10;    log_print('Start training: %s' % datetime.datetime.fromtimestamp(total_start_time).strftime('%Y-%m-%d %H:%M:%S'))&#10;&#10;    device = configure_device()&#10;    log_print('Process number: %d' % (os.getpid()))&#10;    os.makedirs(args.checkpoint,exist_ok=True)&#10;&#10;    train_data, train_loader = load_train_data()&#10;    val_data, val_loader = load_val_data()&#10;&#10;    iter_per_epoch = train_data.N / args.batch&#10;    log_print('Images: train %d, validation %d' % (train_data.N, val_data.N))&#10;&#10;    net = initialize_network(device)&#10;    load_network(net)&#10;&#10;    criterion = nn.CrossEntropyLoss()&#10;    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)&#10;&#10;    logger_test = None&#10;&#10;    ############## TESTING ###############&#10;    if args.evaluate:&#10;        test(net, criterion, None, val_loader, 0, device)&#10;        return&#10;&#10;    ############## TRAINING ###############&#10;    log_print(('Start training: lr %f, batch size %d, classes %d' % (args.lr, args.batch, args.classes)))&#10;    log_print(('Checkpoint: ' + args.checkpoint))&#10;&#10;    # Train the Model&#10;    steps, train_accs, train_losses, val_accs, val_losses = train_model(criterion, device, iter_per_epoch, logger_test, net, optimizer, train_loader, val_loader)&#10;    save_final_model(net)&#10;&#10;    # Print total training time&#10;    total_end_time = time()&#10;    log_print('End training: %s' % datetime.datetime.fromtimestamp(total_end_time).strftime('%Y-%m-%d %H:%M:%S'))&#10;    duration_seconds = total_end_time - total_start_time&#10;    m, s = divmod(duration_seconds, 60)&#10;    h, m = divmod(m, 60)&#10;    log_print(&quot;Training duration: %d hours, %d minutes und %d seconds&quot; % (h, m, s))&#10;&#10;    plot_train_and_val_metrics(train_accs, train_losses, val_accs, val_losses)&#10;&#10;    evaluate_best_model(criterion, device, net, steps, val_loader)&#10;&#10;&#10;def evaluate_best_model(criterion, device, net, steps, val_loader):&#10;    log_print(&quot;\nLoading best model for final evaluation...&quot;)&#10;    best_model_path = os.path.join(args.checkpoint, 'best_model.pth.tar')&#10;    if os.path.exists(best_model_path):&#10;        net.load_state_dict(torch.load(best_model_path))&#10;        log_print(&quot;Best model loaded. Evaluating on validation set...&quot;)&#10;        final_loss, final_acc = test(net, criterion, None, val_loader, steps, device)&#10;        log_print(f&quot;Best model performance - Loss: {final_loss:.4f}, Accuracy: {final_acc:.2f}%&quot;)&#10;    else:&#10;        log_print(&quot;No best model found to evaluate.&quot;)&#10;&#10;&#10;def plot_train_and_val_metrics(train_accs, train_losses, val_accs, val_losses):&#10;    epochs_range = range(1, len(train_losses) + 1)&#10;    plt.figure(figsize=(12, 5))&#10;    # Plot Loss&#10;    plt.subplot(1, 2, 1)&#10;    plt.plot(epochs_range, train_losses, label='Train Loss')&#10;    plt.plot(epochs_range, val_losses, label='Val Loss')&#10;    plt.title('Loss over Epochs')&#10;    plt.xlabel('Epochs')&#10;    plt.ylabel('Loss')&#10;    plt.legend()&#10;    # Plot Accuracy&#10;    plt.subplot(1, 2, 2)&#10;    plt.plot(epochs_range, train_accs, label='Train Accuracy')&#10;    plt.plot(epochs_range, val_accs, label='Val Accuracy')&#10;    plt.title('Accuracy over Epochs')&#10;    plt.xlabel('Epochs')&#10;    plt.ylabel('Accuracy (%)')&#10;    plt.legend()&#10;    plt.tight_layout()&#10;    plot_path = os.path.join(args.checkpoint, 'training_metrics_plot.png')&#10;    plt.savefig(plot_path)&#10;    log_print(f&quot;Training metrics plot saved to {plot_path}&quot;)&#10;    plt.close()&#10;&#10;&#10;def save_final_model(net):&#10;    final_model_path = os.path.join(args.checkpoint, 'final_model.pth.tar')&#10;    net.save(final_model_path)&#10;    log_print(f&quot;Final model saved to {final_model_path}&quot;)&#10;&#10;&#10;def train_model(criterion, device, iter_per_epoch, logger_test, net, optimizer, train_loader, val_loader):&#10;    batch_time, net_time = [], []&#10;    steps = args.iter_start&#10;    best_val_acc = 0.0&#10;    train_losses, train_accs, val_losses, val_accs = [], [], [], []&#10;    for epoch in range(int(args.iter_start / iter_per_epoch), args.epochs):&#10;        lr = adjust_learning_rate(optimizer, epoch, init_lr=args.lr, step=20, decay=0.1)&#10;&#10;        end = time()&#10;        epoch_train_loss = 0.0&#10;        epoch_train_acc = 0.0&#10;        num_batches = 0&#10;        for i, (images, labels, original) in enumerate(train_loader):&#10;            batch_time.append(time() - end)&#10;            if len(batch_time) &gt; 100:&#10;                del batch_time[0]&#10;&#10;            images = Variable(images)&#10;            labels = Variable(labels)&#10;            images = images.to(device)&#10;            labels = labels.to(device)&#10;&#10;            # Forward + Backward + Optimize&#10;            optimizer.zero_grad()&#10;            t = time()&#10;            outputs = net(images)&#10;            net_time.append(time() - t)&#10;            if len(net_time) &gt; 100:&#10;                del net_time[0]&#10;&#10;            prec1, prec5 = compute_accuracy(outputs.cpu().data, labels.cpu().data, topk=(1, 5))&#10;            acc = prec1.item()&#10;&#10;            loss = criterion(outputs, labels)&#10;            loss.backward()&#10;            optimizer.step()&#10;            loss_val = float(loss.cpu().data.numpy())&#10;&#10;            epoch_train_loss += loss_val&#10;            epoch_train_acc += acc&#10;            num_batches += 1&#10;&#10;            if steps % 20 == 0:&#10;                log_print(&#10;                    ('[%2d/%2d] %5d) [batch load % 2.3fsec, net %1.2fsec], LR %.5f, Loss: % 1.3f, Accuracy % 2.2f%%' % (&#10;                        epoch + 1, args.epochs, steps,&#10;                        np.mean(batch_time), np.mean(net_time),&#10;                        lr, loss_val, acc)))&#10;&#10;            if steps % 20 == 0:&#10;                original = [im[0] for im in original]&#10;                imgs = np.zeros([9, 75, 75, 3])&#10;                for ti, img in enumerate(original):&#10;                    img = img.numpy()&#10;                    imgs[ti] = np.stack([&#10;                        (im - im.min()) / (im.max() - im.min()) if im.max() &gt; im.min() else np.zeros_like(im)&#10;                        for im in img&#10;                    ], axis=2)&#10;&#10;            steps += 1&#10;&#10;            if steps % 1000 == 0:&#10;                filename = '%s/jps_%03i_%06d.pth.tar' % (args.checkpoint, epoch, steps)&#10;                net.save(filename)&#10;                log_print('Saved: ' + args.checkpoint)&#10;&#10;            end = time()&#10;&#10;        # Store average epoch metrics for training&#10;        train_losses.append(epoch_train_loss / num_batches)&#10;        train_accs.append(epoch_train_acc / num_batches)&#10;&#10;        val_loss, val_acc = test(net, criterion, logger_test, val_loader, steps, device)&#10;        val_losses.append(val_loss)&#10;        val_accs.append(val_acc)&#10;&#10;        if val_acc &gt; best_val_acc:&#10;            best_val_acc = val_acc&#10;            best_model_path = os.path.join(args.checkpoint, 'best_model.pth.tar')&#10;            net.save(best_model_path)&#10;            log_print(f&quot;New best model saved to {best_model_path} with validation accuracy: {val_acc:.2f}%&quot;)&#10;&#10;        if os.path.exists(args.checkpoint + '/stop.txt'):&#10;            # break without using CTRL+C&#10;            break&#10;    return steps, train_accs, train_losses, val_accs, val_losses&#10;&#10;&#10;def load_val_data():&#10;    val_path = args.data + '/ILSVRC2012_img_val'&#10;    if os.path.exists(val_path + '_255x255'):&#10;        val_path += '_255x255'&#10;    val_data = DataLoader(val_path, args.data + '/ilsvrc12_val.txt', classes=args.classes)&#10;    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=args.batch, shuffle=True,&#10;                                             num_workers=args.cores)&#10;    return val_data, val_loader&#10;&#10;&#10;def load_train_data():&#10;    train_path = args.data + '/ILSVRC2012_img_train'&#10;    if os.path.exists(train_path + '_255x255'):&#10;        train_path += '_255x255'&#10;    train_data = DataLoader(train_path, args.data + '/ilsvrc12_train.txt', classes=args.classes)&#10;    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=args.batch, shuffle=True,&#10;                                               num_workers=args.cores)&#10;    return train_data, train_loader&#10;&#10;&#10;def initialize_network(device):&#10;    net = JigsawPermNetwork(args.classes)&#10;    net.to(device)&#10;    log_print(&quot;Network has been moved to the selected device.&quot;)&#10;    return net&#10;&#10;&#10;def load_network(net):&#10;    # load from checkpoint if exists, otherwise from model&#10;    if os.path.exists(args.checkpoint):&#10;        files = [f for f in os.listdir(args.checkpoint) if 'pth' in f]&#10;        if len(files) &gt; 0:&#10;            files.sort()&#10;            # print files&#10;            ckp = files[-1]&#10;            net.load_state_dict(torch.load(args.checkpoint + '/' + ckp))&#10;            args.iter_start = int(ckp.split(&quot;.&quot;)[-3].split(&quot;_&quot;)[-1])&#10;            log_print('Starting from: ', ckp)&#10;        else:&#10;            if args.model is not None:&#10;                net.load(args.model)&#10;    else:&#10;        if args.model is not None:&#10;            net.load(args.model)&#10;&#10;&#10;def configure_device():&#10;    if args.gpu &gt;= 0 and torch.backends.mps.is_available():&#10;        device = torch.device(&quot;mps&quot;)&#10;        log_print(&quot;MPS available. Training runs on MPS.&quot;)&#10;    elif args.gpu &gt;= 0 and torch.cuda.is_available():&#10;        device = torch.device(&quot;cuda:%d&quot; % args.gpu)&#10;        log_print(('Using GPU %d' % args.gpu))&#10;        os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;&#10;        os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = str(args.gpu)&#10;    else:&#10;        device = torch.device(&quot;cpu&quot;)&#10;        log_print('Training läuft auf CPU.')&#10;    return device&#10;&#10;&#10;def test(net, criterion, logger, val_loader, steps, device):&#10;    log_print('Evaluating network.......')&#10;    accuracy = []&#10;    losses = []&#10;    net.eval()&#10;    for i, (images, labels, _) in enumerate(val_loader):&#10;        images = Variable(images)&#10;        labels = Variable(labels)&#10;        # if args.gpu &gt;= 0 and torch.cuda.is_available():&#10;        #     images = images.cuda()&#10;        images = images.to(device)&#10;        labels = labels.to(device)&#10;&#10;        # Forward + Backward + Optimize&#10;        outputs = net(images)&#10;        loss = criterion(outputs, labels)&#10;        losses.append(loss.item())&#10;&#10;        outputs = outputs.cpu().data&#10;        labels = labels.cpu().data&#10;&#10;        prec1, prec5 = compute_accuracy(outputs, labels, topk=(1, 5))&#10;        accuracy.append(prec1.item())&#10;&#10;    avg_loss = np.mean(losses)&#10;    avg_acc = np.mean(accuracy)&#10;&#10;    if logger is not None:&#10;        logger.scalar_summary('accuracy', avg_acc, steps)&#10;    log_print('TESTING: %d), Loss: %.3f, Accuracy %.2f%%' % (steps, avg_loss, avg_acc))&#10;    net.train()&#10;    return avg_loss, avg_acc&#10;&#10;&#10;def log_print(*args, sep=' ', end='\n', file=None, flush=False):&#10;    # standard output&#10;    print(*args, sep=sep, end=end, file=file, flush=flush)&#10;    # write in log file nur wenn args und args.checkpoint existieren&#10;    if 'args' in globals() and hasattr(args, 'checkpoint'):&#10;        log_dir = args.checkpoint&#10;        os.makedirs(log_dir, exist_ok=True)&#10;        log_path = os.path.join(log_dir, 'logs.txt')&#10;        with open(log_path, 'a', encoding='utf-8') as f:&#10;            print(*args, sep=sep, end=end, file=f)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/ssl/pretext/jigsaw_permutation/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ssl/pretext/jigsaw_permutation/README.md" />
              <option name="originalContent" value="Example command to run jigsaw permutation pretext training&#10;&#10;```bash&#10;python -m ssl.pretext.jigsaw_permutation.JigsawPermTrain data/ML_Pretext_All_Filtered --checkpoint data/SSL_Pretext/JigsawPerm/Experiment01 --gpu 0 --batch 64 --classes 100 --epochs 30&#10;```&#10;&#10;## Command Line Options&#10;&#10;| Option         | Type  | Default Value               | Description                                          |&#10;|----------------|-------|-----------------------------|------------------------------------------------------|&#10;| data           | str   | (required path)             | Path to the Imagenet folder (training data)          |&#10;| --model        | str   | None                        | Path to a pretrained model                           |&#10;| --classes      | int   | 1000                        | Number of permutations/classes                       |&#10;| --gpu          | int   | 0                           | GPU ID (0 = first GPU, -1 = CPU/MPS)                 |&#10;| --epochs       | int   | 70                          | Number of training epochs                            |&#10;| --iter_start   | int   | 0                           | Starting value for iteration counter                 |&#10;| --batch        | int   | 64                          | Batch size                                           |&#10;| --checkpoint   | str   | data/SSL_Pretext/JigsawPerm | Folder for checkpoints/models                        |&#10;| --lr           | float | 0.001                       | Learning rate for the SGD optimizer                  |&#10;| --cores        | int   | 0                           | Number of CPU cores for data loading                 |&#10;| -e, --evaluate | Flag  | False                       | Only evaluate on the validation dataset, no training |&#10;| --pp            | list  | ['torso', 'square']         | Preprocessing pipeline in order. Example: --pp torso square         |&#10;| --torso-target-ratio | float | 0.714 (5/7)              | TorsoRoiPreprocessor.target_ratio (e.g. 5/7)                        |&#10;| --square-resize | int   | 256                        | SquareImagePreprocessor: Resize edge (Default 256)                  |&#10;| --square-crop   | int   | 255                        | SquareImagePreprocessor: CenterCrop edge (Default 255)              |" />
              <option name="updatedContent" value="Example command to run jigsaw permutation pretext training&#10;&#10;```bash&#10;python -m ssl.pretext.jigsaw_permutation.JigsawPermTrain data/ML_Pretext_All_Filtered --checkpoint data/SSL_Pretext/JigsawPerm/Experiment01 --gpu 0 --batch 64 --classes 100 --epochs 30&#10;```&#10;&#10;## Command Line Options&#10;&#10;| Option         | Type  | Default Value               | Description                                          |&#10;|----------------|-------|-----------------------------|------------------------------------------------------|&#10;| data           | str   | (required path)             | Path to the Imagenet folder (training data)          |&#10;| --model        | str   | None                        | Path to a pretrained model                           |&#10;| --classes      | int   | 1000                        | Number of permutations/classes                       |&#10;| --gpu          | int   | 0                           | GPU ID (0 = first GPU, -1 = CPU/MPS)                 |&#10;| --epochs       | int   | 70                          | Number of training epochs                            |&#10;| --iter_start   | int   | 0                           | Starting value for iteration counter                 |&#10;| --batch        | int   | 64                          | Batch size                                           |&#10;| --checkpoint   | str   | data/SSL_Pretext/JigsawPerm | Folder for checkpoints/models                        |&#10;| --lr           | float | 0.001                       | Learning rate for the SGD optimizer                  |&#10;| --cores        | int   | 0                           | Number of CPU cores for data loading                 |&#10;| -e, --evaluate | Flag  | False                       | Only evaluate on the validation dataset, no training |&#10;| --pp            | list  | ['torso', 'square']         | Preprocessing pipeline in order. Example: --pp torso square         |&#10;| --torso-target-ratio | float | 0.714 (5/7)              | TorsoRoiPreprocessor.target_ratio (e.g. 5/7)                        |&#10;| --square-resize | int   | 256                        | SquareImagePreprocessor: Resize edge (Default 256)                  |&#10;| --square-crop   | int   | 255                        | SquareImagePreprocessor: CenterCrop edge (Default 255)              |" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>