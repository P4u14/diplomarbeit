<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/SegmentationRunner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/SegmentationRunner.py" />
              <option name="originalContent" value="from atlas.refiner.color_patch_refiner import ColorPatchRefiner&#10;from atlas.selector.bmi_atlas_selector import BmiAtlasSelector&#10;from atlas.selector.similarity_atlas_selector import SimilarityAtlasSelector&#10;from atlas.voter.majority_voter import MajorityVoter&#10;from atlas.voter.weighted_majority_voter import WeightedMajorityVoter&#10;from preprocessing.blue_color_preprocessor import BlueColorPreprocessor&#10;from preprocessing.color_preprocessor import ColorPreprocessor&#10;from preprocessing.dimples_roi_preprocessor import DimplesRoiPreprocessor&#10;from preprocessing.torso_roi_preprocessor import TorsoRoiPreprocessor&#10;from segmenter.atlas_segmenter import AtlasSegmenter&#10;import time&#10;import os&#10;&#10;&#10;class AtlasSegmentationRunner:&#10;    def __init__(self, num_atlases_to_select, atlas_dir, preprocessing_steps, atlas_selector, segmentation_voter, segmentation_refiner, output_dir, target_images_dir):&#10;        self.segmenter = AtlasSegmenter(&#10;            num_atlases_to_select,&#10;            atlas_dir,&#10;            preprocessing_steps,&#10;            atlas_selector,&#10;            segmentation_voter,&#10;            segmentation_refiner,&#10;            output_dir&#10;        )&#10;        self.target_images_dir = target_images_dir&#10;&#10;    def run(self):&#10;        # start timing&#10;        start_time = time.time()&#10;        target_images = self.segmenter.load_target_images(self.target_images_dir)&#10;        segmented_images = self.segmenter.segment_images(target_images)&#10;        self.segmenter.save_segmentation(segmented_images)&#10;        # end timing and compute durations&#10;        end_time = time.time()&#10;        total_seconds = end_time - start_time&#10;        # format total duration h:m:s&#10;        hrs = int(total_seconds // 3600)&#10;        mins = int((total_seconds % 3600) // 60)&#10;        secs = int(total_seconds % 60)&#10;        duration_str = f&quot;{hrs:02d}:{mins:02d}:{secs:02d}&quot;&#10;        # average per image&#10;        num_images = len(target_images)&#10;        if num_images &gt; 0:&#10;            avg_seconds = total_seconds / num_images&#10;            avg_hrs = int(avg_seconds // 3600)&#10;            avg_mins = int((avg_seconds % 3600) // 60)&#10;            avg_secs = int(avg_seconds % 60)&#10;            avg_str = f&quot;{avg_hrs:02d}:{avg_mins:02d}:{avg_secs:02d}&quot;&#10;        else:&#10;            avg_str = &quot;00:00:00&quot;&#10;        # write durations to file in output_dir&#10;        duration_file = os.path.join(self.segmenter.output_dir, &quot;duration.txt&quot;)&#10;        with open(duration_file, &quot;w&quot;) as f:&#10;            f.write(f&quot;Total duration: {duration_str}\n&quot;)&#10;            f.write(f&quot;Average per image: {avg_str}\n&quot;)&#10;&#10;# Beispiel für die Ausführung:&#10;if __name__ == &quot;__main__&quot;:&#10;    # Hier müssen die passenden Objekte und Parameter übergeben werden&#10;    runner = AtlasSegmentationRunner(&#10;        num_atlases_to_select=13,&#10;        atlas_dir=&quot;data/Atlas_Data_BMI_Percentile&quot;,&#10;        # preprocessing_steps=[DimplesRoiPreprocessor(target_ratio=10/7) ,BlueColorPreprocessor()],  # Liste mit Preprocessing-Objekten&#10;        preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;        atlas_selector=BmiAtlasSelector(&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;, &quot;data/Info_Sheets/bmi_table_who.csv&quot;),      # AtlasSelector-Objekt&#10;        segmentation_voter=WeightedMajorityVoter(scheme=&quot;softmax&quot;, temperature=0.02, threshold=0.5),  # SegmentationVoter-Objekt&#10;        segmentation_refiner=ColorPatchRefiner(BlueColorPreprocessor()),&#10;        output_dir=&quot;data/Atlas_Experiment100&quot;,&#10;        target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    )&#10;    # runner = AtlasSegmentationRunner(&#10;    #     num_atlases_to_select=3,&#10;    #     atlas_dir=&quot;data/Atlas_Data&quot;,&#10;    #     preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;    #     atlas_selector=SimilarityAtlasSelector(),      # AtlasSelector-Objekt&#10;    #     segmentation_voter=MajorityVoter(),  # SegmentationVoter-Objekt&#10;    #     segmentation_refiner=None,&#10;    #     output_dir=&quot;data/Atlas_Experiment01&quot;,&#10;    #     target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    # )&#10;    runner.run()" />
              <option name="updatedContent" value="from atlas.refiner.color_patch_refiner import ColorPatchRefiner&#10;from atlas.selector.bmi_atlas_selector import BmiAtlasSelector&#10;from atlas.selector.similarity_atlas_selector import SimilarityAtlasSelector&#10;from atlas.voter.majority_voter import MajorityVoter&#10;from atlas.voter.weighted_majority_voter import WeightedMajorityVoter&#10;from preprocessing.blue_color_preprocessor import BlueColorPreprocessor&#10;from preprocessing.color_preprocessor import ColorPreprocessor&#10;from preprocessing.dimples_roi_preprocessor import DimplesRoiPreprocessor&#10;from preprocessing.torso_roi_preprocessor import TorsoRoiPreprocessor&#10;from segmenter.atlas_segmenter import AtlasSegmenter&#10;import time&#10;import os&#10;&#10;&#10;class AtlasSegmentationRunner:&#10;    def __init__(self, num_atlases_to_select, atlas_dir, preprocessing_steps, atlas_selector, segmentation_voter, segmentation_refiner, output_dir, target_images_dir):&#10;        self.segmenter = AtlasSegmenter(&#10;            num_atlases_to_select,&#10;            atlas_dir,&#10;            preprocessing_steps,&#10;            atlas_selector,&#10;            segmentation_voter,&#10;            segmentation_refiner,&#10;            output_dir&#10;        )&#10;        self.target_images_dir = target_images_dir&#10;&#10;    def run(self):&#10;        # start timing&#10;        start_time = time.time()&#10;        target_images = self.segmenter.load_target_images(self.target_images_dir)&#10;        segmented_images = self.segmenter.segment_images(target_images)&#10;        self.segmenter.save_segmentation(segmented_images)&#10;        # end timing and compute durations&#10;        end_time = time.time()&#10;        total_seconds = end_time - start_time&#10;        # format total duration h:m:s&#10;        hrs = int(total_seconds // 3600)&#10;        mins = int((total_seconds % 3600) // 60)&#10;        secs = int(total_seconds % 60)&#10;        duration_str = f&quot;{hrs:02d}:{mins:02d}:{secs:02d}&quot;&#10;        # average per image&#10;        num_images = len(target_images)&#10;        if num_images &gt; 0:&#10;            avg_seconds = total_seconds / num_images&#10;            avg_hrs = int(avg_seconds // 3600)&#10;            avg_mins = int((avg_seconds % 3600) // 60)&#10;            avg_secs = int(avg_seconds % 60)&#10;            avg_str = f&quot;{avg_hrs:02d}:{avg_mins:02d}:{avg_secs:02d}&quot;&#10;        else:&#10;            avg_str = &quot;00:00:00&quot;&#10;        # write durations to file in output_dir&#10;        duration_file = os.path.join(self.segmenter.output_dir, &quot;duration.txt&quot;)&#10;        with open(duration_file, &quot;w&quot;) as f:&#10;            f.write(f&quot;Total duration: {duration_str}\n&quot;)&#10;            f.write(f&quot;Average per image: {avg_str}\n&quot;)&#10;&#10;# Beispiel für die Ausführung:&#10;if __name__ == &quot;__main__&quot;:&#10;    # Hier müssen die passenden Objekte und Parameter übergeben werden&#10;    runner = AtlasSegmentationRunner(&#10;        num_atlases_to_select=13,&#10;        atlas_dir=&quot;data/Atlas_Data_BMI_Percentile&quot;,&#10;        # preprocessing_steps=[DimplesRoiPreprocessor(target_ratio=10/7) ,BlueColorPreprocessor()],  # Liste mit Preprocessing-Objekten&#10;        preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;        atlas_selector=BmiAtlasSelector(&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;, &quot;data/Info_Sheets/bmi_table_who.csv&quot;),      # AtlasSelector-Objekt&#10;        segmentation_voter=WeightedMajorityVoter(scheme=&quot;softmax&quot;, temperature=0.02, threshold=0.5),  # SegmentationVoter-Objekt&#10;        segmentation_refiner=ColorPatchRefiner(BlueColorPreprocessor()),&#10;        output_dir=&quot;data/Atlas_Experiment100&quot;,&#10;        target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    )&#10;    # runner = AtlasSegmentationRunner(&#10;    #     num_atlases_to_select=3,&#10;    #     atlas_dir=&quot;data/Atlas_Data&quot;,&#10;    #     preprocessing_steps=[],  # Liste mit Preprocessing-Objekten&#10;    #     atlas_selector=SimilarityAtlasSelector(),      # AtlasSelector-Objekt&#10;    #     segmentation_voter=MajorityVoter(),  # SegmentationVoter-Objekt&#10;    #     segmentation_refiner=None,&#10;    #     output_dir=&quot;data/Atlas_Experiment01&quot;,&#10;    #     target_images_dir=&quot;data/Validation_Data_Small&quot;&#10;    # )&#10;    runner.run()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/evaluation/evaluation_metrics.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/evaluation/evaluation_metrics.py" />
              <option name="originalContent" value="from dataclasses import dataclass&#10;from typing import Optional, Tuple&#10;&#10;&#10;@dataclass&#10;class EvaluationMetrics:&#10;    &quot;&quot;&quot;&#10;    Data class for storing evaluation metrics for segmentation tasks.&#10;    Each attribute represents a specific metric or result from the evaluation process.&#10;&#10;    Attributes:&#10;        tp (Optional[int]): True positives count.&#10;        fp (Optional[int]): False positives count.&#10;        fn (Optional[int]): False negatives count.&#10;        dice (Optional[float]): Dice coefficient.&#10;        precision (Optional[float]): Precision score.&#10;        recall (Optional[float]): Recall score.&#10;        n_gt_segments (Optional[int]): Number of ground truth segments.&#10;        n_pred_segments (Optional[int]): Number of predicted segments.&#10;        n_gt_segments_left (Optional[int]): Number of ground truth segments on the left side.&#10;        n_pred_segments_left (Optional[int]): Number of predicted segments on the left side.&#10;        n_gt_segments_right (Optional[int]): Number of ground truth segments on the right side.&#10;        n_pred_segments_right (Optional[int]): Number of predicted segments on the right side.&#10;        n_segments_success (Optional[int]): Number of successfully matched segments.&#10;        center_gt_left (Optional[Tuple[float, float]]): Center coordinates of ground truth segment (left).&#10;        center_gt_right (Optional[Tuple[float, float]]): Center coordinates of ground truth segment (right).&#10;        center_pred_left (Optional[Tuple[float, float]]): Center coordinates of predicted segment (left).&#10;        center_pred_right (Optional[Tuple[float, float]]): Center coordinates of predicted segment (right).&#10;        center_pred_success (Optional[int]): Number of successful center predictions.&#10;        center_diers_left (Optional[Tuple[float, float]]): Center coordinates from Diers system (left).&#10;        center_diers_right (Optional[Tuple[float, float]]): Center coordinates from Diers system (right).&#10;        center_diers_success (Optional[int]): Number of successful Diers center matches.&#10;        center_angle_error (Optional[float]): Error in center angle (in degrees or radians).&#10;        center_angle_error_abs (Optional[float]): Absolute error in center angle.&#10;        center_angle_success (Optional[int]): Number of successful center angle predictions.&#10;        center_angle_diers_error (Optional[float]): Error in Diers center angle.&#10;        center_angle_diers_error_abs (Optional[float]): Absolute error in Diers center angle.&#10;        center_angle_diers_success (Optional[int]): Number of successful Diers center angle predictions.&#10;    &quot;&quot;&quot;&#10;    tp: Optional[int] = None&#10;    fp: Optional[int] = None&#10;    fn: Optional[int] = None&#10;    dice: Optional[float] = None&#10;    precision: Optional[float] = None&#10;    recall: Optional[float] = None&#10;    n_gt_segments: Optional[int] = None&#10;    n_pred_segments: Optional[int] = None&#10;    n_gt_segments_left: Optional[int] = None&#10;    n_pred_segments_left: Optional[int] = None&#10;    n_gt_segments_right: Optional[int] = None&#10;    n_pred_segments_right: Optional[int] = None&#10;    n_segments_success: Optional[int] = None&#10;    center_gt_left: Optional[Tuple[float, float]] = None&#10;    center_gt_right: Optional[Tuple[float, float]] = None&#10;    center_pred_left: Optional[Tuple[float, float]] = None&#10;    center_pred_right: Optional[Tuple[float, float]] = None&#10;    center_pred_success: Optional[int] = None&#10;    center_diers_left: Optional[Tuple[float, float]] = None&#10;    center_diers_right: Optional[Tuple[float, float]] = None&#10;    center_diers_success: Optional[int] = None&#10;    center_angle_error: Optional[float] = None&#10;    center_angle_error_abs: Optional[float] = None&#10;    center_angle_success: Optional[int] = None&#10;    center_angle_diers_error: Optional[float] = None&#10;    center_angle_diers_error_abs: Optional[float] = None&#10;    center_angle_diers_success: Optional[int] = None" />
              <option name="updatedContent" value="from dataclasses import dataclass&#10;from typing import Optional, Tuple&#10;&#10;&#10;@dataclass&#10;class EvaluationMetrics:&#10;    &quot;&quot;&quot;&#10;    Data class for storing evaluation metrics for segmentation tasks.&#10;    Each attribute represents a specific metric or result from the evaluation process.&#10;&#10;    Attributes:&#10;        tp (Optional[int]): True positives count.&#10;        fp (Optional[int]): False positives count.&#10;        fn (Optional[int]): False negatives count.&#10;        dice (Optional[float]): Dice coefficient.&#10;        precision (Optional[float]): Precision score.&#10;        recall (Optional[float]): Recall score.&#10;        n_gt_segments (Optional[int]): Number of ground truth segments.&#10;        n_pred_segments (Optional[int]): Number of predicted segments.&#10;        n_gt_segments_left (Optional[int]): Number of ground truth segments on the left side.&#10;        n_pred_segments_left (Optional[int]): Number of predicted segments on the left side.&#10;        n_gt_segments_right (Optional[int]): Number of ground truth segments on the right side.&#10;        n_pred_segments_right (Optional[int]): Number of predicted segments on the right side.&#10;        n_segments_success (Optional[int]): Number of successfully matched segments.&#10;        center_gt_left (Optional[Tuple[float, float]]): Center coordinates of ground truth segment (left).&#10;        center_gt_right (Optional[Tuple[float, float]]): Center coordinates of ground truth segment (right).&#10;        center_pred_left (Optional[Tuple[float, float]]): Center coordinates of predicted segment (left).&#10;        center_pred_right (Optional[Tuple[float, float]]): Center coordinates of predicted segment (right).&#10;        center_pred_success (Optional[int]): Number of successful center predictions.&#10;        center_diers_left (Optional[Tuple[float, float]]): Center coordinates from Diers system (left).&#10;        center_diers_right (Optional[Tuple[float, float]]): Center coordinates from Diers system (right).&#10;        center_diers_success (Optional[int]): Number of successful Diers center matches.&#10;        center_angle_error (Optional[float]): Error in center angle (in degrees or radians).&#10;        center_angle_error_abs (Optional[float]): Absolute error in center angle.&#10;        center_angle_success (Optional[int]): Number of successful center angle predictions.&#10;        center_angle_diers_error (Optional[float]): Error in Diers center angle.&#10;        center_angle_diers_error_abs (Optional[float]): Absolute error in Diers center angle.&#10;        center_angle_diers_success (Optional[int]): Number of successful Diers center angle predictions.&#10;    &quot;&quot;&quot;&#10;    tp: Optional[int] = None&#10;    fp: Optional[int] = None&#10;    fn: Optional[int] = None&#10;    dice: Optional[float] = None&#10;    precision: Optional[float] = None&#10;    recall: Optional[float] = None&#10;    n_gt_segments: Optional[int] = None&#10;    n_pred_segments: Optional[int] = None&#10;    n_gt_segments_left: Optional[int] = None&#10;    n_pred_segments_left: Optional[int] = None&#10;    n_gt_segments_right: Optional[int] = None&#10;    n_pred_segments_right: Optional[int] = None&#10;    n_segments_success: Optional[int] = None&#10;    center_gt_left: Optional[Tuple[float, float]] = None&#10;    center_gt_right: Optional[Tuple[float, float]] = None&#10;    center_pred_left: Optional[Tuple[float, float]] = None&#10;    center_pred_right: Optional[Tuple[float, float]] = None&#10;    center_pred_success: Optional[int] = None&#10;    center_diers_left: Optional[Tuple[float, float]] = None&#10;    center_diers_right: Optional[Tuple[float, float]] = None&#10;    center_diers_success: Optional[int] = None&#10;    center_angle_error: Optional[float] = None&#10;    center_angle_error_abs: Optional[float] = None&#10;    center_angle_success: Optional[int] = None&#10;    center_angle_diers_error: Optional[float] = None&#10;    center_angle_diers_error_abs: Optional[float] = None&#10;    center_angle_diers_success: Optional[int] = None" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/evaluation/evaluator.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/evaluation/evaluator.py" />
              <option name="originalContent" value="import csv&#10;import os&#10;import re&#10;from collections import defaultdict&#10;from pathlib import Path&#10;from typing import Optional, Tuple&#10;&#10;import matplotlib.pyplot as plt&#10;import numpy as np&#10;from skimage import io&#10;from tqdm import tqdm&#10;&#10;&#10;class Evaluator:&#10;    &quot;&quot;&quot;&#10;    Evaluator class for validating segmentation predictions against ground truth masks.&#10;    Computes various metrics, saves results, and provides visualization utilities.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, ground_truth_dir, output_dir, metrics):&#10;        &quot;&quot;&quot;&#10;        Initialize Evaluator.&#10;        Parameters:&#10;            ground_truth_dir (str): Path to ground truth masks.&#10;            output_dir (str): Directory to save CSV results.&#10;            metrics (list[Metric], optional): List of metric instances to compute. If None, auto-discover.&#10;        &quot;&quot;&quot;&#10;        self.ground_truth_dir = ground_truth_dir&#10;        self.ground_truths = self.load_masks(ground_truth_dir)&#10;        self.output_dir = output_dir&#10;        self.vp_dm_distances = self.load_vp_dm_distances()&#10;        self.metrics = metrics&#10;        self.health_status_dict = self.load_health_status_dict()&#10;&#10;&#10;    def evaluate(self, predictions_dir):&#10;        &quot;&quot;&quot;&#10;        Evaluate predictions in a directory against ground truth masks.&#10;        Computes metrics for each image and saves per-image and mean results.&#10;        Parameters:&#10;            predictions_dir (str): Directory containing predicted mask images.&#10;        &quot;&quot;&quot;&#10;        # Load ground truth and prediction masks&#10;        ground_truths = self.ground_truths&#10;        predictions = self.load_masks(predictions_dir)&#10;&#10;        # Prepare data structures for computed_metric_results&#10;        metric_scores = []&#10;        metric_scores_by_dataset = defaultdict(list)&#10;        metric_scores_by_health_status = defaultdict(list)&#10;&#10;        # Iterate over all ground truth files and validate predictions&#10;        for file_name in tqdm(ground_truths.keys(), desc=f'Validating predictions for {predictions_dir}'):&#10;            if file_name not in predictions.keys():&#10;                print(f&quot;Ground truth {file_name} does not have a corresponding prediction.&quot;)&#10;                continue&#10;&#10;            # Load ground truth and prediction masks for the current image&#10;            dataset = self.parse_dataset(file_name)&#10;            gt = ground_truths[file_name] &gt; 0  # binary mask&#10;            pred = predictions[file_name] &gt; 0  # binary mask&#10;&#10;            # Compute TP, FP, FN as base metrics&#10;            computed_metric_results = {&#10;                'TP': np.logical_and(gt, pred).sum(),&#10;                'FP': np.logical_and(~gt, pred).sum(),&#10;                'FN': np.logical_and(gt, ~pred).sum()&#10;            }&#10;&#10;            # Compute image metadata&#10;            image_metadata = self.load_image_metadata(file_name)&#10;&#10;            # Compute metrics sequentially, allowing composite metrics to use previous computed_metric_results&#10;            for m in self.metrics:&#10;                computed_metric_results[m.name] = m.compute(gt, pred, computed_metric_results, image_metadata)&#10;            # Collect metric values (keep infinities for per-image output)&#10;            values = [computed_metric_results[m.name] for m in self.metrics]&#10;&#10;            # Determine health status for the patient from the patient index&#10;            pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;            sick = self.health_status_dict.get(pat_idx)&#10;&#10;            # Add metric scores for this file to the overall list&#10;            metric_scores.append([dataset, file_name, sick] + values)&#10;&#10;            # Add metric scores for this file to the dataset-specific list&#10;            metric_scores_by_dataset[dataset].append(values)&#10;&#10;            # Add metric scores for this file to the health status-specific list&#10;            if sick == 1.0:&#10;                metric_scores_by_health_status['Sick'].append(values)&#10;            elif sick == 0.0:&#10;                metric_scores_by_health_status['Healthy'].append(values)&#10;&#10;        # prepare output directories&#10;        all_csv, mean_csv = self.create_output_files(predictions_dir)&#10;        # write all per-file metric_scores&#10;        self.save_per_image_metrics(all_csv, metric_scores)&#10;        # Read duration metric_scores&#10;        avg_image_duration, total_duration = self.read_segmentation_duration(predictions_dir)&#10;        # Write mean metric_scores grouped by dataset, by sick status and overall&#10;        self.save_mean_metrics(avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                               total_duration)&#10;&#10;&#10;    def load_image_metadata(self, file_name):&#10;        &quot;&quot;&quot;&#10;        Load image metadata including marker positions and pixel size in mm.&#10;        Parameters:&#10;            file_name (str): Name of the image file.&#10;        Returns:&#10;            dict: Dictionary with marker positions and pixel size in mm.&#10;        &quot;&quot;&quot;&#10;        # Load markers&#10;        vp, dm, dl_diers, dr_diers = self.load_markers(file_name)&#10;&#10;        # Calc pixel ratio in mm&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        pixel_size = self.compute_distance_per_pixel(pat_idx, vp, dm)&#10;&#10;        return {&#10;            'vp': vp,&#10;            'dm': dm,&#10;            'dl_diers': dl_diers,&#10;            'dr_diers': dr_diers,&#10;            'pixel_size_mm': pixel_size&#10;        }&#10;&#10;&#10;    def load_markers(self, file_name, markers_file=&quot;data/Info_sheets/Markerpositionen.csv&quot;):&#10;        &quot;&quot;&quot;&#10;        Load marker positions for a given image file.&#10;        Tries to find markers by image number, then by patient index.&#10;        Parameters:&#10;            file_name (str): Name of the image file.&#10;            markers_file (str): Path to the marker CSV file.&#10;        Returns:&#10;            tuple: Marker positions (vp, dm, dl_diers, dr_diers) or (None, None, None, None) if not found.&#10;        &quot;&quot;&quot;&#10;        # Check if markers are available for this image&#10;        img_number = self.extract_image_number(file_name)&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile, delimiter=';')&#10;            for row in reader:&#10;                if row.get('BildID', '').startswith(str(img_number)):&#10;                    vp = (int(row['X_VP']) / 10, int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM']) / 10, int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # Check if markers are available for this patient&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        with open('data/Info_Sheets/All_Data_Renamed_overview.csv', 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('Patientenindex') == pat_idx:&#10;                    measure_id = row['DIERS_Mess-ID']&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('MessID', '') == measure_id:&#10;                    vp = (int(row['X_VP'] / 10), int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM'] / 10), int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # If no markers found, return None&#10;        return None, None, None, None&#10;&#10;&#10;    def save_mean_metrics(self, avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                          total_duration):&#10;        &quot;&quot;&quot;&#10;        Save mean metrics to a CSV file, grouped by dataset, health status, and overall.&#10;        Parameters:&#10;            avg_image_duration (str): Average duration per image.&#10;            mean_csv (str): Path to the mean CSV file.&#10;            metric_scores_by_dataset (dict): Metrics grouped by dataset.&#10;            metric_scores_by_health_status (dict): Metrics grouped by health status.&#10;            total_duration (str): Total duration for all images.&#10;        &quot;&quot;&quot;&#10;        with open(mean_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header_mean = ['Dataset'] + [m.name for m in self.metrics] + ['Total duration',&#10;                                                                          'Average duration per image']&#10;            writer.writerow(header_mean)&#10;            # per-dataset means&#10;            for ds, lst in metric_scores_by_dataset.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([ds] + means + ['', ''])&#10;            # by sick status&#10;            for label, lst in metric_scores_by_health_status.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([label] + means + ['', ''])&#10;            # overall&#10;            all_vals = [v for lst in metric_scores_by_dataset.values() for v in lst]&#10;            means = [_safe_nanmean([row[i] for row in all_vals]) for i in range(len(self.metrics))]&#10;            writer.writerow(['All Datasets'] + means + [total_duration, avg_image_duration])&#10;        print(f&quot;Mean validation results saved to {mean_csv}&quot;)&#10;&#10;&#10;    @staticmethod&#10;    def read_segmentation_duration(predictions_dir):&#10;        &quot;&quot;&quot;&#10;        Read segmentation duration information from a duration.txt file in the predictions directory.&#10;        Parameters:&#10;            predictions_dir (str): Directory containing the duration.txt file.&#10;        Returns:&#10;            tuple: (avg_image_duration, total_duration) as strings, or empty strings if not found.&#10;        &quot;&quot;&quot;&#10;        _duration_file = Path(predictions_dir) / 'duration.txt'&#10;        if _duration_file.exists():&#10;            with open(_duration_file, 'r') as _df:&#10;                _lines = _df.readlines()&#10;            if len(_lines) &gt;= 2:&#10;                total_duration = _lines[0].split(':', 1)[1].strip()&#10;                avg_image_duration = _lines[1].split(':', 1)[1].strip()&#10;            else:&#10;                total_duration = ''&#10;                avg_image_duration = ''&#10;        else:&#10;            total_duration = ''&#10;            avg_image_duration = ''&#10;        return avg_image_duration, total_duration&#10;&#10;&#10;    def save_per_image_metrics(self, all_csv, metric_scores):&#10;        &quot;&quot;&quot;&#10;        Save per-image metric scores to a CSV file.&#10;        Parameters:&#10;            all_csv (str): Path to the CSV file.&#10;            metric_scores (list): List of per-image metric scores.&#10;        &quot;&quot;&quot;&#10;        with open(all_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header = ['Dataset', 'File Name', 'Sick'] + [m.name for m in self.metrics]&#10;            writer.writerow(header)&#10;            writer.writerows(metric_scores)&#10;        print(f&quot;Validation results saved to {all_csv}&quot;)&#10;&#10;&#10;    def create_output_files(self, predictions_dir):&#10;        &quot;&quot;&quot;&#10;        Create output file paths for per-image and mean metrics CSVs.&#10;        Parameters:&#10;            predictions_dir (str): Directory containing predictions.&#10;        Returns:&#10;            tuple: (all_csv, mean_csv) file paths.&#10;        &quot;&quot;&quot;&#10;        output_dir = Path(self.output_dir)&#10;        output_dir.mkdir(parents=True, exist_ok=True)&#10;        run_name = os.path.basename(predictions_dir)&#10;        all_csv = output_dir / f&quot;{run_name}_all.csv&quot;&#10;        mean_csv = output_dir / f&quot;{run_name}_mean.csv&quot;&#10;        return all_csv, mean_csv&#10;&#10;&#10;    @staticmethod&#10;    def parse_dataset(file_name):&#10;        &quot;&quot;&quot;&#10;        Parse the dataset name from a file name.&#10;        Parameters:&#10;            file_name (str): Name of the file.&#10;        Returns:&#10;            str: Dataset prefix.&#10;        &quot;&quot;&quot;&#10;        prefix = file_name.split('_')[0]&#10;        return prefix&#10;&#10;&#10;    @staticmethod&#10;    def load_masks(segmentations_dir):&#10;        &quot;&quot;&quot;&#10;        Load mask images from a directory.&#10;        Only files ending with .png and containing '-mask' are loaded.&#10;        Parameters:&#10;            segmentations_dir (str): Directory containing mask images.&#10;        Returns:&#10;            dict: Mapping from file name to mask image (numpy array).&#10;        &quot;&quot;&quot;&#10;        segmentations = {}&#10;        for file in os.listdir(segmentations_dir):&#10;            if file.endswith(&quot;.png&quot;) and &quot;-mask&quot; in file:&#10;                img = io.imread(os.path.join(segmentations_dir, file))&#10;                # If RGB, convert to grayscale&#10;                if img.ndim == 3:&#10;                    img = img[..., 0]  # Use first channel (or np.mean(img, axis=2) for true grayscale)&#10;                segmentations[file] = img&#10;        return segmentations&#10;&#10;&#10;    @staticmethod&#10;    def parse_patient_index_from_image_path(image_path):&#10;        &quot;&quot;&quot;&#10;        Parse the patient index from an image file path.&#10;        Parameters:&#10;            image_path (str): Path or name of the image file.&#10;        Returns:&#10;            str or None: Patient index if found, else None.&#10;        &quot;&quot;&quot;&#10;        pattern = re.compile(r'^[^_]+_([^_]+(?:_\d+)+)(?=_\d{9,})')&#10;        match = pattern.search(os.path.basename(image_path))&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def extract_image_number(file_name):&#10;        &quot;&quot;&quot;&#10;        Extract the image number from a file name.&#10;        Parameters:&#10;            file_name (str): Name of the file.&#10;        Returns:&#10;            str or None: Image number if found, else None.&#10;        &quot;&quot;&quot;&#10;        match = re.search(r'_(\d+)-mask\.Gauss\.png$', file_name)&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def visualize_mask(image: np.ndarray, title: str):&#10;        &quot;&quot;&quot;&#10;        Display the original mask image without any splitting lines or axes.&#10;        Parameters:&#10;            image (np.ndarray): The mask image to display.&#10;            title (str): Title for the plot.&#10;        &quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # display mask or full-color image&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    @staticmethod&#10;    def visualize_middle_line(image: np.ndarray, vp: Optional[Tuple[int, int]], dm: Optional[Tuple[int, int]], title: str):&#10;        &quot;&quot;&quot;&#10;        Display the mask image with a middle splitting line between VP and DM markers.&#10;        Parameters:&#10;            image (np.ndarray): The mask image to display.&#10;            vp (tuple or None): Coordinates of the VP marker.&#10;            dm (tuple or None): Coordinates of the DM marker.&#10;            title (str): Title for the plot.&#10;        &quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # if image has 3 or more channels, show in RGB, else grayscale&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;&#10;        if vp is not None and dm is not None:&#10;            # Draw the splitting line in the middle of the two markers&#10;            xs = [vp[0], dm[0]]&#10;            ys = [vp[1], dm[1]]&#10;            ax.plot(xs, ys, color='magenta', linewidth=2)&#10;&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    def compute_distance_per_pixel(self, patient_idx, vp, dm):&#10;        &quot;&quot;&quot;&#10;        Compute millimeters per pixel using known physical distance between VP and DM markers.&#10;        Parameters:&#10;            patient_idx (str): Patient index.&#10;            vp (tuple): Coordinates of the VP marker.&#10;            dm (tuple): Coordinates of the DM marker.&#10;        Returns:&#10;            float or None: Millimeters per pixel, or None if not computable.&#10;        &quot;&quot;&quot;&#10;        if vp is None or dm is None:&#10;            return None&#10;        pixel_dist = np.hypot(dm[0] - vp[0], dm[1] - vp[1])&#10;        if pixel_dist == 0:&#10;            return None&#10;        mm_dist = self.vp_dm_distances.get(patient_idx)&#10;        if mm_dist is None:&#10;            return None&#10;        return mm_dist / pixel_dist&#10;&#10;&#10;    @staticmethod&#10;    def load_health_status_dict(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        &quot;&quot;&quot;&#10;        Load the mapping from patient index to health status (sick/healthy).&#10;        Parameters:&#10;            file_path (str): Path to the CSV file with health status information.&#10;        Returns:&#10;            dict: Mapping from patient index to health status (float).&#10;        &quot;&quot;&quot;&#10;        health_status_dict = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                sick = row.get('Krank')&#10;                if pat_idx and sick:&#10;                    health_status_dict[pat_idx] = float(sick)&#10;        return health_status_dict&#10;&#10;&#10;    @staticmethod&#10;    def load_vp_dm_distances(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        &quot;&quot;&quot;&#10;        Load the mapping from patient index to VP-DM marker distance in millimeters.&#10;        Parameters:&#10;            file_path (str): Path to the CSV file with distance information.&#10;        Returns:&#10;            dict: Mapping from patient index to VP-DM distance (float, mm).&#10;        &quot;&quot;&quot;&#10;        vp_dm_distance_map = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                dist_str = row.get('Rumpflänge')&#10;                if pat_idx and dist_str:&#10;                    try:&#10;                        dist_mm = float(dist_str)&#10;                        vp_dm_distance_map[pat_idx] = dist_mm&#10;                    except ValueError:&#10;                        continue&#10;        return vp_dm_distance_map&#10;&#10;&#10;def _safe_nanmean(arr):&#10;    &quot;&quot;&quot;&#10;    Compute the mean of an array, ignoring None, NaN, and infinite values.&#10;    Parameters:&#10;        arr (list): List of values.&#10;    Returns:&#10;        float or None: Mean of valid values, or None if no valid values exist.&#10;    &quot;&quot;&quot;&#10;    # filter out None, NaN, and infinite values&#10;    clean = [v for v in arr if v is not None and np.isfinite(v)]&#10;    if len(clean) == 0:&#10;        return None&#10;    return float(np.mean(clean))&#10;" />
              <option name="updatedContent" value="import csv&#10;import os&#10;import re&#10;from collections import defaultdict&#10;from pathlib import Path&#10;from typing import Optional, Tuple&#10;&#10;import matplotlib.pyplot as plt&#10;import numpy as np&#10;from skimage import io&#10;from tqdm import tqdm&#10;&#10;&#10;class Evaluator:&#10;    &quot;&quot;&quot;&#10;    Evaluator class for validating segmentation predictions against ground truth masks.&#10;    Computes various metrics, saves results, and provides visualization utilities.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, ground_truth_dir, output_dir, metrics):&#10;        &quot;&quot;&quot;&#10;        Initialize Evaluator.&#10;        Parameters:&#10;            ground_truth_dir (str): Path to ground truth masks.&#10;            output_dir (str): Directory to save CSV results.&#10;            metrics (list[Metric], optional): List of metric instances to compute. If None, auto-discover.&#10;        &quot;&quot;&quot;&#10;        self.ground_truth_dir = ground_truth_dir&#10;        self.ground_truths = self.load_masks(ground_truth_dir)&#10;        self.output_dir = output_dir&#10;        self.vp_dm_distances = self.load_vp_dm_distances()&#10;        self.metrics = metrics&#10;        self.health_status_dict = self.load_health_status_dict()&#10;&#10;&#10;    def evaluate(self, predictions_dir):&#10;        &quot;&quot;&quot;&#10;        Evaluate predictions in a directory against ground truth masks.&#10;        Computes metrics for each image and saves per-image and mean results.&#10;        Parameters:&#10;            predictions_dir (str): Directory containing predicted mask images.&#10;        &quot;&quot;&quot;&#10;        # Load ground truth and prediction masks&#10;        ground_truths = self.ground_truths&#10;        predictions = self.load_masks(predictions_dir)&#10;&#10;        # Prepare data structures for computed_metric_results&#10;        metric_scores = []&#10;        metric_scores_by_dataset = defaultdict(list)&#10;        metric_scores_by_health_status = defaultdict(list)&#10;&#10;        # Iterate over all ground truth files and validate predictions&#10;        for file_name in tqdm(ground_truths.keys(), desc=f'Validating predictions for {predictions_dir}'):&#10;            if file_name not in predictions.keys():&#10;                print(f&quot;Ground truth {file_name} does not have a corresponding prediction.&quot;)&#10;                continue&#10;&#10;            # Load ground truth and prediction masks for the current image&#10;            dataset = self.parse_dataset(file_name)&#10;            gt = ground_truths[file_name] &gt; 0  # binary mask&#10;            pred = predictions[file_name] &gt; 0  # binary mask&#10;&#10;            # Compute TP, FP, FN as base metrics&#10;            computed_metric_results = {&#10;                'TP': np.logical_and(gt, pred).sum(),&#10;                'FP': np.logical_and(~gt, pred).sum(),&#10;                'FN': np.logical_and(gt, ~pred).sum()&#10;            }&#10;&#10;            # Compute image metadata&#10;            image_metadata = self.load_image_metadata(file_name)&#10;&#10;            # Compute metrics sequentially, allowing composite metrics to use previous computed_metric_results&#10;            for m in self.metrics:&#10;                computed_metric_results[m.name] = m.compute(gt, pred, computed_metric_results, image_metadata)&#10;            # Collect metric values (keep infinities for per-image output)&#10;            values = [computed_metric_results[m.name] for m in self.metrics]&#10;&#10;            # Determine health status for the patient from the patient index&#10;            pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;            sick = self.health_status_dict.get(pat_idx)&#10;&#10;            # Add metric scores for this file to the overall list&#10;            metric_scores.append([dataset, file_name, sick] + values)&#10;&#10;            # Add metric scores for this file to the dataset-specific list&#10;            metric_scores_by_dataset[dataset].append(values)&#10;&#10;            # Add metric scores for this file to the health status-specific list&#10;            if sick == 1.0:&#10;                metric_scores_by_health_status['Sick'].append(values)&#10;            elif sick == 0.0:&#10;                metric_scores_by_health_status['Healthy'].append(values)&#10;&#10;        # prepare output directories&#10;        all_csv, mean_csv = self.create_output_files(predictions_dir)&#10;        # write all per-file metric_scores&#10;        self.save_per_image_metrics(all_csv, metric_scores)&#10;        # Read duration metric_scores&#10;        avg_image_duration, total_duration = self.read_segmentation_duration(predictions_dir)&#10;        # Write mean metric_scores grouped by dataset, by sick status and overall&#10;        self.save_mean_metrics(avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                               total_duration)&#10;&#10;&#10;    def load_image_metadata(self, file_name):&#10;        &quot;&quot;&quot;&#10;        Load image metadata including marker positions and pixel size in mm.&#10;        Parameters:&#10;            file_name (str): Name of the image file.&#10;        Returns:&#10;            dict: Dictionary with marker positions and pixel size in mm.&#10;        &quot;&quot;&quot;&#10;        # Load markers&#10;        vp, dm, dl_diers, dr_diers = self.load_markers(file_name)&#10;&#10;        # Calc pixel ratio in mm&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        pixel_size = self.compute_distance_per_pixel(pat_idx, vp, dm)&#10;&#10;        return {&#10;            'vp': vp,&#10;            'dm': dm,&#10;            'dl_diers': dl_diers,&#10;            'dr_diers': dr_diers,&#10;            'pixel_size_mm': pixel_size&#10;        }&#10;&#10;&#10;    def load_markers(self, file_name, markers_file=&quot;data/Info_sheets/Markerpositionen.csv&quot;):&#10;        &quot;&quot;&quot;&#10;        Load marker positions for a given image file.&#10;        Tries to find markers by image number, then by patient index.&#10;        Parameters:&#10;            file_name (str): Name of the image file.&#10;            markers_file (str): Path to the marker CSV file.&#10;        Returns:&#10;            tuple: Marker positions (vp, dm, dl_diers, dr_diers) or (None, None, None, None) if not found.&#10;        &quot;&quot;&quot;&#10;        # Check if markers are available for this image&#10;        img_number = self.extract_image_number(file_name)&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile, delimiter=';')&#10;            for row in reader:&#10;                if row.get('BildID', '').startswith(str(img_number)):&#10;                    vp = (int(row['X_VP']) / 10, int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM']) / 10, int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # Check if markers are available for this patient&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        with open('data/Info_Sheets/All_Data_Renamed_overview.csv', 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('Patientenindex') == pat_idx:&#10;                    measure_id = row['DIERS_Mess-ID']&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('MessID', '') == measure_id:&#10;                    vp = (int(row['X_VP'] / 10), int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM'] / 10), int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # If no markers found, return None&#10;        return None, None, None, None&#10;&#10;&#10;    def save_mean_metrics(self, avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                          total_duration):&#10;        &quot;&quot;&quot;&#10;        Save mean metrics to a CSV file, grouped by dataset, health status, and overall.&#10;        Parameters:&#10;            avg_image_duration (str): Average duration per image.&#10;            mean_csv (str): Path to the mean CSV file.&#10;            metric_scores_by_dataset (dict): Metrics grouped by dataset.&#10;            metric_scores_by_health_status (dict): Metrics grouped by health status.&#10;            total_duration (str): Total duration for all images.&#10;        &quot;&quot;&quot;&#10;        with open(mean_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header_mean = ['Dataset'] + [m.name for m in self.metrics] + ['Total duration',&#10;                                                                          'Average duration per image']&#10;            writer.writerow(header_mean)&#10;            # per-dataset means&#10;            for ds, lst in metric_scores_by_dataset.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([ds] + means + ['', ''])&#10;            # by sick status&#10;            for label, lst in metric_scores_by_health_status.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([label] + means + ['', ''])&#10;            # overall&#10;            all_vals = [v for lst in metric_scores_by_dataset.values() for v in lst]&#10;            means = [_safe_nanmean([row[i] for row in all_vals]) for i in range(len(self.metrics))]&#10;            writer.writerow(['All Datasets'] + means + [total_duration, avg_image_duration])&#10;        print(f&quot;Mean validation results saved to {mean_csv}&quot;)&#10;&#10;&#10;    @staticmethod&#10;    def read_segmentation_duration(predictions_dir):&#10;        &quot;&quot;&quot;&#10;        Read segmentation duration information from a duration.txt file in the predictions directory.&#10;        Parameters:&#10;            predictions_dir (str): Directory containing the duration.txt file.&#10;        Returns:&#10;            tuple: (avg_image_duration, total_duration) as strings, or empty strings if not found.&#10;        &quot;&quot;&quot;&#10;        _duration_file = Path(predictions_dir) / 'duration.txt'&#10;        if _duration_file.exists():&#10;            with open(_duration_file, 'r') as _df:&#10;                _lines = _df.readlines()&#10;            if len(_lines) &gt;= 2:&#10;                total_duration = _lines[0].split(':', 1)[1].strip()&#10;                avg_image_duration = _lines[1].split(':', 1)[1].strip()&#10;            else:&#10;                total_duration = ''&#10;                avg_image_duration = ''&#10;        else:&#10;            total_duration = ''&#10;            avg_image_duration = ''&#10;        return avg_image_duration, total_duration&#10;&#10;&#10;    def save_per_image_metrics(self, all_csv, metric_scores):&#10;        &quot;&quot;&quot;&#10;        Save per-image metric scores to a CSV file.&#10;        Parameters:&#10;            all_csv (str): Path to the CSV file.&#10;            metric_scores (list): List of per-image metric scores.&#10;        &quot;&quot;&quot;&#10;        with open(all_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header = ['Dataset', 'File Name', 'Sick'] + [m.name for m in self.metrics]&#10;            writer.writerow(header)&#10;            writer.writerows(metric_scores)&#10;        print(f&quot;Validation results saved to {all_csv}&quot;)&#10;&#10;&#10;    def create_output_files(self, predictions_dir):&#10;        &quot;&quot;&quot;&#10;        Create output file paths for per-image and mean metrics CSVs.&#10;        Parameters:&#10;            predictions_dir (str): Directory containing predictions.&#10;        Returns:&#10;            tuple: (all_csv, mean_csv) file paths.&#10;        &quot;&quot;&quot;&#10;        output_dir = Path(self.output_dir)&#10;        output_dir.mkdir(parents=True, exist_ok=True)&#10;        run_name = os.path.basename(predictions_dir)&#10;        all_csv = output_dir / f&quot;{run_name}_all.csv&quot;&#10;        mean_csv = output_dir / f&quot;{run_name}_mean.csv&quot;&#10;        return all_csv, mean_csv&#10;&#10;&#10;    @staticmethod&#10;    def parse_dataset(file_name):&#10;        &quot;&quot;&quot;&#10;        Parse the dataset name from a file name.&#10;        Parameters:&#10;            file_name (str): Name of the file.&#10;        Returns:&#10;            str: Dataset prefix.&#10;        &quot;&quot;&quot;&#10;        prefix = file_name.split('_')[0]&#10;        return prefix&#10;&#10;&#10;    @staticmethod&#10;    def load_masks(segmentations_dir):&#10;        &quot;&quot;&quot;&#10;        Load mask images from a directory.&#10;        Only files ending with .png and containing '-mask' are loaded.&#10;        Parameters:&#10;            segmentations_dir (str): Directory containing mask images.&#10;        Returns:&#10;            dict: Mapping from file name to mask image (numpy array).&#10;        &quot;&quot;&quot;&#10;        segmentations = {}&#10;        for file in os.listdir(segmentations_dir):&#10;            if file.endswith(&quot;.png&quot;) and &quot;-mask&quot; in file:&#10;                img = io.imread(os.path.join(segmentations_dir, file))&#10;                # If RGB, convert to grayscale&#10;                if img.ndim == 3:&#10;                    img = img[..., 0]  # Use first channel (or np.mean(img, axis=2) for true grayscale)&#10;                segmentations[file] = img&#10;        return segmentations&#10;&#10;&#10;    @staticmethod&#10;    def parse_patient_index_from_image_path(image_path):&#10;        &quot;&quot;&quot;&#10;        Parse the patient index from an image file path.&#10;        Parameters:&#10;            image_path (str): Path or name of the image file.&#10;        Returns:&#10;            str or None: Patient index if found, else None.&#10;        &quot;&quot;&quot;&#10;        pattern = re.compile(r'^[^_]+_([^_]+(?:_\d+)+)(?=_\d{9,})')&#10;        match = pattern.search(os.path.basename(image_path))&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def extract_image_number(file_name):&#10;        &quot;&quot;&quot;&#10;        Extract the image number from a file name.&#10;        Parameters:&#10;            file_name (str): Name of the file.&#10;        Returns:&#10;            str or None: Image number if found, else None.&#10;        &quot;&quot;&quot;&#10;        match = re.search(r'_(\d+)-mask\.Gauss\.png$', file_name)&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def visualize_mask(image: np.ndarray, title: str):&#10;        &quot;&quot;&quot;&#10;        Display the original mask image without any splitting lines or axes.&#10;        Parameters:&#10;            image (np.ndarray): The mask image to display.&#10;            title (str): Title for the plot.&#10;        &quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # display mask or full-color image&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    @staticmethod&#10;    def visualize_middle_line(image: np.ndarray, vp: Optional[Tuple[int, int]], dm: Optional[Tuple[int, int]], title: str):&#10;        &quot;&quot;&quot;&#10;        Display the mask image with a middle splitting line between VP and DM markers.&#10;        Parameters:&#10;            image (np.ndarray): The mask image to display.&#10;            vp (tuple or None): Coordinates of the VP marker.&#10;            dm (tuple or None): Coordinates of the DM marker.&#10;            title (str): Title for the plot.&#10;        &quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # if image has 3 or more channels, show in RGB, else grayscale&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;&#10;        if vp is not None and dm is not None:&#10;            # Draw the splitting line in the middle of the two markers&#10;            xs = [vp[0], dm[0]]&#10;            ys = [vp[1], dm[1]]&#10;            ax.plot(xs, ys, color='magenta', linewidth=2)&#10;&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    def compute_distance_per_pixel(self, patient_idx, vp, dm):&#10;        &quot;&quot;&quot;&#10;        Compute millimeters per pixel using known physical distance between VP and DM markers.&#10;        Parameters:&#10;            patient_idx (str): Patient index.&#10;            vp (tuple): Coordinates of the VP marker.&#10;            dm (tuple): Coordinates of the DM marker.&#10;        Returns:&#10;            float or None: Millimeters per pixel, or None if not computable.&#10;        &quot;&quot;&quot;&#10;        if vp is None or dm is None:&#10;            return None&#10;        pixel_dist = np.hypot(dm[0] - vp[0], dm[1] - vp[1])&#10;        if pixel_dist == 0:&#10;            return None&#10;        mm_dist = self.vp_dm_distances.get(patient_idx)&#10;        if mm_dist is None:&#10;            return None&#10;        return mm_dist / pixel_dist&#10;&#10;&#10;    @staticmethod&#10;    def load_health_status_dict(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        &quot;&quot;&quot;&#10;        Load the mapping from patient index to health status (sick/healthy).&#10;        Parameters:&#10;            file_path (str): Path to the CSV file with health status information.&#10;        Returns:&#10;            dict: Mapping from patient index to health status (float).&#10;        &quot;&quot;&quot;&#10;        health_status_dict = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                sick = row.get('Krank')&#10;                if pat_idx and sick:&#10;                    health_status_dict[pat_idx] = float(sick)&#10;        return health_status_dict&#10;&#10;&#10;    @staticmethod&#10;    def load_vp_dm_distances(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        &quot;&quot;&quot;&#10;        Load the mapping from patient index to VP-DM marker distance in millimeters.&#10;        Parameters:&#10;            file_path (str): Path to the CSV file with distance information.&#10;        Returns:&#10;            dict: Mapping from patient index to VP-DM distance (float, mm).&#10;        &quot;&quot;&quot;&#10;        vp_dm_distance_map = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                dist_str = row.get('Rumpflänge')&#10;                if pat_idx and dist_str:&#10;                    try:&#10;                        dist_mm = float(dist_str)&#10;                        vp_dm_distance_map[pat_idx] = dist_mm&#10;                    except ValueError:&#10;                        continue&#10;        return vp_dm_distance_map&#10;&#10;&#10;def _safe_nanmean(arr):&#10;    &quot;&quot;&quot;&#10;    Compute the mean of an array, ignoring None, NaN, and infinite values.&#10;    Parameters:&#10;        arr (list): List of values.&#10;    Returns:&#10;        float or None: Mean of valid values, or None if no valid values exist.&#10;    &quot;&quot;&quot;&#10;    # filter out None, NaN, and infinite values&#10;    clean = [v for v in arr if v is not None and np.isfinite(v)]&#10;    if len(clean) == 0:&#10;        return None&#10;    return float(np.mean(clean))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/postprocessing/color_patch_refiner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/postprocessing/color_patch_refiner.py" />
              <option name="originalContent" value="import numpy as np&#10;from scipy.ndimage import binary_propagation&#10;&#10;from postprocessing.segmentation_refiner import ISegmentationRefiner&#10;&#10;&#10;class ColorPatchRefiner(ISegmentationRefiner):&#10;    &quot;&quot;&quot;&#10;    Refines segmentation masks by growing regions within a specified color range using region growing (binary propagation).&#10;    The color range is defined by a color preprocessor. The refinement ensures that the segmentation includes contiguous&#10;    regions matching the color mask, starting from the intersection of the original segmentation and the color mask.&#10;&#10;    Args:&#10;        color_preprocessor: An instance of a color preprocessor providing a preprocess_image method for color masking.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, color_preprocessor):&#10;        &quot;&quot;&quot;&#10;        Initialize the ColorPatchRefiner.&#10;        Args:&#10;            color_preprocessor: Preprocessor instance for extracting the relevant color mask from images.&#10;        &quot;&quot;&quot;&#10;        self.color_preprocessor = color_preprocessor&#10;&#10;    def refine(self, target_segmentation, target_image):&#10;        &quot;&quot;&quot;&#10;        Refine a segmentation mask by growing regions within the color mask using region growing.&#10;        Args:&#10;            target_segmentation (np.ndarray): The initial segmentation mask (2D or 3D array).&#10;            target_image: An object with an 'image' attribute (the original image as np.ndarray).&#10;        Returns:&#10;            np.ndarray: The refined segmentation mask, same shape as input.&#10;        &quot;&quot;&quot;&#10;        if target_segmentation.max() == 0:&#10;            return target_segmentation&#10;&#10;        # Get the color range mask and set it to 3 channels (0/1)&#10;        target_color_mask, _ = self.color_preprocessor.preprocess_image(target_image.image)&#10;        orig_colors = target_image.image[..., :3]&#10;        colored_mask = np.zeros_like(orig_colors)&#10;        mask_bool = target_color_mask &gt; 0&#10;        colored_mask[mask_bool] = orig_colors[mask_bool]&#10;&#10;        target_color_mask = (np.repeat(target_color_mask[:, :, np.newaxis], 3, axis=2) // 255).astype(np.uint8)&#10;&#10;        # 2D boolean masks&#10;        if target_segmentation.ndim == 2:&#10;            target_segmentation_bool = target_segmentation &gt; 0  # [H,W]&#10;            was_2d = True&#10;            C_in = 1&#10;        elif target_segmentation.ndim == 3:&#10;            was_2d = False&#10;            C_in = target_segmentation.shape[2]&#10;            if C_in == 1:&#10;                target_segmentation_bool = target_segmentation[..., 0] &gt; 0&#10;            else:&#10;                target_segmentation_bool = np.any(target_segmentation &gt; 0, axis=2)  # [H,W]&#10;        else:&#10;            raise ValueError(f&quot;Unexpected target_segmentation shape: {target_segmentation.shape}&quot;)&#10;&#10;        target_color_bool = np.any(target_color_mask &gt; 0, axis=2)  # [H,W]&#10;&#10;        # Starting points: Intersection&#10;        seed = target_segmentation_bool &amp; target_color_bool&#10;&#10;        # Region growing within target_color_bool (8-neighborhood)&#10;        grown_region = binary_propagation(seed, mask=target_color_bool, structure=np.ones((3, 3), bool))&#10;&#10;        # Result = original segmentation ∪ grown_region region&#10;        # refined_segmentation_bool = target_segmentation_bool | grown_region&#10;        refined_segmentation_bool = grown_region&#10;&#10;        # Back in 3-channel form (1 white, 0 black)&#10;        white_val = target_segmentation.max()&#10;        if was_2d:&#10;            refined = np.zeros_like(target_segmentation)&#10;            refined[refined_segmentation_bool] = white_val&#10;            return refined&#10;        else:&#10;            refined_segmentation = np.zeros_like(target_segmentation)&#10;            if C_in == 1:&#10;                refined_segmentation[..., 0][refined_segmentation_bool] = white_val&#10;            else:&#10;                for c in range(C_in):&#10;                    refined_segmentation[..., c][refined_segmentation_bool] = white_val&#10;            return refined_segmentation&#10;" />
              <option name="updatedContent" value="import numpy as np&#10;from scipy.ndimage import binary_propagation&#10;&#10;from postprocessing.segmentation_refiner import ISegmentationRefiner&#10;&#10;&#10;class ColorPatchRefiner(ISegmentationRefiner):&#10;    &quot;&quot;&quot;&#10;    Refines segmentation masks by growing regions within a specified color range using region growing (binary propagation).&#10;    The color range is defined by a color preprocessor. The refinement ensures that the segmentation includes contiguous&#10;    regions matching the color mask, starting from the intersection of the original segmentation and the color mask.&#10;&#10;    Args:&#10;        color_preprocessor: An instance of a color preprocessor providing a preprocess_image method for color masking.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, color_preprocessor):&#10;        &quot;&quot;&quot;&#10;        Initialize the ColorPatchRefiner.&#10;        Args:&#10;            color_preprocessor: Preprocessor instance for extracting the relevant color mask from images.&#10;        &quot;&quot;&quot;&#10;        self.color_preprocessor = color_preprocessor&#10;&#10;    def refine(self, target_segmentation, target_image):&#10;        &quot;&quot;&quot;&#10;        Refine a segmentation mask by growing regions within the color mask using region growing.&#10;        Args:&#10;            target_segmentation (np.ndarray): The initial segmentation mask (2D or 3D array).&#10;            target_image: An object with an 'image' attribute (the original image as np.ndarray).&#10;        Returns:&#10;            np.ndarray: The refined segmentation mask, same shape as input.&#10;        &quot;&quot;&quot;&#10;        if target_segmentation.max() == 0:&#10;            return target_segmentation&#10;&#10;        # Get the color range mask and set it to 3 channels (0/1)&#10;        target_color_mask, _ = self.color_preprocessor.preprocess_image(target_image.image)&#10;        orig_colors = target_image.image[..., :3]&#10;        colored_mask = np.zeros_like(orig_colors)&#10;        mask_bool = target_color_mask &gt; 0&#10;        colored_mask[mask_bool] = orig_colors[mask_bool]&#10;&#10;        target_color_mask = (np.repeat(target_color_mask[:, :, np.newaxis], 3, axis=2) // 255).astype(np.uint8)&#10;&#10;        # 2D boolean masks&#10;        if target_segmentation.ndim == 2:&#10;            target_segmentation_bool = target_segmentation &gt; 0  # [H,W]&#10;            was_2d = True&#10;            C_in = 1&#10;        elif target_segmentation.ndim == 3:&#10;            was_2d = False&#10;            C_in = target_segmentation.shape[2]&#10;            if C_in == 1:&#10;                target_segmentation_bool = target_segmentation[..., 0] &gt; 0&#10;            else:&#10;                target_segmentation_bool = np.any(target_segmentation &gt; 0, axis=2)  # [H,W]&#10;        else:&#10;            raise ValueError(f&quot;Unexpected target_segmentation shape: {target_segmentation.shape}&quot;)&#10;&#10;        target_color_bool = np.any(target_color_mask &gt; 0, axis=2)  # [H,W]&#10;&#10;        # Starting points: Intersection&#10;        seed = target_segmentation_bool &amp; target_color_bool&#10;&#10;        # Region growing within target_color_bool (8-neighborhood)&#10;        grown_region = binary_propagation(seed, mask=target_color_bool, structure=np.ones((3, 3), bool))&#10;&#10;        # Result = original segmentation ∪ grown_region region&#10;        # refined_segmentation_bool = target_segmentation_bool | grown_region&#10;        refined_segmentation_bool = grown_region&#10;&#10;        # Back in 3-channel form (1 white, 0 black)&#10;        white_val = target_segmentation.max()&#10;        if was_2d:&#10;            refined = np.zeros_like(target_segmentation)&#10;            refined[refined_segmentation_bool] = white_val&#10;            return refined&#10;        else:&#10;            refined_segmentation = np.zeros_like(target_segmentation)&#10;            if C_in == 1:&#10;                refined_segmentation[..., 0][refined_segmentation_bool] = white_val&#10;            else:&#10;                for c in range(C_in):&#10;                    refined_segmentation[..., c][refined_segmentation_bool] = white_val&#10;            return refined_segmentation" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/postprocessing/segmentation_refiner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/postprocessing/segmentation_refiner.py" />
              <option name="originalContent" value="from abc import ABC, abstractmethod&#10;&#10;&#10;class ISegmentationRefiner(ABC):&#10;    &quot;&quot;&quot;&#10;    Interface for segmentation refinement steps.&#10;    All segmentation refiner classes should inherit from this interface and implement the refine method.&#10;    &quot;&quot;&quot;&#10;&#10;    @abstractmethod&#10;    def refine(self, target_segmentation, target_image):&#10;        &quot;&quot;&quot;&#10;        Refine a segmentation mask based on the original image or additional information.&#10;        Args:&#10;            target_segmentation (np.ndarray): The initial segmentation mask to be refined.&#10;            target_image: The original image or an object containing the image and metadata.&#10;        Returns:&#10;            np.ndarray: The refined segmentation mask.&#10;        &quot;&quot;&quot;&#10;        pass" />
              <option name="updatedContent" value="from abc import ABC, abstractmethod&#10;&#10;&#10;class ISegmentationRefiner(ABC):&#10;    &quot;&quot;&quot;&#10;    Interface for segmentation refinement steps.&#10;    All segmentation refiner classes should inherit from this interface and implement the refine method.&#10;    &quot;&quot;&quot;&#10;&#10;    @abstractmethod&#10;    def refine(self, target_segmentation, target_image):&#10;        &quot;&quot;&quot;&#10;        Refine a segmentation mask based on the original image or additional information.&#10;        Args:&#10;            target_segmentation (np.ndarray): The initial segmentation mask to be refined.&#10;            target_image: The original image or an object containing the image and metadata.&#10;        Returns:&#10;            np.ndarray: The refined segmentation mask.&#10;        &quot;&quot;&quot;&#10;        pass" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/preprocessing/blue_color_preprocessor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/preprocessing/blue_color_preprocessor.py" />
              <option name="originalContent" value="import numpy as np&#10;&#10;from preprocessing.color_preprocessor import ColorPreprocessor&#10;&#10;&#10;class BlueColorPreprocessor(ColorPreprocessor):&#10;    &quot;&quot;&quot;&#10;    Preprocessor for extracting blue color regions from images using HSV color space thresholds.&#10;    Inherits from ColorPreprocessor and sets the lower and upper bounds for blue color detection.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        &quot;&quot;&quot;&#10;        Initialize the BlueColorPreprocessor with predefined HSV bounds for blue color.&#10;        &quot;&quot;&quot;&#10;        lower_blue = np.array([100, 50, 30])&#10;        upper_blue = np.array([130, 255, 255])&#10;        super().__init__(lower_color=lower_blue, upper_color=upper_blue)&#10;" />
              <option name="updatedContent" value="import numpy as np&#10;&#10;from preprocessing.color_preprocessor import ColorPreprocessor&#10;&#10;&#10;class BlueColorPreprocessor(ColorPreprocessor):&#10;    &quot;&quot;&quot;&#10;    Preprocessor for extracting blue color regions from images using HSV color space thresholds.&#10;    Inherits from ColorPreprocessor and sets the lower and upper bounds for blue color detection.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self):&#10;        &quot;&quot;&quot;&#10;        Initialize the BlueColorPreprocessor with predefined HSV bounds for blue color.&#10;        &quot;&quot;&quot;&#10;        lower_blue = np.array([100, 50, 30])&#10;        upper_blue = np.array([130, 255, 255])&#10;        super().__init__(lower_color=lower_blue, upper_color=upper_blue)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/preprocessing/color_preprocessor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/preprocessing/color_preprocessor.py" />
              <option name="originalContent" value="import cv2&#10;&#10;from preprocessing.preprocessing_step import IPreprocessingStep&#10;&#10;&#10;class ColorPreprocessor(IPreprocessingStep):&#10;    &quot;&quot;&quot;&#10;    Preprocessor for extracting regions of a specific color from images using HSV color space thresholds.&#10;    This class can be subclassed for different color ranges.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, lower_color, upper_color):&#10;        &quot;&quot;&quot;&#10;        Initialize the ColorPreprocessor with lower and upper HSV bounds.&#10;        Args:&#10;            lower_color (np.ndarray): Lower HSV bound for color extraction.&#10;            upper_color (np.ndarray): Upper HSV bound for color extraction.&#10;        &quot;&quot;&quot;&#10;        self.lower_color = lower_color&#10;        self.upper_color = upper_color&#10;&#10;    def preprocess_image(self, image):&#10;        rgb_image = image[..., :3]&#10;        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)&#10;        result_mask = cv2.inRange(hsv, self.lower_color, self.upper_color)&#10;        return result_mask, None&#10;&#10;    def preprocess_mask(self, image, parameters):&#10;        return image&#10;&#10;    def undo_preprocessing(self, preprocessed_image, parameters, is_already_color=False):&#10;        return preprocessed_image" />
              <option name="updatedContent" value="import cv2&#10;&#10;from preprocessing.preprocessing_step import IPreprocessingStep&#10;&#10;&#10;class ColorPreprocessor(IPreprocessingStep):&#10;    &quot;&quot;&quot;&#10;    Preprocessor for extracting regions of a specific color from images using HSV color space thresholds.&#10;    This class can be subclassed for different color ranges.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, lower_color, upper_color):&#10;        &quot;&quot;&quot;&#10;        Initialize the ColorPreprocessor with lower and upper HSV bounds.&#10;        Args:&#10;            lower_color (np.ndarray): Lower HSV bound for color extraction.&#10;            upper_color (np.ndarray): Upper HSV bound for color extraction.&#10;        &quot;&quot;&quot;&#10;        self.lower_color = lower_color&#10;        self.upper_color = upper_color&#10;&#10;    def preprocess_image(self, image):&#10;        rgb_image = image[..., :3]&#10;        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)&#10;        result_mask = cv2.inRange(hsv, self.lower_color, self.upper_color)&#10;        return result_mask, None&#10;&#10;    def preprocess_mask(self, image, parameters):&#10;        return image&#10;&#10;    def undo_preprocessing(self, preprocessed_image, parameters, is_already_color=False):&#10;        return preprocessed_image" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/preprocessing/dimples_roi_preprocessor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/preprocessing/dimples_roi_preprocessor.py" />
              <option name="originalContent" value="from typing_extensions import override&#10;&#10;from preprocessing.torso_roi_preprocessor import TorsoRoiPreprocessor&#10;&#10;&#10;class DimplesRoiPreprocessor(TorsoRoiPreprocessor):&#10;    &quot;&quot;&quot;&#10;    Preprocessing step for extracting the region of interest (ROI) around dimples from an image.&#10;    This class provides methods to crop, pad, and resize the dimples ROI and reverse these operations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, target_ratio):&#10;        &quot;&quot;&quot;&#10;        Initialize the DimplesRoiPreprocessor.&#10;        Args:&#10;            target_ratio (int): Desired size (width and height) for the output ROI image.&#10;        &quot;&quot;&quot;&#10;        super().__init__(target_ratio)&#10;&#10;    @override&#10;    def preprocess_image(self, image):&#10;        original_size = {&#10;            'height': image.shape[0],&#10;            'width': image.shape[1]&#10;        }&#10;        cropped_image, bbox = self.crop_dimples_roi(image)&#10;        cropped_and_padded_image, padding, padded_size = super().pad_image_to_correct_ratio(cropped_image, bbox)&#10;        resized_image = super().rescale_image(cropped_and_padded_image, original_size)&#10;        parameters = {&#10;            'original_size': original_size,&#10;            'bbox': bbox,&#10;            'padding': padding,&#10;            'padded_size': padded_size,&#10;        }&#10;        return resized_image, parameters&#10;&#10;    @override&#10;    def preprocess_mask(self, image, parameters):&#10;         return super().preprocess_mask(image, parameters)&#10;&#10;    @override&#10;    def undo_preprocessing(self, preprocessed_image, parameters, is_already_color=False):&#10;        return super().undo_preprocessing(preprocessed_image, parameters, is_already_color)&#10;&#10;    def crop_dimples_roi(self, image):&#10;        torso_roi_image, torso_bbox = super().crop_torso_roi(image)&#10;        torso_roi_height, torso_roi_width = torso_roi_image.shape[:2]&#10;        dimples_bbox_from_torso = {&#10;            'min_x': 0,&#10;            'max_x': torso_roi_width,&#10;            'min_y': torso_roi_height // 2,&#10;            'max_y': torso_roi_height&#10;        }&#10;        dimples_roi_image = torso_roi_image[dimples_bbox_from_torso['min_y']:dimples_bbox_from_torso['max_y'], dimples_bbox_from_torso['min_x']:dimples_bbox_from_torso['max_x']]&#10;        dimples_bbox_from_original = {&#10;            'min_x': torso_bbox['min_x'],&#10;            'max_x': torso_bbox['min_x'] + torso_roi_width,&#10;            'min_y': torso_bbox['min_y'] + torso_roi_height // 2,&#10;            'max_y': torso_bbox['min_y'] + torso_roi_height,&#10;        }&#10;        return dimples_roi_image, dimples_bbox_from_original&#10;" />
              <option name="updatedContent" value="from typing_extensions import override&#10;&#10;from preprocessing.torso_roi_preprocessor import TorsoRoiPreprocessor&#10;&#10;&#10;class DimplesRoiPreprocessor(TorsoRoiPreprocessor):&#10;    &quot;&quot;&quot;&#10;    Preprocessing step for extracting the region of interest (ROI) around dimples from an image.&#10;    This class provides methods to crop, pad, and resize the dimples ROI and reverse these operations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, target_ratio):&#10;        &quot;&quot;&quot;&#10;        Initialize the DimplesRoiPreprocessor.&#10;        Args:&#10;            target_ratio (int): Desired size (width and height) for the output ROI image.&#10;        &quot;&quot;&quot;&#10;        super().__init__(target_ratio)&#10;&#10;    @override&#10;    def preprocess_image(self, image):&#10;        original_size = {&#10;            'height': image.shape[0],&#10;            'width': image.shape[1]&#10;        }&#10;        cropped_image, bbox = self.crop_dimples_roi(image)&#10;        cropped_and_padded_image, padding, padded_size = super().pad_image_to_correct_ratio(cropped_image, bbox)&#10;        resized_image = super().rescale_image(cropped_and_padded_image, original_size)&#10;        parameters = {&#10;            'original_size': original_size,&#10;            'bbox': bbox,&#10;            'padding': padding,&#10;            'padded_size': padded_size,&#10;        }&#10;        return resized_image, parameters&#10;&#10;    @override&#10;    def preprocess_mask(self, image, parameters):&#10;         return super().preprocess_mask(image, parameters)&#10;&#10;    @override&#10;    def undo_preprocessing(self, preprocessed_image, parameters, is_already_color=False):&#10;        return super().undo_preprocessing(preprocessed_image, parameters, is_already_color)&#10;&#10;    def crop_dimples_roi(self, image):&#10;        torso_roi_image, torso_bbox = super().crop_torso_roi(image)&#10;        torso_roi_height, torso_roi_width = torso_roi_image.shape[:2]&#10;        dimples_bbox_from_torso = {&#10;            'min_x': 0,&#10;            'max_x': torso_roi_width,&#10;            'min_y': torso_roi_height // 2,&#10;            'max_y': torso_roi_height&#10;        }&#10;        dimples_roi_image = torso_roi_image[dimples_bbox_from_torso['min_y']:dimples_bbox_from_torso['max_y'], dimples_bbox_from_torso['min_x']:dimples_bbox_from_torso['max_x']]&#10;        dimples_bbox_from_original = {&#10;            'min_x': torso_bbox['min_x'],&#10;            'max_x': torso_bbox['min_x'] + torso_roi_width,&#10;            'min_y': torso_bbox['min_y'] + torso_roi_height // 2,&#10;            'max_y': torso_bbox['min_y'] + torso_roi_height,&#10;        }&#10;        return dimples_roi_image, dimples_bbox_from_original" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/preprocessing/preprocessing_step.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/preprocessing/preprocessing_step.py" />
              <option name="originalContent" value="from abc import ABC, abstractmethod&#10;&#10;&#10;class IPreprocessingStep(ABC):&#10;    &quot;&quot;&quot;&#10;    Interface for preprocessing steps. All preprocessing step classes should inherit from this interface&#10;    and implement the required methods for image and mask preprocessing.&#10;    &quot;&quot;&quot;&#10;&#10;    @abstractmethod&#10;    def preprocess_image(self, image):&#10;        &quot;&quot;&quot;&#10;        Preprocess an image and return the processed image and parameters.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;        Returns:&#10;            tuple: (processed_image, parameters)&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def preprocess_mask(self, image, parameters):&#10;        &quot;&quot;&quot;&#10;        Preprocess a mask using the same parameters as the corresponding image.&#10;        Args:&#10;            image (np.ndarray): Input mask.&#10;            parameters (dict): Parameters from preprocess_image.&#10;        Returns:&#10;            np.ndarray: Preprocessed mask.&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def undo_preprocessing(self, image, parameters):&#10;        &quot;&quot;&quot;&#10;        Reverse the preprocessing to restore the original image size and content.&#10;        Args:&#10;            preprocessed_image (np.ndarray): The preprocessed image.&#10;            parameters (dict): Parameters from preprocess_image.&#10;            is_already_color (bool): If True, treat as color image.&#10;        Returns:&#10;            np.ndarray: Restored image.&#10;        &quot;&quot;&quot;&#10;        pass" />
              <option name="updatedContent" value="from abc import ABC, abstractmethod&#10;&#10;&#10;class IPreprocessingStep(ABC):&#10;    &quot;&quot;&quot;&#10;    Interface for preprocessing steps. All preprocessing step classes should inherit from this interface&#10;    and implement the required methods for image and mask preprocessing.&#10;    &quot;&quot;&quot;&#10;&#10;    @abstractmethod&#10;    def preprocess_image(self, image):&#10;        &quot;&quot;&quot;&#10;        Preprocess an image and return the processed image and parameters.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;        Returns:&#10;            tuple: (processed_image, parameters)&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def preprocess_mask(self, image, parameters):&#10;        &quot;&quot;&quot;&#10;        Preprocess a mask using the same parameters as the corresponding image.&#10;        Args:&#10;            image (np.ndarray): Input mask.&#10;            parameters (dict): Parameters from preprocess_image.&#10;        Returns:&#10;            np.ndarray: Preprocessed mask.&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def undo_preprocessing(self, image, parameters):&#10;        &quot;&quot;&quot;&#10;        Reverse the preprocessing to restore the original image size and content.&#10;        Args:&#10;            preprocessed_image (np.ndarray): The preprocessed image.&#10;            parameters (dict): Parameters from preprocess_image.&#10;            is_already_color (bool): If True, treat as color image.&#10;        Returns:&#10;            np.ndarray: Restored image.&#10;        &quot;&quot;&quot;&#10;        pass" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/preprocessing/square_image_preprocessor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/preprocessing/square_image_preprocessor.py" />
              <option name="originalContent" value="from typing import Tuple, Optional, Dict, Any&#10;&#10;import numpy as np&#10;from PIL import Image&#10;from preprocessing.preprocessing_step import IPreprocessingStep&#10;&#10;&#10;class SquareImagePreprocessor(IPreprocessingStep):&#10;    &quot;&quot;&quot;&#10;    Preprocessor for cropping and resizing images to a square aspect ratio.&#10;    This class provides methods to crop the largest possible square from the center and resize to a target size.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, resize_size: int = 256, crop_size: int = 255):&#10;        &quot;&quot;&quot;&#10;        Initialize the SquareImagePreprocessor.&#10;&#10;        Args:&#10;            resize_size (int): Desired size (width and height) for the output square image.&#10;            crop_size (int): Size of the square crop from the center of the image.&#10;        &quot;&quot;&quot;&#10;        assert resize_size &gt;= crop_size, &quot;resize_size must be &gt;= crop_size&quot;&#10;        self.resize_size = int(resize_size)&#10;        self.crop_size = int(crop_size)&#10;&#10;    def preprocess_image(self, image):&#10;        H, W = image.shape[:2]&#10;        pil, mode = self._to_pil(image)&#10;&#10;        # 1) Resize (bilinear)&#10;        pil = pil.resize((self.resize_size, self.resize_size), resample=Image.BILINEAR)&#10;&#10;        # 2) Center crop&#10;        dh = self.resize_size - self.crop_size&#10;        dw = self.resize_size - self.crop_size&#10;        # round off left/top (like torchvision)&#10;        left   = int(np.floor(dw / 2.0))&#10;        top    = int(np.floor(dh / 2.0))&#10;        right  = left + self.crop_size&#10;        bottom = top + self.crop_size&#10;        pil = pil.crop((left, top, right, bottom))&#10;&#10;        out = self._from_pil(pil, mode, image.ndim)&#10;&#10;        params = {&#10;            &quot;orig_size&quot;: (H, W),&#10;            &quot;resize_size&quot;: self.resize_size,&#10;            &quot;crop_box&quot;: (left, top, right, bottom),&#10;            &quot;out_mode&quot;: mode,&#10;            &quot;orig_ndim&quot;: image.ndim,&#10;        }&#10;        return out, params&#10;&#10;    def preprocess_mask(self, mask: np.ndarray, params: Optional[Dict[str, Any]] = None) -&gt; np.ndarray:&#10;        &quot;&quot;&quot;&#10;        Apply the same steps to a mask:&#10;        - Resize with NEAREST&#10;        - CenterCrop with the same crop_box&#10;        Expects 2D or 3D (H,W,1) mask.&#10;        &quot;&quot;&quot;&#10;        assert mask.ndim in (2, 3), f&quot;Unexpected mask shape: {mask.shape}&quot;&#10;        # For masks, values usually remain {0.1} / {0.255}; never use bilinear blending.&#10;        pil_m, _ = self._to_pil(mask.astype(np.uint8))  # auf uint8 für PIL&#10;&#10;        # Resize (nearest)&#10;        pil_m = pil_m.resize((self.resize_size, self.resize_size), resample=Image.NEAREST)&#10;&#10;        # Crop with the same crop box as for the image&#10;        if params is None:&#10;            dh = self.resize_size - self.crop_size&#10;            dw = self.resize_size - self.crop_size&#10;            left = int(np.floor(dw / 2.0))&#10;            top = int(np.floor(dh / 2.0))&#10;            right = left + self.crop_size&#10;            bottom = top + self.crop_size&#10;        else:&#10;            left, top, right, bottom = params[&quot;crop_box&quot;]&#10;&#10;        pil_m = pil_m.crop((left, top, right, bottom))&#10;&#10;        arr = np.array(pil_m)&#10;        if mask.ndim == 3:&#10;            arr = arr[:, :, None]&#10;        return arr&#10;&#10;    def undo_preprocessing(self, mask: np.ndarray, params: Dict[str, Any], is_already_color=False) -&gt; np.ndarray:&#10;        &quot;&quot;&quot;&#10;        Save the steps for a mask:&#10;        1) Place the mask (crop_size x crop_size) in a (resize_size x resize_size) canvas&#10;        back to the same crop_box (rest = 0).&#10;        2) Scale the canvas back to orig_size with NEAREST.&#10;        &quot;&quot;&quot;&#10;        resize_size = int(params[&quot;resize_size&quot;])&#10;        left, top, right, bottom = params[&quot;crop_box&quot;]&#10;        orig_h, orig_w = params[&quot;orig_size&quot;]&#10;&#10;        # Step 1: Back to Resize Canvas&#10;        if mask.ndim == 3:&#10;            canvas = np.zeros((resize_size, resize_size, mask.shape[2]), dtype=mask.dtype)&#10;            canvas[top:bottom, left:right, :] = mask&#10;        else:&#10;            canvas = np.zeros((resize_size, resize_size), dtype=mask.dtype)&#10;            canvas[top:bottom, left:right] = mask&#10;&#10;        # Step 2: NEAREST back to original size&#10;        pil = Image.fromarray(canvas.squeeze() if canvas.ndim == 3 and canvas.shape[2] == 1 else canvas)&#10;        pil = pil.resize((orig_w, orig_h), resample=Image.NEAREST)&#10;        out = np.array(pil)&#10;&#10;        if mask.ndim == 3 and out.ndim == 2:&#10;            out = out[:, :, None]&#10;        return out&#10;&#10;    @staticmethod&#10;    def _to_pil(img: np.ndarray) -&gt; Tuple[Image.Image, str]:&#10;        &quot;&quot;&quot;&#10;        Takes np.ndarray with shape (H,W) or (H,W,C), dtype uint8/float.&#10;        Returns PIL image and a hint on how to scale when converting back.&#10;        &quot;&quot;&quot;&#10;        assert img.ndim in (2, 3), f&quot;Unexpected image shape: {img.shape}&quot;&#10;        mode = &quot;uint8&quot;&#10;        if img.dtype != np.uint8:&#10;            # accept float; in [0,1] → scale to uint8&#10;            mode = &quot;float01&quot;&#10;            arr = np.clip(img, 0.0, 1.0)&#10;            arr = (arr * 255.0 + 0.5).astype(np.uint8)&#10;        else:&#10;            arr = img&#10;&#10;        if arr.ndim == 2:&#10;            pil = Image.fromarray(arr, mode=&quot;L&quot;)&#10;        else:&#10;            if arr.shape[2] == 1:&#10;                pil = Image.fromarray(arr.squeeze(2), mode=&quot;L&quot;)&#10;            elif arr.shape[2] == 3:&#10;                pil = Image.fromarray(arr, mode=&quot;RGB&quot;)&#10;            else:&#10;                # RGBA -&gt; RGB&#10;                pil = Image.fromarray(arr[:, :, :3], mode=&quot;RGB&quot;)&#10;        return pil, mode&#10;&#10;    @staticmethod&#10;    def _from_pil(pil: Image.Image, out_mode: str, orig_ndim: int) -&gt; np.ndarray:&#10;        &quot;&quot;&quot;&#10;        Converts PIL back to np.ndarray. out_mode==“float01” → scales to [0,1] float32,&#10;        otherwise uint8. orig_ndim controls whether (H,W) or (H,W,1/3) is returned.&#10;        &quot;&quot;&quot;&#10;        arr = np.array(pil)&#10;        if arr.ndim == 2 and orig_ndim == 3:&#10;            arr = arr[:, :, None]  # (H,W,1)&#10;&#10;        if out_mode == &quot;float01&quot;:&#10;            arr = arr.astype(np.float32) / 255.0&#10;        else:&#10;            arr = arr.astype(np.uint8)&#10;        return arr&#10;" />
              <option name="updatedContent" value="from typing import Tuple, Optional, Dict, Any&#10;&#10;import numpy as np&#10;from PIL import Image&#10;from preprocessing.preprocessing_step import IPreprocessingStep&#10;&#10;&#10;class SquareImagePreprocessor(IPreprocessingStep):&#10;    &quot;&quot;&quot;&#10;    Preprocessor for cropping and resizing images to a square aspect ratio.&#10;    This class provides methods to crop the largest possible square from the center and resize to a target size.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, resize_size: int = 256, crop_size: int = 255):&#10;        &quot;&quot;&quot;&#10;        Initialize the SquareImagePreprocessor.&#10;&#10;        Args:&#10;            resize_size (int): Desired size (width and height) for the output square image.&#10;            crop_size (int): Size of the square crop from the center of the image.&#10;        &quot;&quot;&quot;&#10;        assert resize_size &gt;= crop_size, &quot;resize_size must be &gt;= crop_size&quot;&#10;        self.resize_size = int(resize_size)&#10;        self.crop_size = int(crop_size)&#10;&#10;    def preprocess_image(self, image):&#10;        H, W = image.shape[:2]&#10;        pil, mode = self._to_pil(image)&#10;&#10;        # 1) Resize (bilinear)&#10;        pil = pil.resize((self.resize_size, self.resize_size), resample=Image.BILINEAR)&#10;&#10;        # 2) Center crop&#10;        dh = self.resize_size - self.crop_size&#10;        dw = self.resize_size - self.crop_size&#10;        # round off left/top (like torchvision)&#10;        left   = int(np.floor(dw / 2.0))&#10;        top    = int(np.floor(dh / 2.0))&#10;        right  = left + self.crop_size&#10;        bottom = top + self.crop_size&#10;        pil = pil.crop((left, top, right, bottom))&#10;&#10;        out = self._from_pil(pil, mode, image.ndim)&#10;&#10;        params = {&#10;            &quot;orig_size&quot;: (H, W),&#10;            &quot;resize_size&quot;: self.resize_size,&#10;            &quot;crop_box&quot;: (left, top, right, bottom),&#10;            &quot;out_mode&quot;: mode,&#10;            &quot;orig_ndim&quot;: image.ndim,&#10;        }&#10;        return out, params&#10;&#10;    def preprocess_mask(self, mask: np.ndarray, params: Optional[Dict[str, Any]] = None) -&gt; np.ndarray:&#10;        &quot;&quot;&quot;&#10;        Apply the same steps to a mask:&#10;        - Resize with NEAREST&#10;        - CenterCrop with the same crop_box&#10;        Expects 2D or 3D (H,W,1) mask.&#10;        &quot;&quot;&quot;&#10;        assert mask.ndim in (2, 3), f&quot;Unexpected mask shape: {mask.shape}&quot;&#10;        # For masks, values usually remain {0.1} / {0.255}; never use bilinear blending.&#10;        pil_m, _ = self._to_pil(mask.astype(np.uint8))  # auf uint8 für PIL&#10;&#10;        # Resize (nearest)&#10;        pil_m = pil_m.resize((self.resize_size, self.resize_size), resample=Image.NEAREST)&#10;&#10;        # Crop with the same crop box as for the image&#10;        if params is None:&#10;            dh = self.resize_size - self.crop_size&#10;            dw = self.resize_size - self.crop_size&#10;            left = int(np.floor(dw / 2.0))&#10;            top = int(np.floor(dh / 2.0))&#10;            right = left + self.crop_size&#10;            bottom = top + self.crop_size&#10;        else:&#10;            left, top, right, bottom = params[&quot;crop_box&quot;]&#10;&#10;        pil_m = pil_m.crop((left, top, right, bottom))&#10;&#10;        arr = np.array(pil_m)&#10;        if mask.ndim == 3:&#10;            arr = arr[:, :, None]&#10;        return arr&#10;&#10;    def undo_preprocessing(self, mask: np.ndarray, params: Dict[str, Any], is_already_color=False) -&gt; np.ndarray:&#10;        &quot;&quot;&quot;&#10;        Save the steps for a mask:&#10;        1) Place the mask (crop_size x crop_size) in a (resize_size x resize_size) canvas&#10;        back to the same crop_box (rest = 0).&#10;        2) Scale the canvas back to orig_size with NEAREST.&#10;        &quot;&quot;&quot;&#10;        resize_size = int(params[&quot;resize_size&quot;])&#10;        left, top, right, bottom = params[&quot;crop_box&quot;]&#10;        orig_h, orig_w = params[&quot;orig_size&quot;]&#10;&#10;        # Step 1: Back to Resize Canvas&#10;        if mask.ndim == 3:&#10;            canvas = np.zeros((resize_size, resize_size, mask.shape[2]), dtype=mask.dtype)&#10;            canvas[top:bottom, left:right, :] = mask&#10;        else:&#10;            canvas = np.zeros((resize_size, resize_size), dtype=mask.dtype)&#10;            canvas[top:bottom, left:right] = mask&#10;&#10;        # Step 2: NEAREST back to original size&#10;        pil = Image.fromarray(canvas.squeeze() if canvas.ndim == 3 and canvas.shape[2] == 1 else canvas)&#10;        pil = pil.resize((orig_w, orig_h), resample=Image.NEAREST)&#10;        out = np.array(pil)&#10;&#10;        if mask.ndim == 3 and out.ndim == 2:&#10;            out = out[:, :, None]&#10;        return out&#10;&#10;    @staticmethod&#10;    def _to_pil(img: np.ndarray) -&gt; Tuple[Image.Image, str]:&#10;        &quot;&quot;&quot;&#10;        Takes np.ndarray with shape (H,W) or (H,W,C), dtype uint8/float.&#10;        Returns PIL image and a hint on how to scale when converting back.&#10;        &quot;&quot;&quot;&#10;        assert img.ndim in (2, 3), f&quot;Unexpected image shape: {img.shape}&quot;&#10;        mode = &quot;uint8&quot;&#10;        if img.dtype != np.uint8:&#10;            # accept float; in [0,1] → scale to uint8&#10;            mode = &quot;float01&quot;&#10;            arr = np.clip(img, 0.0, 1.0)&#10;            arr = (arr * 255.0 + 0.5).astype(np.uint8)&#10;        else:&#10;            arr = img&#10;&#10;        if arr.ndim == 2:&#10;            pil = Image.fromarray(arr, mode=&quot;L&quot;)&#10;        else:&#10;            if arr.shape[2] == 1:&#10;                pil = Image.fromarray(arr.squeeze(2), mode=&quot;L&quot;)&#10;            elif arr.shape[2] == 3:&#10;                pil = Image.fromarray(arr, mode=&quot;RGB&quot;)&#10;            else:&#10;                # RGBA -&gt; RGB&#10;                pil = Image.fromarray(arr[:, :, :3], mode=&quot;RGB&quot;)&#10;        return pil, mode&#10;&#10;    @staticmethod&#10;    def _from_pil(pil: Image.Image, out_mode: str, orig_ndim: int) -&gt; np.ndarray:&#10;        &quot;&quot;&quot;&#10;        Converts PIL back to np.ndarray. out_mode==“float01” → scales to [0,1] float32,&#10;        otherwise uint8. orig_ndim controls whether (H,W) or (H,W,1/3) is returned.&#10;        &quot;&quot;&quot;&#10;        arr = np.array(pil)&#10;        if arr.ndim == 2 and orig_ndim == 3:&#10;            arr = arr[:, :, None]  # (H,W,1)&#10;&#10;        if out_mode == &quot;float01&quot;:&#10;            arr = arr.astype(np.float32) / 255.0&#10;        else:&#10;            arr = arr.astype(np.uint8)&#10;        return arr" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/preprocessing/torso_roi_preprocessor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/preprocessing/torso_roi_preprocessor.py" />
              <option name="originalContent" value="import numpy as np&#10;from skimage.transform import resize&#10;&#10;from preprocessing.preprocessing_step import IPreprocessingStep&#10;&#10;&#10;class TorsoRoiPreprocessor(IPreprocessingStep):&#10;    &quot;&quot;&quot;&#10;    Preprocessing step for extracting, padding, and resizing the torso region of interest (ROI) from an image.&#10;    This class provides methods to crop the torso, pad to a target aspect ratio, rescale, and reverse these operations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, target_ratio):&#10;        &quot;&quot;&quot;&#10;        Initialize the TorsoRoiPreprocessor.&#10;        Args:&#10;            target_ratio (float): Desired width-to-height ratio for the output image.&#10;        &quot;&quot;&quot;&#10;        self.target_ratio = target_ratio  # width : height&#10;&#10;    def preprocess_image(self, image):&#10;        &quot;&quot;&quot;&#10;        Preprocess an image by cropping the torso ROI, padding to the target ratio, and resizing to the original size.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;        Returns:&#10;            tuple: (resized_image, parameters) where parameters is a dict with crop, pad, and size info.&#10;        &quot;&quot;&quot;&#10;        original_size = {&#10;            'height': image.shape[0],&#10;            'width': image.shape[1]&#10;        }&#10;        cropped_image, bbox = self.crop_torso_roi(image)&#10;        cropped_and_padded_image, padding, padded_size = self.pad_image_to_correct_ratio(cropped_image, bbox)&#10;        resized_image = self.rescale_image(cropped_and_padded_image, original_size)&#10;        parameters = {&#10;            'original_size': original_size,&#10;            'bbox': bbox,&#10;            'padding': padding,&#10;            'padded_size': padded_size,&#10;        }&#10;        return resized_image, parameters&#10;&#10;    def preprocess_mask(self, image, parameters):&#10;        &quot;&quot;&quot;&#10;        Preprocess a mask using the same parameters as the corresponding image.&#10;        Args:&#10;            image (np.ndarray): Input mask.&#10;            parameters (dict): Parameters from preprocess_image.&#10;        Returns:&#10;            np.ndarray: Preprocessed mask.&#10;        &quot;&quot;&quot;&#10;        cropped_image = self.crop_with_parameters(image, parameters['bbox'])&#10;        cropped_and_padded_image = self.pad_image_with_parameters(cropped_image, parameters['padding'])&#10;        resized_image = self.rescale_image(cropped_and_padded_image, parameters['original_size'])&#10;        return resized_image&#10;&#10;    def undo_preprocessing(self, preprocessed_image, parameters, is_already_color=False):&#10;        &quot;&quot;&quot;&#10;        Reverse the preprocessing to restore the original image size and content.&#10;        Args:&#10;            preprocessed_image (np.ndarray): The preprocessed image.&#10;            parameters (dict): Parameters from preprocess_image.&#10;            is_already_color (bool): If True, treat as color image.&#10;        Returns:&#10;            np.ndarray: Restored image.&#10;        &quot;&quot;&quot;&#10;        cropped_and_padded_image = self.undo_rescale_image(preprocessed_image, parameters['padded_size'])&#10;        cropped_image = self.undo_pad_image_to_correct_ratio(cropped_and_padded_image, parameters['padding'])&#10;        image = self.undo_crop_torso_roi(cropped_image, parameters['original_size'], parameters['bbox'])&#10;        return image&#10;&#10;    @staticmethod&#10;    def crop_torso_roi(image):&#10;        &quot;&quot;&quot;&#10;        Crop the torso region of interest (ROI) from the image based on non-black pixels.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;        Returns:&#10;            tuple: (cropped_image, bbox) where bbox is a dict with crop coordinates.&#10;        &quot;&quot;&quot;&#10;        image_rgb = image[..., :3]&#10;        torso_roi = np.any(image_rgb != [0, 0, 0], axis=-1)&#10;        coords = np.argwhere(torso_roi)&#10;        if coords.size == 0:&#10;            raise ValueError(&quot;No torso region found in the image.&quot;)&#10;        min_y, min_x = coords.min(axis=0)&#10;        max_y, max_x = coords.max(axis=0)&#10;        bbox = {&#10;            'min_x': min_x,&#10;            'max_x': max_x + 1,  # use exclusive values&#10;            'min_y': min_y,&#10;            'max_y': max_y + 1  # use exclusive values&#10;        }&#10;        cropped_image = image[min_y:max_y, min_x:max_x, ...]&#10;        return cropped_image, bbox&#10;&#10;    @staticmethod&#10;    def crop_with_parameters(image, bbox):&#10;        &quot;&quot;&quot;&#10;        Crop an image using a bounding box.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;            bbox (dict): Bounding box with min/max x/y.&#10;        Returns:&#10;            np.ndarray: Cropped image.&#10;        &quot;&quot;&quot;&#10;        cropped_image = image[bbox['min_y']:bbox['max_y'], bbox['min_x']:bbox['max_x'], ...]&#10;        return cropped_image&#10;&#10;    @staticmethod&#10;    def undo_crop_torso_roi(cropped_image, original_size, bbox):&#10;        &quot;&quot;&quot;&#10;        Restore a cropped image to its original size using the bounding box.&#10;        Args:&#10;            cropped_image (np.ndarray): Cropped image.&#10;            original_size (dict): Original image size.&#10;            bbox (dict): Bounding box used for cropping.&#10;        Returns:&#10;            np.ndarray: Restored image.&#10;        &quot;&quot;&quot;&#10;        H, W = original_size['height'], original_size['width']&#10;&#10;        if cropped_image.ndim == 3:&#10;            C = cropped_image.shape[2]&#10;            restored = np.zeros((H, W, C), dtype=cropped_image.dtype)&#10;            restored[bbox['min_y']:bbox['max_y'], bbox['min_x']:bbox['max_x'], :] = cropped_image&#10;        elif cropped_image.ndim == 2:&#10;            restored = np.zeros((H, W), dtype=cropped_image.dtype)&#10;            restored[bbox['min_y']:bbox['max_y'], bbox['min_x']:bbox['max_x']] = cropped_image&#10;        else:&#10;            raise ValueError(f&quot;Unsupported ndim {cropped_image.ndim} in undo_crop_torso_roi&quot;)&#10;        return restored&#10;&#10;    def pad_image_to_correct_ratio(self, cropped_image, bbox):&#10;        &quot;&quot;&quot;&#10;        Pad the cropped image to achieve the target aspect ratio.&#10;        Args:&#10;            cropped_image (np.ndarray): Cropped image.&#10;            bbox (dict): Bounding box used for cropping.&#10;        Returns:&#10;            tuple: (padded_image, padding, padded_size)&#10;        &quot;&quot;&quot;&#10;        width = bbox['max_x'] - bbox['min_x']&#10;        height = bbox['max_y'] - bbox['min_y']&#10;        current_ratio = width / height&#10;&#10;        if current_ratio &lt; self.target_ratio:&#10;            padded_width = int(round(height * self.target_ratio))&#10;            padded_height = height&#10;            pad_total = padded_width - width&#10;            pad_left = pad_total // 2&#10;            pad_right = pad_total - pad_left&#10;            pad_top = 0&#10;            pad_bottom = 0&#10;        elif current_ratio &gt; self.target_ratio:&#10;            padded_width = width&#10;            padded_height = int(round(width / self.target_ratio))&#10;            pad_total = padded_height - height&#10;            pad_left = 0&#10;            pad_right = 0&#10;            pad_top = pad_total // 2&#10;            pad_bottom = pad_total - pad_top&#10;        else:&#10;            padded_width = width&#10;            padded_height = height&#10;            pad_left = pad_right = pad_top = pad_bottom = 0&#10;&#10;        padding = {&#10;            'left': pad_left,&#10;            'right': pad_right,&#10;            'top': pad_top,&#10;            'bottom': pad_bottom&#10;        }&#10;&#10;        padded_size = {&#10;            'width': padded_width,&#10;            'height': padded_height&#10;        }&#10;&#10;        padded_image = np.pad(&#10;            cropped_image,&#10;            ((padding['top'], padding['bottom']), (padding['left'], padding['right']), (0, 0)),&#10;            mode='constant',&#10;            constant_values=0&#10;        )&#10;        return padded_image, padding, padded_size&#10;&#10;    @staticmethod&#10;    def pad_image_with_parameters(cropped_image, padding):&#10;        &quot;&quot;&quot;&#10;        Pad an image using specified padding values.&#10;        Args:&#10;            cropped_image (np.ndarray): Cropped image.&#10;            padding (dict): Padding values for each side.&#10;        Returns:&#10;            np.ndarray: Padded image.&#10;        &quot;&quot;&quot;&#10;        padded_image = np.pad(&#10;            cropped_image,&#10;            ((padding['top'], padding['bottom']), (padding['left'], padding['right']), (0, 0)),&#10;            mode='constant',&#10;            constant_values=0&#10;        )&#10;        return padded_image&#10;&#10;    @staticmethod&#10;    def undo_pad_image_to_correct_ratio(padded_image, padding):&#10;        &quot;&quot;&quot;&#10;        Remove padding from an image using specified padding values.&#10;        Args:&#10;            padded_image (np.ndarray): Padded image.&#10;            padding (dict): Padding values for each side.&#10;        Returns:&#10;            np.ndarray: Unpadded image.&#10;        &quot;&quot;&quot;&#10;        top, bottom = padding['top'], padding['bottom']&#10;        left, right = padding['left'], padding['right']&#10;        height, width = padded_image.shape[:2]&#10;        unpadded_image = padded_image[&#10;            top:height - bottom if bottom &gt; 0 else height,&#10;            left:width - right if right &gt; 0 else width,&#10;            ...&#10;        ]&#10;        return unpadded_image&#10;&#10;    def rescale_image(self, cropped_and_padded_image, original_size):&#10;        &quot;&quot;&quot;&#10;        Rescale the image to the original size (width, height) after padding.&#10;        Args:&#10;            cropped_and_padded_image (np.ndarray): Image after cropping and padding.&#10;            original_size (dict): Original image size.&#10;        Returns:&#10;            np.ndarray: Rescaled image.&#10;        &quot;&quot;&quot;&#10;        target_height = original_size['width'] / self.target_ratio&#10;        output_shape = (target_height, original_size['width'], cropped_and_padded_image.shape[2])&#10;        rescaled_image = resize(&#10;            cropped_and_padded_image,&#10;            output_shape,&#10;            order=1,  # bilinear&#10;            mode='constant',&#10;            cval=0,&#10;            anti_aliasing=True,&#10;            preserve_range=True&#10;        ).astype(cropped_and_padded_image.dtype)&#10;        return rescaled_image&#10;&#10;    @staticmethod&#10;    def undo_rescale_image(rescaled_image, padded_size):&#10;        &quot;&quot;&quot;&#10;        Reverse the rescaling to restore the padded image to its original padded size.&#10;        Args:&#10;            rescaled_image (np.ndarray): Rescaled image.&#10;            padded_size (dict): Size after padding.&#10;        Returns:&#10;            np.ndarray: Unscaled image.&#10;        &quot;&quot;&quot;&#10;        # Automatically detect image type (color, grayscale, or binary)&#10;        is_binary = len(np.unique(rescaled_image)) &lt;= 2&#10;&#10;        if rescaled_image.ndim == 3:&#10;            # Color image&#10;            output_shape = (padded_size['height'], padded_size['width'], rescaled_image.shape[2])&#10;            if is_binary:&#10;                order = 0  # Nearest-neighbor for binary&#10;                anti_aliasing = False&#10;            else:&#10;                order = 1  # Bilinear for grayscale&#10;                anti_aliasing = True&#10;        elif rescaled_image.ndim == 2:&#10;            # Grayscale or binary image&#10;            output_shape = (padded_size['height'], padded_size['width'])&#10;            if is_binary:&#10;                order = 0  # Nearest-neighbor for binary&#10;                anti_aliasing = False&#10;            else:&#10;                order = 1  # Bilinear for grayscale&#10;                anti_aliasing = True&#10;        else:&#10;            raise ValueError(f&quot;Unsupported image ndim: {rescaled_image.ndim}. Image must be 2D or 3D.&quot;)&#10;&#10;        unscaled_image = resize(&#10;            rescaled_image,&#10;            output_shape,&#10;            order=order,&#10;            mode='constant',&#10;            cval=0,&#10;            anti_aliasing=anti_aliasing,&#10;            preserve_range=True&#10;        ).astype(rescaled_image.dtype)&#10;        return unscaled_image&#10;" />
              <option name="updatedContent" value="import numpy as np&#10;from skimage.transform import resize&#10;&#10;from preprocessing.preprocessing_step import IPreprocessingStep&#10;&#10;&#10;class TorsoRoiPreprocessor(IPreprocessingStep):&#10;    &quot;&quot;&quot;&#10;    Preprocessing step for extracting, padding, and resizing the torso region of interest (ROI) from an image.&#10;    This class provides methods to crop the torso, pad to a target aspect ratio, rescale, and reverse these operations.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, target_ratio):&#10;        &quot;&quot;&quot;&#10;        Initialize the TorsoRoiPreprocessor.&#10;        Args:&#10;            target_ratio (float): Desired width-to-height ratio for the output image.&#10;        &quot;&quot;&quot;&#10;        self.target_ratio = target_ratio  # width : height&#10;&#10;    def preprocess_image(self, image):&#10;        &quot;&quot;&quot;&#10;        Preprocess an image by cropping the torso ROI, padding to the target ratio, and resizing to the original size.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;        Returns:&#10;            tuple: (resized_image, parameters) where parameters is a dict with crop, pad, and size info.&#10;        &quot;&quot;&quot;&#10;        original_size = {&#10;            'height': image.shape[0],&#10;            'width': image.shape[1]&#10;        }&#10;        cropped_image, bbox = self.crop_torso_roi(image)&#10;        cropped_and_padded_image, padding, padded_size = self.pad_image_to_correct_ratio(cropped_image, bbox)&#10;        resized_image = self.rescale_image(cropped_and_padded_image, original_size)&#10;        parameters = {&#10;            'original_size': original_size,&#10;            'bbox': bbox,&#10;            'padding': padding,&#10;            'padded_size': padded_size,&#10;        }&#10;        return resized_image, parameters&#10;&#10;    def preprocess_mask(self, image, parameters):&#10;        &quot;&quot;&quot;&#10;        Preprocess a mask using the same parameters as the corresponding image.&#10;        Args:&#10;            image (np.ndarray): Input mask.&#10;            parameters (dict): Parameters from preprocess_image.&#10;        Returns:&#10;            np.ndarray: Preprocessed mask.&#10;        &quot;&quot;&quot;&#10;        cropped_image = self.crop_with_parameters(image, parameters['bbox'])&#10;        cropped_and_padded_image = self.pad_image_with_parameters(cropped_image, parameters['padding'])&#10;        resized_image = self.rescale_image(cropped_and_padded_image, parameters['original_size'])&#10;        return resized_image&#10;&#10;    def undo_preprocessing(self, preprocessed_image, parameters, is_already_color=False):&#10;        &quot;&quot;&quot;&#10;        Reverse the preprocessing to restore the original image size and content.&#10;        Args:&#10;            preprocessed_image (np.ndarray): The preprocessed image.&#10;            parameters (dict): Parameters from preprocess_image.&#10;            is_already_color (bool): If True, treat as color image.&#10;        Returns:&#10;            np.ndarray: Restored image.&#10;        &quot;&quot;&quot;&#10;        cropped_and_padded_image = self.undo_rescale_image(preprocessed_image, parameters['padded_size'])&#10;        cropped_image = self.undo_pad_image_to_correct_ratio(cropped_and_padded_image, parameters['padding'])&#10;        image = self.undo_crop_torso_roi(cropped_image, parameters['original_size'], parameters['bbox'])&#10;        return image&#10;&#10;    @staticmethod&#10;    def crop_torso_roi(image):&#10;        &quot;&quot;&quot;&#10;        Crop the torso region of interest (ROI) from the image based on non-black pixels.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;        Returns:&#10;            tuple: (cropped_image, bbox) where bbox is a dict with crop coordinates.&#10;        &quot;&quot;&quot;&#10;        image_rgb = image[..., :3]&#10;        torso_roi = np.any(image_rgb != [0, 0, 0], axis=-1)&#10;        coords = np.argwhere(torso_roi)&#10;        if coords.size == 0:&#10;            raise ValueError(&quot;No torso region found in the image.&quot;)&#10;        min_y, min_x = coords.min(axis=0)&#10;        max_y, max_x = coords.max(axis=0)&#10;        bbox = {&#10;            'min_x': min_x,&#10;            'max_x': max_x + 1,  # use exclusive values&#10;            'min_y': min_y,&#10;            'max_y': max_y + 1  # use exclusive values&#10;        }&#10;        cropped_image = image[min_y:max_y, min_x:max_x, ...]&#10;        return cropped_image, bbox&#10;&#10;    @staticmethod&#10;    def crop_with_parameters(image, bbox):&#10;        &quot;&quot;&quot;&#10;        Crop an image using a bounding box.&#10;        Args:&#10;            image (np.ndarray): Input image.&#10;            bbox (dict): Bounding box with min/max x/y.&#10;        Returns:&#10;            np.ndarray: Cropped image.&#10;        &quot;&quot;&quot;&#10;        cropped_image = image[bbox['min_y']:bbox['max_y'], bbox['min_x']:bbox['max_x'], ...]&#10;        return cropped_image&#10;&#10;    @staticmethod&#10;    def undo_crop_torso_roi(cropped_image, original_size, bbox):&#10;        &quot;&quot;&quot;&#10;        Restore a cropped image to its original size using the bounding box.&#10;        Args:&#10;            cropped_image (np.ndarray): Cropped image.&#10;            original_size (dict): Original image size.&#10;            bbox (dict): Bounding box used for cropping.&#10;        Returns:&#10;            np.ndarray: Restored image.&#10;        &quot;&quot;&quot;&#10;        H, W = original_size['height'], original_size['width']&#10;&#10;        if cropped_image.ndim == 3:&#10;            C = cropped_image.shape[2]&#10;            restored = np.zeros((H, W, C), dtype=cropped_image.dtype)&#10;            restored[bbox['min_y']:bbox['max_y'], bbox['min_x']:bbox['max_x'], :] = cropped_image&#10;        elif cropped_image.ndim == 2:&#10;            restored = np.zeros((H, W), dtype=cropped_image.dtype)&#10;            restored[bbox['min_y']:bbox['max_y'], bbox['min_x']:bbox['max_x']] = cropped_image&#10;        else:&#10;            raise ValueError(f&quot;Unsupported ndim {cropped_image.ndim} in undo_crop_torso_roi&quot;)&#10;        return restored&#10;&#10;    def pad_image_to_correct_ratio(self, cropped_image, bbox):&#10;        &quot;&quot;&quot;&#10;        Pad the cropped image to achieve the target aspect ratio.&#10;        Args:&#10;            cropped_image (np.ndarray): Cropped image.&#10;            bbox (dict): Bounding box used for cropping.&#10;        Returns:&#10;            tuple: (padded_image, padding, padded_size)&#10;        &quot;&quot;&quot;&#10;        width = bbox['max_x'] - bbox['min_x']&#10;        height = bbox['max_y'] - bbox['min_y']&#10;        current_ratio = width / height&#10;&#10;        if current_ratio &lt; self.target_ratio:&#10;            padded_width = int(round(height * self.target_ratio))&#10;            padded_height = height&#10;            pad_total = padded_width - width&#10;            pad_left = pad_total // 2&#10;            pad_right = pad_total - pad_left&#10;            pad_top = 0&#10;            pad_bottom = 0&#10;        elif current_ratio &gt; self.target_ratio:&#10;            padded_width = width&#10;            padded_height = int(round(width / self.target_ratio))&#10;            pad_total = padded_height - height&#10;            pad_left = 0&#10;            pad_right = 0&#10;            pad_top = pad_total // 2&#10;            pad_bottom = pad_total - pad_top&#10;        else:&#10;            padded_width = width&#10;            padded_height = height&#10;            pad_left = pad_right = pad_top = pad_bottom = 0&#10;&#10;        padding = {&#10;            'left': pad_left,&#10;            'right': pad_right,&#10;            'top': pad_top,&#10;            'bottom': pad_bottom&#10;        }&#10;&#10;        padded_size = {&#10;            'width': padded_width,&#10;            'height': padded_height&#10;        }&#10;&#10;        padded_image = np.pad(&#10;            cropped_image,&#10;            ((padding['top'], padding['bottom']), (padding['left'], padding['right']), (0, 0)),&#10;            mode='constant',&#10;            constant_values=0&#10;        )&#10;        return padded_image, padding, padded_size&#10;&#10;    @staticmethod&#10;    def pad_image_with_parameters(cropped_image, padding):&#10;        &quot;&quot;&quot;&#10;        Pad an image using specified padding values.&#10;        Args:&#10;            cropped_image (np.ndarray): Cropped image.&#10;            padding (dict): Padding values for each side.&#10;        Returns:&#10;            np.ndarray: Padded image.&#10;        &quot;&quot;&quot;&#10;        padded_image = np.pad(&#10;            cropped_image,&#10;            ((padding['top'], padding['bottom']), (padding['left'], padding['right']), (0, 0)),&#10;            mode='constant',&#10;            constant_values=0&#10;        )&#10;        return padded_image&#10;&#10;    @staticmethod&#10;    def undo_pad_image_to_correct_ratio(padded_image, padding):&#10;        &quot;&quot;&quot;&#10;        Remove padding from an image using specified padding values.&#10;        Args:&#10;            padded_image (np.ndarray): Padded image.&#10;            padding (dict): Padding values for each side.&#10;        Returns:&#10;            np.ndarray: Unpadded image.&#10;        &quot;&quot;&quot;&#10;        top, bottom = padding['top'], padding['bottom']&#10;        left, right = padding['left'], padding['right']&#10;        height, width = padded_image.shape[:2]&#10;        unpadded_image = padded_image[&#10;            top:height - bottom if bottom &gt; 0 else height,&#10;            left:width - right if right &gt; 0 else width,&#10;            ...&#10;        ]&#10;        return unpadded_image&#10;&#10;    def rescale_image(self, cropped_and_padded_image, original_size):&#10;        &quot;&quot;&quot;&#10;        Rescale the image to the original size (width, height) after padding.&#10;        Args:&#10;            cropped_and_padded_image (np.ndarray): Image after cropping and padding.&#10;            original_size (dict): Original image size.&#10;        Returns:&#10;            np.ndarray: Rescaled image.&#10;        &quot;&quot;&quot;&#10;        target_height = original_size['width'] / self.target_ratio&#10;        output_shape = (target_height, original_size['width'], cropped_and_padded_image.shape[2])&#10;        rescaled_image = resize(&#10;            cropped_and_padded_image,&#10;            output_shape,&#10;            order=1,  # bilinear&#10;            mode='constant',&#10;            cval=0,&#10;            anti_aliasing=True,&#10;            preserve_range=True&#10;        ).astype(cropped_and_padded_image.dtype)&#10;        return rescaled_image&#10;&#10;    @staticmethod&#10;    def undo_rescale_image(rescaled_image, padded_size):&#10;        &quot;&quot;&quot;&#10;        Reverse the rescaling to restore the padded image to its original padded size.&#10;        Args:&#10;            rescaled_image (np.ndarray): Rescaled image.&#10;            padded_size (dict): Size after padding.&#10;        Returns:&#10;            np.ndarray: Unscaled image.&#10;        &quot;&quot;&quot;&#10;        # Automatically detect image type (color, grayscale, or binary)&#10;        is_binary = len(np.unique(rescaled_image)) &lt;= 2&#10;&#10;        if rescaled_image.ndim == 3:&#10;            # Color image&#10;            output_shape = (padded_size['height'], padded_size['width'], rescaled_image.shape[2])&#10;            if is_binary:&#10;                order = 0  # Nearest-neighbor for binary&#10;                anti_aliasing = False&#10;            else:&#10;                order = 1  # Bilinear for grayscale&#10;                anti_aliasing = True&#10;        elif rescaled_image.ndim == 2:&#10;            # Grayscale or binary image&#10;            output_shape = (padded_size['height'], padded_size['width'])&#10;            if is_binary:&#10;                order = 0  # Nearest-neighbor for binary&#10;                anti_aliasing = False&#10;            else:&#10;                order = 1  # Bilinear for grayscale&#10;                anti_aliasing = True&#10;        else:&#10;            raise ValueError(f&quot;Unsupported image ndim: {rescaled_image.ndim}. Image must be 2D or 3D.&quot;)&#10;&#10;        unscaled_image = resize(&#10;            rescaled_image,&#10;            output_shape,&#10;            order=order,&#10;            mode='constant',&#10;            cval=0,&#10;            anti_aliasing=anti_aliasing,&#10;            preserve_range=True&#10;        ).astype(rescaled_image.dtype)&#10;        return unscaled_image&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/segmenter/atlas_segmenter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/segmenter/atlas_segmenter.py" />
              <option name="originalContent" value="import os.path&#10;&#10;from tqdm import tqdm&#10;&#10;from segmenter.atlas.atlas import Atlas&#10;from segmenter.base_segmenter import BaseSegmenter&#10;from target_image.target_image import TargetImage&#10;from target_image.target_segmentation import TargetSegmentation&#10;&#10;&#10;class AtlasSegmenter(BaseSegmenter):&#10;    &quot;&quot;&quot;&#10;    Atlas-based segmenter for image segmentation tasks.&#10;    Inherits from BaseSegmenter and implements the segment_images method using an atlas-based approach.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, num_atlases_to_select, atlas_dir, preprocessing_steps, atlas_selector, segmentation_voter, segmentation_refiner, output_dir):&#10;        &quot;&quot;&quot;&#10;        Initialize the AtlasSegmenter.&#10;&#10;        Args:&#10;            num_atlases_to_select (int): Number of atlases to select for segmentation.&#10;            atlas_dir (str): Directory containing atlas images and masks.&#10;            preprocessing_steps (list): List of preprocessing step instances to apply to images.&#10;            atlas_selector (object): Atlas selection strategy.&#10;            segmentation_voter (object): Segmentation voting mechanism.&#10;            segmentation_refiner (object): Optional refinement step for segmentation results.&#10;            output_dir (str): Directory to save segmentation results.&#10;        &quot;&quot;&quot;&#10;        super().__init__(output_dir, preprocessing_steps, segmentation_refiner)&#10;        self.num_atlases_to_select = num_atlases_to_select&#10;        self.atlas_dir = atlas_dir&#10;        self.atlas_selector = atlas_selector&#10;        self.segmentation_voter = segmentation_voter&#10;&#10;    def load_atlases(self):&#10;        &quot;&quot;&quot;&#10;        Load atlases from the specified directory.&#10;&#10;        Returns:&#10;            list: List of Atlas objects loaded from the directory.&#10;        &quot;&quot;&quot;&#10;        atlases = []&#10;        for file in os.listdir(self.atlas_dir):&#10;            if file.endswith(&quot;-mask.Gauss&quot; + self.img_extension):&#10;                mask_path = os.path.join(self.atlas_dir, file)&#10;                prefix = file[:-15]&#10;                image_path = os.path.join(self.atlas_dir, prefix + &quot;.Gauss.png&quot;)&#10;                atlases.append(Atlas(image_path, mask_path))&#10;        return atlases&#10;&#10;    def segment_images(self, target_images: list[TargetImage]):&#10;        &quot;&quot;&quot;&#10;        Segment a list of target images using the atlas-based approach.&#10;&#10;        Args:&#10;            target_images (list[TargetImage]): List of TargetImage objects to segment.&#10;&#10;        Returns:&#10;            list: List of segmentation results.&#10;        &quot;&quot;&quot;&#10;        atlases = self.load_atlases()&#10;&#10;        # Preprocess atlases&#10;        for atlas in tqdm(atlases, desc='Preprocessing atlases'):&#10;            for pp_step in self.preprocessing_steps:&#10;                atlas.preprocessed_image, parameters = pp_step.preprocess_image(atlas.preprocessed_image)&#10;                atlas.append_preprocessing_parameters(parameters)&#10;                atlas.preprocessed_mask = pp_step.preprocess_mask(atlas.preprocessed_mask, parameters)&#10;&#10;        for target_image in tqdm(target_images, desc='Processed validation images'):&#10;            # Preprocessing&#10;            for pp_step in self.preprocessing_steps:&#10;                target_image.preprocessed_image, parameters = pp_step.preprocess_image(target_image.preprocessed_image)&#10;                target_image.append_preprocessing_parameters(parameters)&#10;&#10;            # Atlas selection&#10;            selected_atlases = self.atlas_selector.select_atlases(atlases, target_image, self.num_atlases_to_select)&#10;&#10;            # Segmentation voting&#10;            target_mask = self.segmentation_voter.vote(selected_atlases)&#10;&#10;            # Undo-Preprocessing (reversed)&#10;            for pp_step, parameters in reversed(list(zip(self.preprocessing_steps, target_image.preprocessing_parameters))):&#10;                # pp_step.undo_preprocessing(target_image.preprocessed_image, parameters, True)&#10;                target_mask = pp_step.undo_preprocessing(target_mask, parameters)&#10;&#10;            # Segmentation refinement (optional)&#10;            if self.segmentation_refiner is not None:&#10;                target_mask = self.segmentation_refiner.refine(target_mask, target_image)&#10;&#10;            # Save segmentation&#10;            target_segmentation_path = os.path.basename(target_image.image_path)[:-10] + &quot;-mask.Gauss.png&quot;&#10;            self.save_segmentation(TargetSegmentation(target_segmentation_path, target_mask))&#10;" />
              <option name="updatedContent" value="import os.path&#10;&#10;from tqdm import tqdm&#10;&#10;from segmenter.atlas.atlas import Atlas&#10;from segmenter.base_segmenter import BaseSegmenter&#10;from target_image.target_image import TargetImage&#10;from target_image.target_segmentation import TargetSegmentation&#10;&#10;&#10;class AtlasSegmenter(BaseSegmenter):&#10;    &quot;&quot;&quot;&#10;    Atlas-based segmenter for image segmentation tasks.&#10;    Inherits from BaseSegmenter and implements the segment_images method using an atlas-based approach.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, num_atlases_to_select, atlas_dir, preprocessing_steps, atlas_selector, segmentation_voter, segmentation_refiner, output_dir):&#10;        &quot;&quot;&quot;&#10;        Initialize the AtlasSegmenter.&#10;&#10;        Args:&#10;            num_atlases_to_select (int): Number of atlases to select for segmentation.&#10;            atlas_dir (str): Directory containing atlas images and masks.&#10;            preprocessing_steps (list): List of preprocessing step instances to apply to images.&#10;            atlas_selector (object): Atlas selection strategy.&#10;            segmentation_voter (object): Segmentation voting mechanism.&#10;            segmentation_refiner (object): Optional refinement step for segmentation results.&#10;            output_dir (str): Directory to save segmentation results.&#10;        &quot;&quot;&quot;&#10;        super().__init__(output_dir, preprocessing_steps, segmentation_refiner)&#10;        self.num_atlases_to_select = num_atlases_to_select&#10;        self.atlas_dir = atlas_dir&#10;        self.atlas_selector = atlas_selector&#10;        self.segmentation_voter = segmentation_voter&#10;&#10;    def load_atlases(self):&#10;        &quot;&quot;&quot;&#10;        Load atlases from the specified directory.&#10;&#10;        Returns:&#10;            list: List of Atlas objects loaded from the directory.&#10;        &quot;&quot;&quot;&#10;        atlases = []&#10;        for file in os.listdir(self.atlas_dir):&#10;            if file.endswith(&quot;-mask.Gauss&quot; + self.img_extension):&#10;                mask_path = os.path.join(self.atlas_dir, file)&#10;                prefix = file[:-15]&#10;                image_path = os.path.join(self.atlas_dir, prefix + &quot;.Gauss.png&quot;)&#10;                atlases.append(Atlas(image_path, mask_path))&#10;        return atlases&#10;&#10;    def segment_images(self, target_images: list[TargetImage]):&#10;        &quot;&quot;&quot;&#10;        Segment a list of target images using the atlas-based approach.&#10;&#10;        Args:&#10;            target_images (list[TargetImage]): List of TargetImage objects to segment.&#10;&#10;        Returns:&#10;            list: List of segmentation results.&#10;        &quot;&quot;&quot;&#10;        atlases = self.load_atlases()&#10;&#10;        # Preprocess atlases&#10;        for atlas in tqdm(atlases, desc='Preprocessing atlases'):&#10;            for pp_step in self.preprocessing_steps:&#10;                atlas.preprocessed_image, parameters = pp_step.preprocess_image(atlas.preprocessed_image)&#10;                atlas.append_preprocessing_parameters(parameters)&#10;                atlas.preprocessed_mask = pp_step.preprocess_mask(atlas.preprocessed_mask, parameters)&#10;&#10;        for target_image in tqdm(target_images, desc='Processed validation images'):&#10;            # Preprocessing&#10;            for pp_step in self.preprocessing_steps:&#10;                target_image.preprocessed_image, parameters = pp_step.preprocess_image(target_image.preprocessed_image)&#10;                target_image.append_preprocessing_parameters(parameters)&#10;&#10;            # Atlas selection&#10;            selected_atlases = self.atlas_selector.select_atlases(atlases, target_image, self.num_atlases_to_select)&#10;&#10;            # Segmentation voting&#10;            target_mask = self.segmentation_voter.vote(selected_atlases)&#10;&#10;            # Undo-Preprocessing (reversed)&#10;            for pp_step, parameters in reversed(list(zip(self.preprocessing_steps, target_image.preprocessing_parameters))):&#10;                # pp_step.undo_preprocessing(target_image.preprocessed_image, parameters, True)&#10;                target_mask = pp_step.undo_preprocessing(target_mask, parameters)&#10;&#10;            # Segmentation refinement (optional)&#10;            if self.segmentation_refiner is not None:&#10;                target_mask = self.segmentation_refiner.refine(target_mask, target_image)&#10;&#10;            # Save segmentation&#10;            target_segmentation_path = os.path.basename(target_image.image_path)[:-10] + &quot;-mask.Gauss.png&quot;&#10;            self.save_segmentation(TargetSegmentation(target_segmentation_path, target_mask))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/segmenter/base_segmenter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/segmenter/base_segmenter.py" />
              <option name="originalContent" value="import os&#10;from abc import ABC&#10;&#10;import numpy as np&#10;from skimage import io&#10;&#10;from segmenter.image_segmenter import IImageSegmenter&#10;from target_image.target_image import TargetImage&#10;&#10;&#10;class BaseSegmenter(IImageSegmenter, ABC):&#10;    &quot;&quot;&quot;&#10;    Abstract base class for image segmentation workflows.&#10;    Provides common logic for loading target images, saving segmentations, and managing preprocessing and refinement steps.&#10;    Subclasses must implement the segment_images method.&#10;&#10;    Args:&#10;        output_dir (str): Directory to save segmentation results.&#10;        preprocessing_steps (list): List of preprocessing step instances to apply to images.&#10;        segmentation_refiner (object): Optional refinement step for segmentation results.&#10;        img_extension (str): File extension for input images (default: '.png').&#10;    &quot;&quot;&quot;&#10;    def __init__(self, output_dir, preprocessing_steps, segmentation_refiner, img_extension=&quot;.png&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the BaseSegmenter.&#10;        Args:&#10;            output_dir (str): Directory to save segmentation results.&#10;            preprocessing_steps (list): List of preprocessing step instances to apply to images.&#10;            segmentation_refiner (object): Optional refinement step for segmentation results.&#10;            img_extension (str): File extension for input images (default: '.png').&#10;        &quot;&quot;&quot;&#10;        self.output_dir = output_dir&#10;        self.preprocessing_steps = preprocessing_steps&#10;        self.segmentation_refiner = segmentation_refiner&#10;        self.img_extension = img_extension&#10;&#10;    def load_target_images(self, directory_path):&#10;        &quot;&quot;&quot;&#10;        Load all target images from a directory, excluding mask files.&#10;        Args:&#10;            directory_path (str): Path to the directory containing input images.&#10;        Returns:&#10;            list[TargetImage]: List of TargetImage objects for segmentation.&#10;        &quot;&quot;&quot;&#10;        target_images = []&#10;        for file in os.listdir(directory_path):&#10;            if file.endswith(self.img_extension) and &quot;-mask&quot; not in file:&#10;                target_images.append(TargetImage(os.path.join(directory_path, file)))&#10;        return target_images&#10;&#10;    def segment_images(self, target_images):&#10;        &quot;&quot;&quot;&#10;        Abstract method for segmenting a list of target images.&#10;        Must be implemented by subclasses.&#10;        Args:&#10;            target_images (list[TargetImage]): List of TargetImage objects to segment.&#10;        Raises:&#10;            NotImplementedError: Always, unless implemented in subclass.&#10;        &quot;&quot;&quot;&#10;        raise NotImplementedError(&quot;Subclasses must implement segment method&quot;)&#10;&#10;    def save_segmentation(self, target_segmentation):&#10;        &quot;&quot;&quot;&#10;        Save a segmentation mask to the output directory as a PNG file.&#10;        Args:&#10;            target_segmentation (TargetSegmentation): Segmentation result to save. Must have 'output_path' and 'result_mask'.&#10;        &quot;&quot;&quot;&#10;        if not os.path.exists(self.output_dir):&#10;            os.makedirs(self.output_dir)&#10;&#10;        filepath = os.path.join(self.output_dir, target_segmentation.output_path)&#10;        segmentation = target_segmentation.result_mask&#10;        io.imsave(str(filepath), (segmentation * 255).astype(np.uint8))&#10;        print(&quot;Saved segmentation mask to {}&quot;.format(filepath))&#10;" />
              <option name="updatedContent" value="import os&#10;from abc import ABC&#10;&#10;import numpy as np&#10;from skimage import io&#10;&#10;from segmenter.image_segmenter import IImageSegmenter&#10;from target_image.target_image import TargetImage&#10;&#10;&#10;class BaseSegmenter(IImageSegmenter, ABC):&#10;    &quot;&quot;&quot;&#10;    Abstract base class for image segmentation workflows.&#10;    Provides common logic for loading target images, saving segmentations, and managing preprocessing and refinement steps.&#10;    Subclasses must implement the segment_images method.&#10;&#10;    Args:&#10;        output_dir (str): Directory to save segmentation results.&#10;        preprocessing_steps (list): List of preprocessing step instances to apply to images.&#10;        segmentation_refiner (object): Optional refinement step for segmentation results.&#10;        img_extension (str): File extension for input images (default: '.png').&#10;    &quot;&quot;&quot;&#10;    def __init__(self, output_dir, preprocessing_steps, segmentation_refiner, img_extension=&quot;.png&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the BaseSegmenter.&#10;        Args:&#10;            output_dir (str): Directory to save segmentation results.&#10;            preprocessing_steps (list): List of preprocessing step instances to apply to images.&#10;            segmentation_refiner (object): Optional refinement step for segmentation results.&#10;            img_extension (str): File extension for input images (default: '.png').&#10;        &quot;&quot;&quot;&#10;        self.output_dir = output_dir&#10;        self.preprocessing_steps = preprocessing_steps&#10;        self.segmentation_refiner = segmentation_refiner&#10;        self.img_extension = img_extension&#10;&#10;    def load_target_images(self, directory_path):&#10;        &quot;&quot;&quot;&#10;        Load all target images from a directory, excluding mask files.&#10;        Args:&#10;            directory_path (str): Path to the directory containing input images.&#10;        Returns:&#10;            list[TargetImage]: List of TargetImage objects for segmentation.&#10;        &quot;&quot;&quot;&#10;        target_images = []&#10;        for file in os.listdir(directory_path):&#10;            if file.endswith(self.img_extension) and &quot;-mask&quot; not in file:&#10;                target_images.append(TargetImage(os.path.join(directory_path, file)))&#10;        return target_images&#10;&#10;    def segment_images(self, target_images):&#10;        &quot;&quot;&quot;&#10;        Abstract method for segmenting a list of target images.&#10;        Must be implemented by subclasses.&#10;        Args:&#10;            target_images (list[TargetImage]): List of TargetImage objects to segment.&#10;        Raises:&#10;            NotImplementedError: Always, unless implemented in subclass.&#10;        &quot;&quot;&quot;&#10;        raise NotImplementedError(&quot;Subclasses must implement segment method&quot;)&#10;&#10;    def save_segmentation(self, target_segmentation):&#10;        &quot;&quot;&quot;&#10;        Save a segmentation mask to the output directory as a PNG file.&#10;        Args:&#10;            target_segmentation (TargetSegmentation): Segmentation result to save. Must have 'output_path' and 'result_mask'.&#10;        &quot;&quot;&quot;&#10;        if not os.path.exists(self.output_dir):&#10;            os.makedirs(self.output_dir)&#10;&#10;        filepath = os.path.join(self.output_dir, target_segmentation.output_path)&#10;        segmentation = target_segmentation.result_mask&#10;        io.imsave(str(filepath), (segmentation * 255).astype(np.uint8))&#10;        print(&quot;Saved segmentation mask to {}&quot;.format(filepath))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/segmenter/experiment_runner.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/segmenter/experiment_runner.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Module for running segmentation experiments.&#10;&#10;This module provides the ExperimentRunner class which handles loading target images,&#10;running the segmentation process via a provided segmenter, and recording timing&#10;information for the experiment.&#10;&quot;&quot;&quot;&#10;&#10;&#10;import os&#10;import time&#10;&#10;&#10;class ExperimentRunner:&#10;    &quot;&quot;&quot;&#10;    Class for running segmentation experiments and managing experiment results.&#10;    Provides methods to execute experiments, collect results, and save experiment outputs.&#10;&#10;    This class initializes with a segmenter and a directory of target images,&#10;    executes the segmentation process, and saves duration metrics.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, segmenter, target_images_dir):&#10;        &quot;&quot;&quot;&#10;        Initialize the ExperimentRunner.&#10;&#10;        Args:&#10;            segmenter: The segmenter instance to use for experiments. It must have&#10;                methods `load_target_images(directory)` and `segment_images(images)`,&#10;                and an attribute `output_dir`.&#10;            target_images_dir: Path to the directory containing target images.&#10;        &quot;&quot;&quot;&#10;        self.segmenter = segmenter&#10;        self.target_images_dir = target_images_dir&#10;&#10;    def run(self):&#10;        &quot;&quot;&quot;&#10;        Run the segmentation experiment using the provided segmenter and configuration.&#10;&#10;        Returns:&#10;            dict: Results of the experiment, including total and average duration.&#10;        &quot;&quot;&quot;&#10;        start_time = time.time()&#10;&#10;        target_images = self.segmenter.load_target_images(self.target_images_dir)&#10;        self.segmenter.segment_images(target_images)&#10;&#10;        end_time = time.time()&#10;        self.save_duration(start_time, end_time, len(target_images))&#10;&#10;    def save_duration(self, start_time, end_time, num_images):&#10;        &quot;&quot;&quot;&#10;        Calculate and save experiment duration.&#10;&#10;        Args:&#10;            start_time: Start time in seconds since the epoch.&#10;            end_time: End time in seconds since the epoch.&#10;            num_images: Number of images processed.&#10;&#10;        The method computes total duration and average duration per image,&#10;        formats them as HH:MM:SS.mmm strings, and writes them to a duration.txt&#10;        file in the segmenter's output directory.&#10;        &quot;&quot;&quot;&#10;        total_seconds = end_time - start_time&#10;&#10;        # format total duration h:m:s.ms&#10;        hrs = int(total_seconds // 3600)&#10;        mins = int((total_seconds % 3600) // 60)&#10;        secs_float = total_seconds % 60&#10;        secs = int(secs_float)&#10;        millis = int((secs_float - secs) * 1000)&#10;        duration_str = f&quot;{hrs:02d}:{mins:02d}:{secs:02d}.{millis:03d}&quot;&#10;&#10;        # average per image&#10;        if num_images &gt; 0:&#10;            avg_seconds = total_seconds / num_images&#10;            avg_hrs = int(avg_seconds // 3600)&#10;            avg_mins = int((avg_seconds % 3600) // 60)&#10;            avg_secs_float = avg_seconds % 60&#10;            avg_secs = int(avg_secs_float)&#10;            avg_millis = int((avg_secs_float - avg_secs) * 1000)&#10;            avg_str = f&quot;{avg_hrs:02d}:{avg_mins:02d}:{avg_secs:02d}.{avg_millis:03d}&quot;&#10;        else:&#10;            avg_str = &quot;00:00:00&quot;&#10;&#10;        # write durations to file in output_dir&#10;        duration_file = os.path.join(self.segmenter.output_dir, &quot;duration.txt&quot;)&#10;        with open(duration_file, &quot;w&quot;) as f:&#10;            f.write(f&quot;Total duration: {duration_str}\n&quot;)&#10;            f.write(f&quot;Average per image: {avg_str}\n&quot;)&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Module for running segmentation experiments.&#10;&#10;This module provides the ExperimentRunner class which handles loading target images,&#10;running the segmentation process via a provided segmenter, and recording timing&#10;information for the experiment.&#10;&quot;&quot;&quot;&#10;&#10;&#10;import os&#10;import time&#10;&#10;&#10;class ExperimentRunner:&#10;    &quot;&quot;&quot;&#10;    Class for running segmentation experiments and managing experiment results.&#10;    Provides methods to execute experiments, collect results, and save experiment outputs.&#10;&#10;    This class initializes with a segmenter and a directory of target images,&#10;    executes the segmentation process, and saves duration metrics.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, segmenter, target_images_dir):&#10;        &quot;&quot;&quot;&#10;        Initialize the ExperimentRunner.&#10;&#10;        Args:&#10;            segmenter: The segmenter instance to use for experiments. It must have&#10;                methods `load_target_images(directory)` and `segment_images(images)`,&#10;                and an attribute `output_dir`.&#10;            target_images_dir: Path to the directory containing target images.&#10;        &quot;&quot;&quot;&#10;        self.segmenter = segmenter&#10;        self.target_images_dir = target_images_dir&#10;&#10;    def run(self):&#10;        &quot;&quot;&quot;&#10;        Run the segmentation experiment using the provided segmenter and configuration.&#10;&#10;        Returns:&#10;            dict: Results of the experiment, including total and average duration.&#10;        &quot;&quot;&quot;&#10;        start_time = time.time()&#10;&#10;        target_images = self.segmenter.load_target_images(self.target_images_dir)&#10;        self.segmenter.segment_images(target_images)&#10;&#10;        end_time = time.time()&#10;        self.save_duration(start_time, end_time, len(target_images))&#10;&#10;    def save_duration(self, start_time, end_time, num_images):&#10;        &quot;&quot;&quot;&#10;        Calculate and save experiment duration.&#10;&#10;        Args:&#10;            start_time: Start time in seconds since the epoch.&#10;            end_time: End time in seconds since the epoch.&#10;            num_images: Number of images processed.&#10;&#10;        The method computes total duration and average duration per image,&#10;        formats them as HH:MM:SS.mmm strings, and writes them to a duration.txt&#10;        file in the segmenter's output directory.&#10;        &quot;&quot;&quot;&#10;        total_seconds = end_time - start_time&#10;&#10;        # format total duration h:m:s.ms&#10;        hrs = int(total_seconds // 3600)&#10;        mins = int((total_seconds % 3600) // 60)&#10;        secs_float = total_seconds % 60&#10;        secs = int(secs_float)&#10;        millis = int((secs_float - secs) * 1000)&#10;        duration_str = f&quot;{hrs:02d}:{mins:02d}:{secs:02d}.{millis:03d}&quot;&#10;&#10;        # average per image&#10;        if num_images &gt; 0:&#10;            avg_seconds = total_seconds / num_images&#10;            avg_hrs = int(avg_seconds // 3600)&#10;            avg_mins = int((avg_seconds % 3600) // 60)&#10;            avg_secs_float = avg_seconds % 60&#10;            avg_secs = int(avg_secs_float)&#10;            avg_millis = int((avg_secs_float - avg_secs) * 1000)&#10;            avg_str = f&quot;{avg_hrs:02d}:{avg_mins:02d}:{avg_secs:02d}.{avg_millis:03d}&quot;&#10;        else:&#10;            avg_str = &quot;00:00:00&quot;&#10;&#10;        # write durations to file in output_dir&#10;        duration_file = os.path.join(self.segmenter.output_dir, &quot;duration.txt&quot;)&#10;        with open(duration_file, &quot;w&quot;) as f:&#10;            f.write(f&quot;Total duration: {duration_str}\n&quot;)&#10;            f.write(f&quot;Average per image: {avg_str}\n&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/segmenter/image_segmenter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/segmenter/image_segmenter.py" />
              <option name="originalContent" value="from abc import ABC, abstractmethod&#10;&#10;&#10;class IImageSegmenter(ABC):&#10;    &quot;&quot;&quot;&#10;    Interface for image segmentation workflows.&#10;    All segmenter classes should inherit from this interface and implement the required methods for loading images,&#10;    segmenting them, and saving the segmentation results.&#10;    &quot;&quot;&quot;&#10;&#10;    @abstractmethod&#10;    def load_target_images(self, directory_path):&#10;        &quot;&quot;&quot;&#10;        Load all target images from a directory for segmentation.&#10;&#10;        Args:&#10;            directory_path (str): Path to the directory containing input images.&#10;&#10;        Returns:&#10;            list: List of loaded images or image objects for segmentation.&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def segment_images(self, image_paths):&#10;        &quot;&quot;&quot;&#10;        Segment a list of images.&#10;&#10;        Args:&#10;            image_paths (list): List of image paths or image objects to segment.&#10;&#10;        Returns:&#10;            list: List of segmentation results.&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def save_segmentation(self, segmented_image):&#10;        &quot;&quot;&quot;&#10;        Save a segmentation result to disk or another output format.&#10;&#10;        Args:&#10;            segmented_image: The segmentation result to save.&#10;        &quot;&quot;&quot;&#10;        pass&#10;" />
              <option name="updatedContent" value="from abc import ABC, abstractmethod&#10;&#10;&#10;class IImageSegmenter(ABC):&#10;    &quot;&quot;&quot;&#10;    Interface for image segmentation workflows.&#10;    All segmenter classes should inherit from this interface and implement the required methods for loading images,&#10;    segmenting them, and saving the segmentation results.&#10;    &quot;&quot;&quot;&#10;&#10;    @abstractmethod&#10;    def load_target_images(self, directory_path):&#10;        &quot;&quot;&quot;&#10;        Load all target images from a directory for segmentation.&#10;&#10;        Args:&#10;            directory_path (str): Path to the directory containing input images.&#10;&#10;        Returns:&#10;            list: List of loaded images or image objects for segmentation.&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def segment_images(self, image_paths):&#10;        &quot;&quot;&quot;&#10;        Segment a list of images.&#10;&#10;        Args:&#10;            image_paths (list): List of image paths or image objects to segment.&#10;&#10;        Returns:&#10;            list: List of segmentation results.&#10;        &quot;&quot;&quot;&#10;        pass&#10;&#10;    @abstractmethod&#10;    def save_segmentation(self, segmented_image):&#10;        &quot;&quot;&quot;&#10;        Save a segmentation result to disk or another output format.&#10;&#10;        Args:&#10;            segmented_image: The segmentation result to save.&#10;        &quot;&quot;&quot;&#10;        pass" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/segmenter/ml_segmenter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/segmenter/ml_segmenter.py" />
              <option name="originalContent" value="import os&#10;from typing import Optional&#10;&#10;import numpy as np&#10;import torch&#10;import torch.nn.functional as F&#10;from torch import nn&#10;from tqdm import tqdm&#10;&#10;from segmenter.base_segmenter import BaseSegmenter&#10;from ssl.downstream.simple_classifier.segmentation_head import SegmentationHead&#10;from ssl.downstream.unet.jigsaw_absolut_position.unet_decoder_jigsaw_abs_pos import UnetDecoderJigsawAbsPos&#10;from ssl.downstream.unet.jigsaw_permutation.alex_net_attention_unet_decoder import AlexNetAttentionUNetDecoder&#10;from ssl.pretext.jigsaw_permutation.JigsawPermNetwork import JigsawPermNetwork&#10;from target_image.target_image import TargetImage&#10;from target_image.target_segmentation import TargetSegmentation&#10;from postprocessing.segmentation_refiner import ISegmentationRefiner&#10;&#10;&#10;class MLSegmenter(BaseSegmenter):&#10;    &quot;&quot;&quot;&#10;    Machine learning-based segmenter for image segmentation tasks.&#10;    Inherits from BaseSegmenter and implements the segment_images method using a trained ML model.&#10;&#10;    Args:&#10;        model_type (str): Type of the model (e.g., 'attention_unet').&#10;        weights_path (str): Path to the model weights.&#10;        backbone (str): Backbone architecture (e.g., 'jigsaw_abs_pos', 'jigsaw_perm').&#10;        pretext_classes (int): Number of pretext classes.&#10;        downstream_classes (int): Number of downstream classes.&#10;        output_dir (str): Directory to save segmentation results.&#10;        device (str, optional): Device to run the model on (e.g., 'cpu', 'cuda').&#10;        preprocessing_steps (list, optional): List of preprocessing step instances to apply to images.&#10;        segmentation_refiner (Optional[ISegmentationRefiner], optional): Optional refinement step for segmentation results. Must implement ISegmentationRefiner.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, model_type, weights_path, backbone, pretext_classes, downstream_classes, output_dir, device=None, preprocessing_steps=None, segmentation_refiner: Optional[ISegmentationRefiner]=None):&#10;        &quot;&quot;&quot;&#10;        Initialize the MLSegmenter.&#10;        Args:&#10;            model_type (str): Type of the model (e.g., 'attention_unet').&#10;            weights_path (str): Path to the model weights.&#10;            backbone (str): Backbone architecture (e.g., 'jigsaw_abs_pos', 'jigsaw_perm').&#10;            pretext_classes (int): Number of pretext classes.&#10;            downstream_classes (int): Number of downstream classes.&#10;            output_dir (str): Directory to save segmentation results.&#10;            device (str, optional): Device to run the model on (e.g., 'cpu', 'cuda').&#10;            preprocessing_steps (list, optional): List of preprocessing step instances to apply to images.&#10;            segmentation_refiner (Optional[ISegmentationRefiner], optional): Optional refinement step for segmentation results. Must implement ISegmentationRefiner.&#10;        Raises:&#10;            TypeError: If segmentation_refiner is not None and does not implement ISegmentationRefiner.&#10;        &quot;&quot;&quot;&#10;        if segmentation_refiner is not None and not isinstance(segmentation_refiner, ISegmentationRefiner):&#10;            raise TypeError(&quot;segmentation_refiner must implement ISegmentationRefiner&quot;)&#10;        super().__init__(output_dir, preprocessing_steps, segmentation_refiner)&#10;        self.model_type = model_type&#10;        self.weights_path = weights_path&#10;        self.backbone = backbone&#10;        self.pretext_classes = pretext_classes&#10;        self.downstream_classes = downstream_classes&#10;        self.device = self.select_device(device)&#10;        self.model = self.build_model().to(self.device)&#10;        self.load_model()&#10;&#10;    @staticmethod&#10;    def select_device(selected_device=None):&#10;        &quot;&quot;&quot;&#10;        Select the device to run the model on.&#10;        Args:&#10;            selected_device (str, optional): Device selected by the user (e.g., 'cpu', 'cuda').&#10;        Returns:&#10;            torch.device: Selected device.&#10;        &quot;&quot;&quot;&#10;        if selected_device is not None:&#10;            return torch.device(selected_device)&#10;        if torch.backends.mps.is_available(): # Apple Silicon&#10;            return torch.device(&quot;mps&quot;)&#10;        return torch.device(&quot;cpu&quot;)&#10;&#10;    def build_model(self):&#10;        &quot;&quot;&quot;&#10;        Build the segmentation model based on the selected architecture and backbone.&#10;        Returns:&#10;            torch.nn.Module: Constructed segmentation model.&#10;        &quot;&quot;&quot;&#10;        if self.model_type == &quot;attention_unet&quot; and self.backbone == &quot;jigsaw_abs_pos&quot;:&#10;            return UnetDecoderJigsawAbsPos(pretext_model_path=None, num_classes=self.downstream_classes, pretext_classes=self.pretext_classes)&#10;        elif self.model_type == &quot;attention_unet&quot; and self.backbone == &quot;jigsaw_perm&quot;:&#10;            encoder = JigsawPermNetwork(classes=self.pretext_classes).conv&#10;            return AlexNetAttentionUNetDecoder(encoder=encoder, num_classes=self.downstream_classes)&#10;        elif self.model_type == &quot;segmentation_head&quot; and self.backbone == &quot;jigsaw_perm&quot;:&#10;            encoder = JigsawPermNetwork(classes=self.pretext_classes).conv&#10;            head = SegmentationHead(num_classes=self.downstream_classes)&#10;            return nn.Sequential(encoder, head)&#10;        else:&#10;            raise ValueError(f&quot;Unknown combination of model_type: {self.model_type} and backbone: {self.backbone}&quot;)&#10;&#10;    def load_model(self):&#10;        &quot;&quot;&quot;&#10;        Load the model weights from the specified path.&#10;        &quot;&quot;&quot;&#10;        ckpt = torch.load(self.weights_path, map_location=self.device)&#10;        state = ckpt[&quot;model_state_dict&quot;] if isinstance(ckpt, dict) and &quot;model_state_dict&quot; in ckpt else ckpt&#10;        self.model.load_state_dict(state)&#10;        self.model.eval()&#10;        print(f&quot;Model loaded: {self.weights_path}&quot;)&#10;&#10;    @staticmethod&#10;    def _ensure_tensor_chw(arr_or_tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Ensure the input is a tensor with shape (C, H, W).&#10;        Args:&#10;            arr_or_tensor (numpy.ndarray or torch.Tensor): Input image as numpy array or torch tensor.&#10;        Returns:&#10;            torch.Tensor: Image tensor with shape (C, H, W).&#10;        &quot;&quot;&quot;&#10;        if torch.is_tensor(arr_or_tensor):&#10;            t = arr_or_tensor&#10;            if t.dim() == 2:&#10;                t = t.unsqueeze(0)  # (1,H,W)&#10;            elif t.dim() == 3 and t.shape[-1] in (1, 3):&#10;                t = t.permute(2, 0, 1)  # (H,W,C) -&gt; (C,H,W)&#10;            # otherwise already (C,H,W)&#10;            t = t.float()&#10;            if t.max() &gt; 1.0:&#10;                t = t / 255.0&#10;            return t&#10;&#10;        # numpy / list&#10;        arr = np.asarray(arr_or_tensor)&#10;        if arr.ndim == 2:&#10;            arr = arr[None, ...]  # (1,H,W)&#10;        elif arr.ndim == 3 and arr.shape[-1] in (1, 3):&#10;            arr = np.transpose(arr, (2, 0, 1))  # (H,W,C) -&gt; (C,H,W)&#10;        elif arr.ndim == 3 and arr.shape[0] in (1, 3):&#10;            pass&#10;        else:&#10;            raise ValueError(&quot;Unexpected image shape; need (H,W), (H,W,C) or (C,H,W)&quot;)&#10;        arr = arr.astype(np.float32)&#10;        if arr.max() &gt; 1.0:&#10;            arr = arr / 255.0&#10;        return torch.from_numpy(arr)&#10;&#10;    @torch.inference_mode()&#10;    def segment_images(self, target_images: list[TargetImage]):&#10;        &quot;&quot;&quot;&#10;        Segment a list of target images using the trained ML model.&#10;        Args:&#10;            target_images (list[TargetImage]): List of TargetImage objects to segment.&#10;        Returns:&#10;            list: List of segmentation results.&#10;        &quot;&quot;&quot;&#10;        for target_image in tqdm(target_images, desc=&quot;Segmenting images (1-by-1)&quot;):&#10;            # Preprocessing&#10;            for pp_step in self.preprocessing_steps:&#10;                target_image.preprocessed_image, parameters = pp_step.preprocess_image(target_image.preprocessed_image)&#10;                target_image.append_preprocessing_parameters(parameters)&#10;&#10;            # To tensor, on device, batch dim&#10;            x = self._ensure_tensor_chw(target_image.preprocessed_image).unsqueeze(0).to(self.device)  # (1,C,H,W)&#10;            in_h, in_w = x.shape[-2:]&#10;&#10;            # Forward, map logits to input size if necessary (as in training)&#10;            logits = self.model(x)  # (1,1,h,w) oder (1,C,h,w)&#10;            if logits.shape[-2:] != (in_h, in_w):&#10;                logits = F.interpolate(logits, size=(in_h, in_w), mode='bilinear', align_corners=False)&#10;&#10;            # Binary target_mask (1 downstream class)&#10;            if self.downstream_classes == 1:&#10;                probs = torch.sigmoid(logits)&#10;                pred = (probs &gt; 0.5).float()  # (1,1,H,W)&#10;                target_mask = pred.squeeze(0).squeeze(0).cpu().numpy()  # (H,W) in {0,1}&#10;            else:&#10;                # If multiple classes: Softmax + Argmax, etc.&#10;                probs = torch.softmax(logits, dim=1)&#10;                cls = probs.argmax(dim=1).squeeze(0).cpu().numpy()  # (H,W) in {0..C-1}&#10;                target_mask = (cls &gt; 0).astype(np.float32)&#10;&#10;            # Undo-Preprocessing (reversed))&#10;            for pp_step, params in reversed(list(zip(self.preprocessing_steps, target_image.preprocessing_parameters))):&#10;                # pp_step.undo_preprocessing(target_image.preprocessed_image, params, True)&#10;                target_mask = pp_step.undo_preprocessing(target_mask, params)&#10;&#10;            # Segmentation refinement (optional)&#10;            if self.segmentation_refiner is not None:&#10;                target_mask = self.segmentation_refiner.refine(target_mask, target_image)&#10;&#10;            # Save segmentation&#10;            target_segmentation_path = os.path.basename(target_image.image_path)[:-10] + &quot;-mask.Gauss.png&quot;&#10;            self.save_segmentation(TargetSegmentation(target_segmentation_path, target_mask))&#10;" />
              <option name="updatedContent" value="import os&#10;from typing import Optional&#10;&#10;import numpy as np&#10;import torch&#10;import torch.nn.functional as F&#10;from torch import nn&#10;from tqdm import tqdm&#10;&#10;from segmenter.base_segmenter import BaseSegmenter&#10;from ssl.downstream.simple_classifier.segmentation_head import SegmentationHead&#10;from ssl.downstream.unet.jigsaw_absolut_position.unet_decoder_jigsaw_abs_pos import UnetDecoderJigsawAbsPos&#10;from ssl.downstream.unet.jigsaw_permutation.alex_net_attention_unet_decoder import AlexNetAttentionUNetDecoder&#10;from ssl.pretext.jigsaw_permutation.JigsawPermNetwork import JigsawPermNetwork&#10;from target_image.target_image import TargetImage&#10;from target_image.target_segmentation import TargetSegmentation&#10;from postprocessing.segmentation_refiner import ISegmentationRefiner&#10;&#10;&#10;class MLSegmenter(BaseSegmenter):&#10;    &quot;&quot;&quot;&#10;    Machine learning-based segmenter for image segmentation tasks.&#10;    Inherits from BaseSegmenter and implements the segment_images method using a trained ML model.&#10;&#10;    Args:&#10;        model_type (str): Type of the model (e.g., 'attention_unet').&#10;        weights_path (str): Path to the model weights.&#10;        backbone (str): Backbone architecture (e.g., 'jigsaw_abs_pos', 'jigsaw_perm').&#10;        pretext_classes (int): Number of pretext classes.&#10;        downstream_classes (int): Number of downstream classes.&#10;        output_dir (str): Directory to save segmentation results.&#10;        device (str, optional): Device to run the model on (e.g., 'cpu', 'cuda').&#10;        preprocessing_steps (list, optional): List of preprocessing step instances to apply to images.&#10;        segmentation_refiner (Optional[ISegmentationRefiner], optional): Optional refinement step for segmentation results. Must implement ISegmentationRefiner.&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, model_type, weights_path, backbone, pretext_classes, downstream_classes, output_dir, device=None, preprocessing_steps=None, segmentation_refiner: Optional[ISegmentationRefiner]=None):&#10;        &quot;&quot;&quot;&#10;        Initialize the MLSegmenter.&#10;        Args:&#10;            model_type (str): Type of the model (e.g., 'attention_unet').&#10;            weights_path (str): Path to the model weights.&#10;            backbone (str): Backbone architecture (e.g., 'jigsaw_abs_pos', 'jigsaw_perm').&#10;            pretext_classes (int): Number of pretext classes.&#10;            downstream_classes (int): Number of downstream classes.&#10;            output_dir (str): Directory to save segmentation results.&#10;            device (str, optional): Device to run the model on (e.g., 'cpu', 'cuda').&#10;            preprocessing_steps (list, optional): List of preprocessing step instances to apply to images.&#10;            segmentation_refiner (Optional[ISegmentationRefiner], optional): Optional refinement step for segmentation results. Must implement ISegmentationRefiner.&#10;        Raises:&#10;            TypeError: If segmentation_refiner is not None and does not implement ISegmentationRefiner.&#10;        &quot;&quot;&quot;&#10;        if segmentation_refiner is not None and not isinstance(segmentation_refiner, ISegmentationRefiner):&#10;            raise TypeError(&quot;segmentation_refiner must implement ISegmentationRefiner&quot;)&#10;        super().__init__(output_dir, preprocessing_steps, segmentation_refiner)&#10;        self.model_type = model_type&#10;        self.weights_path = weights_path&#10;        self.backbone = backbone&#10;        self.pretext_classes = pretext_classes&#10;        self.downstream_classes = downstream_classes&#10;        self.device = self.select_device(device)&#10;        self.model = self.build_model().to(self.device)&#10;        self.load_model()&#10;&#10;    @staticmethod&#10;    def select_device(selected_device=None):&#10;        &quot;&quot;&quot;&#10;        Select the device to run the model on.&#10;        Args:&#10;            selected_device (str, optional): Device selected by the user (e.g., 'cpu', 'cuda').&#10;        Returns:&#10;            torch.device: Selected device.&#10;        &quot;&quot;&quot;&#10;        if selected_device is not None:&#10;            return torch.device(selected_device)&#10;        if torch.backends.mps.is_available(): # Apple Silicon&#10;            return torch.device(&quot;mps&quot;)&#10;        return torch.device(&quot;cpu&quot;)&#10;&#10;    def build_model(self):&#10;        &quot;&quot;&quot;&#10;        Build the segmentation model based on the selected architecture and backbone.&#10;        Returns:&#10;            torch.nn.Module: Constructed segmentation model.&#10;        &quot;&quot;&quot;&#10;        if self.model_type == &quot;attention_unet&quot; and self.backbone == &quot;jigsaw_abs_pos&quot;:&#10;            return UnetDecoderJigsawAbsPos(pretext_model_path=None, num_classes=self.downstream_classes, pretext_classes=self.pretext_classes)&#10;        elif self.model_type == &quot;attention_unet&quot; and self.backbone == &quot;jigsaw_perm&quot;:&#10;            encoder = JigsawPermNetwork(classes=self.pretext_classes).conv&#10;            return AlexNetAttentionUNetDecoder(encoder=encoder, num_classes=self.downstream_classes)&#10;        elif self.model_type == &quot;segmentation_head&quot; and self.backbone == &quot;jigsaw_perm&quot;:&#10;            encoder = JigsawPermNetwork(classes=self.pretext_classes).conv&#10;            head = SegmentationHead(num_classes=self.downstream_classes)&#10;            return nn.Sequential(encoder, head)&#10;        else:&#10;            raise ValueError(f&quot;Unknown combination of model_type: {self.model_type} and backbone: {self.backbone}&quot;)&#10;&#10;    def load_model(self):&#10;        &quot;&quot;&quot;&#10;        Load the model weights from the specified path.&#10;        &quot;&quot;&quot;&#10;        ckpt = torch.load(self.weights_path, map_location=self.device)&#10;        state = ckpt[&quot;model_state_dict&quot;] if isinstance(ckpt, dict) and &quot;model_state_dict&quot; in ckpt else ckpt&#10;        self.model.load_state_dict(state)&#10;        self.model.eval()&#10;        print(f&quot;Model loaded: {self.weights_path}&quot;)&#10;&#10;    @staticmethod&#10;    def _ensure_tensor_chw(arr_or_tensor) -&gt; torch.Tensor:&#10;        &quot;&quot;&quot;&#10;        Ensure the input is a tensor with shape (C, H, W).&#10;        Args:&#10;            arr_or_tensor (numpy.ndarray or torch.Tensor): Input image as numpy array or torch tensor.&#10;        Returns:&#10;            torch.Tensor: Image tensor with shape (C, H, W).&#10;        &quot;&quot;&quot;&#10;        if torch.is_tensor(arr_or_tensor):&#10;            t = arr_or_tensor&#10;            if t.dim() == 2:&#10;                t = t.unsqueeze(0)  # (1,H,W)&#10;            elif t.dim() == 3 and t.shape[-1] in (1, 3):&#10;                t = t.permute(2, 0, 1)  # (H,W,C) -&gt; (C,H,W)&#10;            # otherwise already (C,H,W)&#10;            t = t.float()&#10;            if t.max() &gt; 1.0:&#10;                t = t / 255.0&#10;            return t&#10;&#10;        # numpy / list&#10;        arr = np.asarray(arr_or_tensor)&#10;        if arr.ndim == 2:&#10;            arr = arr[None, ...]  # (1,H,W)&#10;        elif arr.ndim == 3 and arr.shape[-1] in (1, 3):&#10;            arr = np.transpose(arr, (2, 0, 1))  # (H,W,C) -&gt; (C,H,W)&#10;        elif arr.ndim == 3 and arr.shape[0] in (1, 3):&#10;            pass&#10;        else:&#10;            raise ValueError(&quot;Unexpected image shape; need (H,W), (H,W,C) or (C,H,W)&quot;)&#10;        arr = arr.astype(np.float32)&#10;        if arr.max() &gt; 1.0:&#10;            arr = arr / 255.0&#10;        return torch.from_numpy(arr)&#10;&#10;    @torch.inference_mode()&#10;    def segment_images(self, target_images: list[TargetImage]):&#10;        &quot;&quot;&quot;&#10;        Segment a list of target images using the trained ML model.&#10;        Args:&#10;            target_images (list[TargetImage]): List of TargetImage objects to segment.&#10;        Returns:&#10;            list: List of segmentation results.&#10;        &quot;&quot;&quot;&#10;        for target_image in tqdm(target_images, desc=&quot;Segmenting images (1-by-1)&quot;):&#10;            # Preprocessing&#10;            for pp_step in self.preprocessing_steps:&#10;                target_image.preprocessed_image, parameters = pp_step.preprocess_image(target_image.preprocessed_image)&#10;                target_image.append_preprocessing_parameters(parameters)&#10;&#10;            # To tensor, on device, batch dim&#10;            x = self._ensure_tensor_chw(target_image.preprocessed_image).unsqueeze(0).to(self.device)  # (1,C,H,W)&#10;            in_h, in_w = x.shape[-2:]&#10;&#10;            # Forward, map logits to input size if necessary (as in training)&#10;            logits = self.model(x)  # (1,1,h,w) oder (1,C,h,w)&#10;            if logits.shape[-2:] != (in_h, in_w):&#10;                logits = F.interpolate(logits, size=(in_h, in_w), mode='bilinear', align_corners=False)&#10;&#10;            # Binary target_mask (1 downstream class)&#10;            if self.downstream_classes == 1:&#10;                probs = torch.sigmoid(logits)&#10;                pred = (probs &gt; 0.5).float()  # (1,1,H,W)&#10;                target_mask = pred.squeeze(0).squeeze(0).cpu().numpy()  # (H,W) in {0,1}&#10;            else:&#10;                # If multiple classes: Softmax + Argmax, etc.&#10;                probs = torch.softmax(logits, dim=1)&#10;                cls = probs.argmax(dim=1).squeeze(0).cpu().numpy()  # (H,W) in {0..C-1}&#10;                target_mask = (cls &gt; 0).astype(np.float32)&#10;&#10;            # Undo-Preprocessing (reversed))&#10;            for pp_step, params in reversed(list(zip(self.preprocessing_steps, target_image.preprocessing_parameters))):&#10;                # pp_step.undo_preprocessing(target_image.preprocessed_image, params, True)&#10;                target_mask = pp_step.undo_preprocessing(target_mask, params)&#10;&#10;            # Segmentation refinement (optional)&#10;            if self.segmentation_refiner is not None:&#10;                target_mask = self.segmentation_refiner.refine(target_mask, target_image)&#10;&#10;            # Save segmentation&#10;            target_segmentation_path = os.path.basename(target_image.image_path)[:-10] + &quot;-mask.Gauss.png&quot;&#10;            self.save_segmentation(TargetSegmentation(target_segmentation_path, target_mask))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/validation/aggregator.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/validation/aggregator.py" />
              <option name="originalContent" value="import numpy as np&#10;from collections import defaultdict&#10;from validation.evaluation_metrics import EvaluationMetrics&#10;&#10;&#10;def _safe_nanmean(arr):&#10;    if len(arr) == 0:&#10;        return None&#10;    return float(np.nanmean(arr))&#10;&#10;&#10;class MetricsAggregator:&#10;    &quot;&quot;&quot;Aggregate EvaluationMetrics into grouped means dynamically based on selected metrics.&quot;&quot;&quot;&#10;    def __init__(self, metric_names):&#10;        self.metric_names = metric_names&#10;        self.metrics_by_set = defaultdict(list)&#10;        self.metrics_by_sick = {'Sick': [], 'Healthy': []}&#10;        self.all_metrics = []&#10;&#10;    def add(self, dataset: str, sick: float, metrics: EvaluationMetrics):&#10;        self.metrics_by_set[dataset].append(metrics)&#10;        if sick == 1.0:&#10;            self.metrics_by_sick['Sick'].append(metrics)&#10;        elif sick == 0.0:&#10;            self.metrics_by_sick['Healthy'].append(metrics)&#10;        self.all_metrics.append(metrics)&#10;&#10;    def compute_means(self):&#10;        &quot;&quot;&quot;Return list of rows for mean CSV: per dataset, Sick, Healthy, All Datasets&quot;&quot;&quot;&#10;        rows = []&#10;        # per-dataset&#10;        for ds, mlist in self.metrics_by_set.items():&#10;            rows.append(self._make_row(ds, mlist))&#10;        # Sick and Healthy&#10;        for status in ['Sick', 'Healthy']:&#10;            rows.append(self._make_row(status, self.metrics_by_sick.get(status, [])))&#10;        # overall&#10;        rows.append(self._make_row('All Datasets', self.all_metrics))&#10;        return rows&#10;&#10;    def _make_row(self, label, mlist):&#10;        # compute mean per selected metric&#10;        row = [label]&#10;        for name in self.metric_names:&#10;            vals = [getattr(m, name) for m in mlist if getattr(m, name) is not None]&#10;            mean = float(np.nanmean(vals)) if vals else None&#10;            row.append(mean)&#10;        return row" />
              <option name="updatedContent" value="import numpy as np&#10;from collections import defaultdict&#10;from validation.evaluation_metrics import EvaluationMetrics&#10;&#10;&#10;def _safe_nanmean(arr):&#10;    if len(arr) == 0:&#10;        return None&#10;    return float(np.nanmean(arr))&#10;&#10;&#10;class MetricsAggregator:&#10;    &quot;&quot;&quot;Aggregate EvaluationMetrics into grouped means dynamically based on selected metrics.&quot;&quot;&quot;&#10;    def __init__(self, metric_names):&#10;        self.metric_names = metric_names&#10;        self.metrics_by_set = defaultdict(list)&#10;        self.metrics_by_sick = {'Sick': [], 'Healthy': []}&#10;        self.all_metrics = []&#10;&#10;    def add(self, dataset: str, sick: float, metrics: EvaluationMetrics):&#10;        self.metrics_by_set[dataset].append(metrics)&#10;        if sick == 1.0:&#10;            self.metrics_by_sick['Sick'].append(metrics)&#10;        elif sick == 0.0:&#10;            self.metrics_by_sick['Healthy'].append(metrics)&#10;        self.all_metrics.append(metrics)&#10;&#10;    def compute_means(self):&#10;        &quot;&quot;&quot;Return list of rows for mean CSV: per dataset, Sick, Healthy, All Datasets&quot;&quot;&quot;&#10;        rows = []&#10;        # per-dataset&#10;        for ds, mlist in self.metrics_by_set.items():&#10;            rows.append(self._make_row(ds, mlist))&#10;        # Sick and Healthy&#10;        for status in ['Sick', 'Healthy']:&#10;            rows.append(self._make_row(status, self.metrics_by_sick.get(status, [])))&#10;        # overall&#10;        rows.append(self._make_row('All Datasets', self.all_metrics))&#10;        return rows&#10;&#10;    def _make_row(self, label, mlist):&#10;        # compute mean per selected metric&#10;        row = [label]&#10;        for name in self.metric_names:&#10;            vals = [getattr(m, name) for m in mlist if getattr(m, name) is not None]&#10;            mean = float(np.nanmean(vals)) if vals else None&#10;            row.append(mean)&#10;        return row" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/validation/validator.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/validation/validator.py" />
              <option name="originalContent" value="import csv&#10;import os&#10;import re&#10;from collections import defaultdict&#10;from pathlib import Path&#10;from typing import Optional, Tuple&#10;&#10;import matplotlib.pyplot as plt&#10;import numpy as np&#10;from skimage import io&#10;from tqdm import tqdm&#10;&#10;&#10;class Validator:&#10;&#10;    def __init__(self, ground_truth_dir, output_dir, metrics):&#10;        &quot;&quot;&quot;&#10;        Initialize Validator.&#10;        Parameters:&#10;            ground_truth_dir (str): Path to ground truth masks.&#10;            output_dir (str): Directory to save CSV results.&#10;            metrics (list[Metric], optional): List of metric instances to compute. If None, auto-discover.&#10;        &quot;&quot;&quot;&#10;        self.ground_truth_dir = ground_truth_dir&#10;        self.ground_truths = self.load_masks(ground_truth_dir)&#10;        self.output_dir = output_dir&#10;        self.vp_dm_distances = self.load_vp_dm_distances()&#10;        self.metrics = metrics&#10;        self.health_status_dict = self.load_health_status_dict()&#10;&#10;&#10;    def validate(self, predictions_dir):&#10;        # Load ground truth and prediction masks&#10;        ground_truths = self.ground_truths&#10;        predictions = self.load_masks(predictions_dir)&#10;&#10;        # Prepare data structures for computed_metric_results&#10;        metric_scores = []&#10;        metric_scores_by_dataset = defaultdict(list)&#10;        metric_scores_by_health_status = defaultdict(list)&#10;&#10;        # Iterate over all ground truth files and validate predictions&#10;        for file_name in tqdm(ground_truths.keys(), desc=f'Validating predictions for {predictions_dir}'):&#10;            if file_name not in predictions.keys():&#10;                print(f&quot;Ground truth {file_name} does not have a corresponding prediction.&quot;)&#10;                continue&#10;&#10;            # Load ground truth and prediction masks for the current image&#10;            dataset = self.parse_dataset(file_name)&#10;            gt = ground_truths[file_name] &gt; 0  # binary mask&#10;            pred = predictions[file_name] &gt; 0  # binary mask&#10;&#10;            # Compute TP, FP, FN as base metrics&#10;            computed_metric_results = {&#10;                'TP': np.logical_and(gt, pred).sum(),&#10;                'FP': np.logical_and(~gt, pred).sum(),&#10;                'FN': np.logical_and(gt, ~pred).sum()&#10;            }&#10;&#10;            # Compute image metadata&#10;            image_metadata = self.load_image_metadata(file_name)&#10;&#10;            # Compute metrics sequentially, allowing composite metrics to use previous computed_metric_results&#10;            for m in self.metrics:&#10;                computed_metric_results[m.name] = m.compute(gt, pred, computed_metric_results, image_metadata)&#10;            # Collect metric values (keep infinities for per-image output)&#10;            values = [computed_metric_results[m.name] for m in self.metrics]&#10;&#10;            # Determine health status for the patient from the patient index&#10;            pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;            sick = self.health_status_dict.get(pat_idx)&#10;&#10;            # Add metric scores for this file to the overall list&#10;            metric_scores.append([dataset, file_name, sick] + values)&#10;&#10;            # Add metric scores for this file to the dataset-specific list&#10;            metric_scores_by_dataset[dataset].append(values)&#10;&#10;            # Add metric scores for this file to the health status-specific list&#10;            if sick == 1.0:&#10;                metric_scores_by_health_status['Sick'].append(values)&#10;            elif sick == 0.0:&#10;                metric_scores_by_health_status['Healthy'].append(values)&#10;&#10;        # prepare output directories&#10;        all_csv, mean_csv = self.create_output_files(predictions_dir)&#10;        # write all per-file metric_scores&#10;        self.save_per_image_metrics(all_csv, metric_scores)&#10;        # Read duration metric_scores&#10;        avg_image_duration, total_duration = self.read_segmentation_duration(predictions_dir)&#10;        # Write mean metric_scores grouped by dataset, by sick status and overall&#10;        self.save_mean_metrics(avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                               total_duration)&#10;&#10;&#10;&#10;&#10;    def load_image_metadata(self, file_name):&#10;        # Load markers&#10;        vp, dm, dl_diers, dr_diers = self.load_markers(file_name)&#10;&#10;        # Calc pixel ratio in mm&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        pixel_size = self.compute_distance_per_pixel(pat_idx, vp, dm)&#10;&#10;        return {&#10;            'vp': vp,&#10;            'dm': dm,&#10;            'dl_diers': dl_diers,&#10;            'dr_diers': dr_diers,&#10;            'pixel_size_mm': pixel_size&#10;        }&#10;&#10;&#10;    def load_markers(self, file_name, markers_file=&quot;data/Info_sheets/Markerpositionen.csv&quot;):&#10;        # Check if markers are available for this image&#10;        img_number = self.extract_image_number(file_name)&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile, delimiter=';')&#10;            for row in reader:&#10;                if row.get('BildID', '').startswith(str(img_number)):&#10;                    vp = (int(row['X_VP']) / 10, int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM']) / 10, int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # Check if markers are available for this patient&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        with open('data/Info_Sheets/All_Data_Renamed_overview.csv', 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('Patientenindex') == pat_idx:&#10;                    measure_id = row['DIERS_Mess-ID']&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('MessID', '') == measure_id:&#10;                    vp = (int(row['X_VP'] / 10), int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM'] / 10), int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # If no markers found, return None&#10;        return None, None, None, None&#10;&#10;&#10;    def save_mean_metrics(self, avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                          total_duration):&#10;        with open(mean_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header_mean = ['Dataset'] + [m.name for m in self.metrics] + ['Total duration',&#10;                                                                          'Average duration per image']&#10;            writer.writerow(header_mean)&#10;            # per-dataset means&#10;            for ds, lst in metric_scores_by_dataset.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([ds] + means + ['', ''])&#10;            # by sick status&#10;            for label, lst in metric_scores_by_health_status.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([label] + means + ['', ''])&#10;            # overall&#10;            all_vals = [v for lst in metric_scores_by_dataset.values() for v in lst]&#10;            means = [_safe_nanmean([row[i] for row in all_vals]) for i in range(len(self.metrics))]&#10;            writer.writerow(['All Datasets'] + means + [total_duration, avg_image_duration])&#10;        print(f&quot;Mean validation results saved to {mean_csv}&quot;)&#10;&#10;&#10;    @staticmethod&#10;    def read_segmentation_duration(predictions_dir):&#10;        _duration_file = Path(predictions_dir) / 'duration.txt'&#10;        if _duration_file.exists():&#10;            with open(_duration_file, 'r') as _df:&#10;                _lines = _df.readlines()&#10;            if len(_lines) &gt;= 2:&#10;                total_duration = _lines[0].split(':', 1)[1].strip()&#10;                avg_image_duration = _lines[1].split(':', 1)[1].strip()&#10;            else:&#10;                total_duration = ''&#10;                avg_image_duration = ''&#10;        else:&#10;            total_duration = ''&#10;            avg_image_duration = ''&#10;        return avg_image_duration, total_duration&#10;&#10;&#10;    def save_per_image_metrics(self, all_csv, metric_scores):&#10;        with open(all_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header = ['Dataset', 'File Name', 'Sick'] + [m.name for m in self.metrics]&#10;            writer.writerow(header)&#10;            writer.writerows(metric_scores)&#10;        print(f&quot;Validation results saved to {all_csv}&quot;)&#10;&#10;&#10;    def create_output_files(self, predictions_dir):&#10;        output_dir = Path(self.output_dir)&#10;        output_dir.mkdir(parents=True, exist_ok=True)&#10;        run_name = os.path.basename(predictions_dir)&#10;        all_csv = output_dir / f&quot;{run_name}_all.csv&quot;&#10;        mean_csv = output_dir / f&quot;{run_name}_mean.csv&quot;&#10;        return all_csv, mean_csv&#10;&#10;&#10;    @staticmethod&#10;    def parse_dataset(file_name):&#10;        prefix = file_name.split('_')[0]&#10;        return prefix&#10;&#10;&#10;    @staticmethod&#10;    def load_masks(segmentations_dir):&#10;        segmentations = {}&#10;        for file in os.listdir(segmentations_dir):&#10;            if file.endswith(&quot;.png&quot;) and &quot;-mask&quot; in file:&#10;                img = io.imread(os.path.join(segmentations_dir, file))&#10;                # Falls RGB, in Graustufen umwandeln&#10;                if img.ndim == 3:&#10;                    img = img[..., 0]  # Nur ersten Kanal nehmen (oder np.mean(img, axis=2) für echten Grauwert)&#10;                segmentations[file] = img&#10;        return segmentations&#10;&#10;&#10;    @staticmethod&#10;    def parse_patient_index_from_image_path(image_path):&#10;        pattern = re.compile(r'^[^_]+_([^_]+(?:_\d+)+)(?=_\d{9,})')&#10;        match = pattern.search(os.path.basename(image_path))&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def extract_image_number(file_name):&#10;        match = re.search(r'_(\d+)-mask\.Gauss\.png$', file_name)&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def visualize_mask(image: np.ndarray, title: str):&#10;        &quot;&quot;&quot;Display the original mask image without any splitting lines or axes.&quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # display mask or full-color image&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    @staticmethod&#10;    def visualize_middle_line(image: np.ndarray, vp: Optional[Tuple[int, int]], dm: Optional[Tuple[int, int]], title: str):&#10;        &quot;&quot;&quot;Display the mask image with a middle splitting line.&quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # if image has 3 or more channels, show in RGB, else grayscale&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;&#10;        if vp is not None and dm is not None:&#10;            # Draw the splitting line in the middle of the two markers&#10;            xs = [vp[0], dm[0]]&#10;            ys = [vp[1], dm[1]]&#10;            ax.plot(xs, ys, color='magenta', linewidth=2)&#10;&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    def compute_distance_per_pixel(self, patient_idx, vp, dm):&#10;        &quot;&quot;&quot;&#10;        Compute millimeters per pixel using known physical distance between VP and DM markers.&#10;        &quot;&quot;&quot;&#10;        if vp is None or dm is None:&#10;            return None&#10;        pixel_dist = np.hypot(dm[0] - vp[0], dm[1] - vp[1])&#10;        if pixel_dist == 0:&#10;            return None&#10;        mm_dist = self.vp_dm_distances.get(patient_idx)&#10;        if mm_dist is None:&#10;            return None&#10;        return mm_dist / pixel_dist&#10;&#10;&#10;    @staticmethod&#10;    def load_health_status_dict(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        &quot;&quot;&quot;Load the mapping from Patientenindex to Krank value.&quot;&quot;&quot;&#10;        health_status_dict = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                sick = row.get('Krank')&#10;                if pat_idx and sick:&#10;                    health_status_dict[pat_idx] = float(sick)&#10;        return health_status_dict&#10;&#10;&#10;    @staticmethod&#10;    def load_vp_dm_distances(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        vp_dm_distance_map = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                dist_str = row.get('Rumpflänge')&#10;                if pat_idx and dist_str:&#10;                    try:&#10;                        dist_mm = float(dist_str)&#10;                        vp_dm_distance_map[pat_idx] = dist_mm&#10;                    except ValueError:&#10;                        continue&#10;        return vp_dm_distance_map&#10;&#10;&#10;def _safe_nanmean(arr):&#10;    # filter out None, NaN, and infinite values&#10;    clean = [v for v in arr if v is not None and np.isfinite(v)]&#10;    if len(clean) == 0:&#10;        return None&#10;    return float(np.mean(clean))&#10;" />
              <option name="updatedContent" value="import csv&#10;import os&#10;import re&#10;from collections import defaultdict&#10;from pathlib import Path&#10;from typing import Optional, Tuple&#10;&#10;import matplotlib.pyplot as plt&#10;import numpy as np&#10;from skimage import io&#10;from tqdm import tqdm&#10;&#10;&#10;class Validator:&#10;&#10;    def __init__(self, ground_truth_dir, output_dir, metrics):&#10;        &quot;&quot;&quot;&#10;        Initialize Validator.&#10;        Parameters:&#10;            ground_truth_dir (str): Path to ground truth masks.&#10;            output_dir (str): Directory to save CSV results.&#10;            metrics (list[Metric], optional): List of metric instances to compute. If None, auto-discover.&#10;        &quot;&quot;&quot;&#10;        self.ground_truth_dir = ground_truth_dir&#10;        self.ground_truths = self.load_masks(ground_truth_dir)&#10;        self.output_dir = output_dir&#10;        self.vp_dm_distances = self.load_vp_dm_distances()&#10;        self.metrics = metrics&#10;        self.health_status_dict = self.load_health_status_dict()&#10;&#10;&#10;    def validate(self, predictions_dir):&#10;        # Load ground truth and prediction masks&#10;        ground_truths = self.ground_truths&#10;        predictions = self.load_masks(predictions_dir)&#10;&#10;        # Prepare data structures for computed_metric_results&#10;        metric_scores = []&#10;        metric_scores_by_dataset = defaultdict(list)&#10;        metric_scores_by_health_status = defaultdict(list)&#10;&#10;        # Iterate over all ground truth files and validate predictions&#10;        for file_name in tqdm(ground_truths.keys(), desc=f'Validating predictions for {predictions_dir}'):&#10;            if file_name not in predictions.keys():&#10;                print(f&quot;Ground truth {file_name} does not have a corresponding prediction.&quot;)&#10;                continue&#10;&#10;            # Load ground truth and prediction masks for the current image&#10;            dataset = self.parse_dataset(file_name)&#10;            gt = ground_truths[file_name] &gt; 0  # binary mask&#10;            pred = predictions[file_name] &gt; 0  # binary mask&#10;&#10;            # Compute TP, FP, FN as base metrics&#10;            computed_metric_results = {&#10;                'TP': np.logical_and(gt, pred).sum(),&#10;                'FP': np.logical_and(~gt, pred).sum(),&#10;                'FN': np.logical_and(gt, ~pred).sum()&#10;            }&#10;&#10;            # Compute image metadata&#10;            image_metadata = self.load_image_metadata(file_name)&#10;&#10;            # Compute metrics sequentially, allowing composite metrics to use previous computed_metric_results&#10;            for m in self.metrics:&#10;                computed_metric_results[m.name] = m.compute(gt, pred, computed_metric_results, image_metadata)&#10;            # Collect metric values (keep infinities for per-image output)&#10;            values = [computed_metric_results[m.name] for m in self.metrics]&#10;&#10;            # Determine health status for the patient from the patient index&#10;            pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;            sick = self.health_status_dict.get(pat_idx)&#10;&#10;            # Add metric scores for this file to the overall list&#10;            metric_scores.append([dataset, file_name, sick] + values)&#10;&#10;            # Add metric scores for this file to the dataset-specific list&#10;            metric_scores_by_dataset[dataset].append(values)&#10;&#10;            # Add metric scores for this file to the health status-specific list&#10;            if sick == 1.0:&#10;                metric_scores_by_health_status['Sick'].append(values)&#10;            elif sick == 0.0:&#10;                metric_scores_by_health_status['Healthy'].append(values)&#10;&#10;        # prepare output directories&#10;        all_csv, mean_csv = self.create_output_files(predictions_dir)&#10;        # write all per-file metric_scores&#10;        self.save_per_image_metrics(all_csv, metric_scores)&#10;        # Read duration metric_scores&#10;        avg_image_duration, total_duration = self.read_segmentation_duration(predictions_dir)&#10;        # Write mean metric_scores grouped by dataset, by sick status and overall&#10;        self.save_mean_metrics(avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                               total_duration)&#10;&#10;&#10;&#10;&#10;    def load_image_metadata(self, file_name):&#10;        # Load markers&#10;        vp, dm, dl_diers, dr_diers = self.load_markers(file_name)&#10;&#10;        # Calc pixel ratio in mm&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        pixel_size = self.compute_distance_per_pixel(pat_idx, vp, dm)&#10;&#10;        return {&#10;            'vp': vp,&#10;            'dm': dm,&#10;            'dl_diers': dl_diers,&#10;            'dr_diers': dr_diers,&#10;            'pixel_size_mm': pixel_size&#10;        }&#10;&#10;&#10;    def load_markers(self, file_name, markers_file=&quot;data/Info_sheets/Markerpositionen.csv&quot;):&#10;        # Check if markers are available for this image&#10;        img_number = self.extract_image_number(file_name)&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile, delimiter=';')&#10;            for row in reader:&#10;                if row.get('BildID', '').startswith(str(img_number)):&#10;                    vp = (int(row['X_VP']) / 10, int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM']) / 10, int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # Check if markers are available for this patient&#10;        pat_idx = self.parse_patient_index_from_image_path(file_name)&#10;        with open('data/Info_Sheets/All_Data_Renamed_overview.csv', 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('Patientenindex') == pat_idx:&#10;                    measure_id = row['DIERS_Mess-ID']&#10;        with open(markers_file, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                if row.get('MessID', '') == measure_id:&#10;                    vp = (int(row['X_VP'] / 10), int(row['Y_VP']) / 10)&#10;                    dm = (int(row['X_DM'] / 10), int(row['Y_DM']) / 10)&#10;                    dl_diers = (int(row['X_DL']) / 10, int(row['Y_DL']) / 10)&#10;                    dr_diers = (int(row['X_DR']) / 10, int(row['Y_DR']) / 10)&#10;                    return vp, dm, dl_diers, dr_diers&#10;&#10;        # If no markers found, return None&#10;        return None, None, None, None&#10;&#10;&#10;    def save_mean_metrics(self, avg_image_duration, mean_csv, metric_scores_by_dataset, metric_scores_by_health_status,&#10;                          total_duration):&#10;        with open(mean_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header_mean = ['Dataset'] + [m.name for m in self.metrics] + ['Total duration',&#10;                                                                          'Average duration per image']&#10;            writer.writerow(header_mean)&#10;            # per-dataset means&#10;            for ds, lst in metric_scores_by_dataset.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([ds] + means + ['', ''])&#10;            # by sick status&#10;            for label, lst in metric_scores_by_health_status.items():&#10;                means = [_safe_nanmean([row[i] for row in lst]) for i in range(len(self.metrics))]&#10;                writer.writerow([label] + means + ['', ''])&#10;            # overall&#10;            all_vals = [v for lst in metric_scores_by_dataset.values() for v in lst]&#10;            means = [_safe_nanmean([row[i] for row in all_vals]) for i in range(len(self.metrics))]&#10;            writer.writerow(['All Datasets'] + means + [total_duration, avg_image_duration])&#10;        print(f&quot;Mean validation results saved to {mean_csv}&quot;)&#10;&#10;&#10;    @staticmethod&#10;    def read_segmentation_duration(predictions_dir):&#10;        _duration_file = Path(predictions_dir) / 'duration.txt'&#10;        if _duration_file.exists():&#10;            with open(_duration_file, 'r') as _df:&#10;                _lines = _df.readlines()&#10;            if len(_lines) &gt;= 2:&#10;                total_duration = _lines[0].split(':', 1)[1].strip()&#10;                avg_image_duration = _lines[1].split(':', 1)[1].strip()&#10;            else:&#10;                total_duration = ''&#10;                avg_image_duration = ''&#10;        else:&#10;            total_duration = ''&#10;            avg_image_duration = ''&#10;        return avg_image_duration, total_duration&#10;&#10;&#10;    def save_per_image_metrics(self, all_csv, metric_scores):&#10;        with open(all_csv, 'w', newline='') as csvfile:&#10;            writer = csv.writer(csvfile)&#10;            header = ['Dataset', 'File Name', 'Sick'] + [m.name for m in self.metrics]&#10;            writer.writerow(header)&#10;            writer.writerows(metric_scores)&#10;        print(f&quot;Validation results saved to {all_csv}&quot;)&#10;&#10;&#10;    def create_output_files(self, predictions_dir):&#10;        output_dir = Path(self.output_dir)&#10;        output_dir.mkdir(parents=True, exist_ok=True)&#10;        run_name = os.path.basename(predictions_dir)&#10;        all_csv = output_dir / f&quot;{run_name}_all.csv&quot;&#10;        mean_csv = output_dir / f&quot;{run_name}_mean.csv&quot;&#10;        return all_csv, mean_csv&#10;&#10;&#10;    @staticmethod&#10;    def parse_dataset(file_name):&#10;        prefix = file_name.split('_')[0]&#10;        return prefix&#10;&#10;&#10;    @staticmethod&#10;    def load_masks(segmentations_dir):&#10;        segmentations = {}&#10;        for file in os.listdir(segmentations_dir):&#10;            if file.endswith(&quot;.png&quot;) and &quot;-mask&quot; in file:&#10;                img = io.imread(os.path.join(segmentations_dir, file))&#10;                # Falls RGB, in Graustufen umwandeln&#10;                if img.ndim == 3:&#10;                    img = img[..., 0]  # Nur ersten Kanal nehmen (oder np.mean(img, axis=2) für echten Grauwert)&#10;                segmentations[file] = img&#10;        return segmentations&#10;&#10;&#10;    @staticmethod&#10;    def parse_patient_index_from_image_path(image_path):&#10;        pattern = re.compile(r'^[^_]+_([^_]+(?:_\d+)+)(?=_\d{9,})')&#10;        match = pattern.search(os.path.basename(image_path))&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def extract_image_number(file_name):&#10;        match = re.search(r'_(\d+)-mask\.Gauss\.png$', file_name)&#10;        if match:&#10;            return match.group(1)&#10;        return None&#10;&#10;&#10;    @staticmethod&#10;    def visualize_mask(image: np.ndarray, title: str):&#10;        &quot;&quot;&quot;Display the original mask image without any splitting lines or axes.&quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # display mask or full-color image&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    @staticmethod&#10;    def visualize_middle_line(image: np.ndarray, vp: Optional[Tuple[int, int]], dm: Optional[Tuple[int, int]], title: str):&#10;        &quot;&quot;&quot;Display the mask image with a middle splitting line.&quot;&quot;&quot;&#10;        fig, ax = plt.subplots()&#10;        # if image has 3 or more channels, show in RGB, else grayscale&#10;        if image.ndim == 3 and image.shape[2] &gt;= 3:&#10;            ax.imshow(image, aspect='equal', interpolation='nearest')&#10;        else:&#10;            vis = image[...,0] if image.ndim == 3 else image&#10;            ax.imshow(vis, cmap='gray', aspect='equal', interpolation='nearest')&#10;&#10;        if vp is not None and dm is not None:&#10;            # Draw the splitting line in the middle of the two markers&#10;            xs = [vp[0], dm[0]]&#10;            ys = [vp[1], dm[1]]&#10;            ax.plot(xs, ys, color='magenta', linewidth=2)&#10;&#10;        ax.axis('off')&#10;        ax.set_title(title)&#10;        plt.show()&#10;&#10;&#10;    def compute_distance_per_pixel(self, patient_idx, vp, dm):&#10;        &quot;&quot;&quot;&#10;        Compute millimeters per pixel using known physical distance between VP and DM markers.&#10;        &quot;&quot;&quot;&#10;        if vp is None or dm is None:&#10;            return None&#10;        pixel_dist = np.hypot(dm[0] - vp[0], dm[1] - vp[1])&#10;        if pixel_dist == 0:&#10;            return None&#10;        mm_dist = self.vp_dm_distances.get(patient_idx)&#10;        if mm_dist is None:&#10;            return None&#10;        return mm_dist / pixel_dist&#10;&#10;&#10;    @staticmethod&#10;    def load_health_status_dict(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        &quot;&quot;&quot;Load the mapping from Patientenindex to Krank value.&quot;&quot;&quot;&#10;        health_status_dict = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                sick = row.get('Krank')&#10;                if pat_idx and sick:&#10;                    health_status_dict[pat_idx] = float(sick)&#10;        return health_status_dict&#10;&#10;&#10;    @staticmethod&#10;    def load_vp_dm_distances(file_path=&quot;data/Info_Sheets/All_Data_Renamed_overview.csv&quot;):&#10;        vp_dm_distance_map = {}&#10;        with open(file_path, 'r', newline='') as csvfile:&#10;            reader = csv.DictReader(csvfile)&#10;            for row in reader:&#10;                pat_idx = row.get('Patientenindex')&#10;                dist_str = row.get('Rumpflänge')&#10;                if pat_idx and dist_str:&#10;                    try:&#10;                        dist_mm = float(dist_str)&#10;                        vp_dm_distance_map[pat_idx] = dist_mm&#10;                    except ValueError:&#10;                        continue&#10;        return vp_dm_distance_map&#10;&#10;&#10;def _safe_nanmean(arr):&#10;    # filter out None, NaN, and infinite values&#10;    clean = [v for v in arr if v is not None and np.isfinite(v)]&#10;    if len(clean) == 0:&#10;        return None&#10;    return float(np.mean(clean))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/visualization/avg_dice_by_nsegments_plotter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/visualization/avg_dice_by_nsegments_plotter.py" />
              <option name="originalContent" value="import warnings&#10;import numpy as np&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.base_plotter import BasePlotter&#10;&#10;&#10;class AvgDiceByNSegmentsPlotter(BasePlotter):&#10;    &quot;&quot;&quot;&#10;    Plot average of one metric (e.g., Dice) as a function of another (e.g., N Segments GT) per experiment.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, experiments, metrics, directory='avg_dice_by_nsegments'):  # metrics: [x_metric, y_metric]&#10;        super().__init__(experiments, metrics, directory)&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        if len(self.metrics) != 2:&#10;            warnings.warn('AvgDiceByNSegmentsPlotter needs exactly 2 metrics: [x_metric, y_metric].')&#10;            return&#10;        metric_x, metric_y = self.metrics&#10;        if not data_frames:&#10;            warnings.warn('No data to plot.')&#10;            return&#10;        # collect all unique x values across experiments&#10;        x_vals_all = set()&#10;        for df in data_frames:&#10;            if metric_x in df.columns:&#10;                x_vals_all.update(df[metric_x].dropna().unique())&#10;        if not x_vals_all:&#10;            warnings.warn(f&quot;Metric '{metric_x}' not found in any data.&quot;)&#10;            return&#10;        sorted_x = sorted(x_vals_all)&#10;        # prepare plot&#10;        fig, ax = plt.subplots(figsize=(max(3, int((len(sorted_x)+1)*0.3)), 6))&#10;        # plot per experiment&#10;        for df, name in zip(data_frames, self.experiments):&#10;            if metric_x not in df.columns or metric_y not in df.columns:&#10;                warnings.warn(f&quot;Skipping experiment '{name}': missing '{metric_x}' or '{metric_y}'.&quot;)&#10;                continue&#10;            grouped = df.groupby(metric_x)[metric_y].mean()&#10;            y_vals = [grouped.get(x, np.nan) for x in sorted_x]&#10;            ax.plot(sorted_x, y_vals, marker='o', label=name)&#10;        ax.set_title(f&quot;Average {metric_y} by {metric_x} per experiment&quot;)&#10;        ax.set_xlabel(metric_x)&#10;        ax.set_ylabel(f&quot;Average {metric_y}&quot;)&#10;        ax.set_xticks(sorted_x)&#10;        # place legend below plot with multiple columns like scatter plotter&#10;        ncol = min(len(self.experiments), 3)&#10;        ax.legend(ncol=ncol, loc='upper center', bbox_to_anchor=(0.5, -0.15), fontsize='small', frameon=False)&#10;        ax.xaxis.grid(True, which='major', linestyle='--', alpha=0.5)&#10;        ax.yaxis.grid(True, which='major', linestyle='--', alpha=0.5)&#10;        fig.tight_layout()&#10;        fig.subplots_adjust(bottom=0.3)&#10;        # filename&#10;        fn = f&quot;avg_{metric_y.replace(' ', '_')}_by_{metric_x.replace(' ', '_')}.png&quot;&#10;        self.save_plot(fig, output_dir, filename=fn)&#10;" />
              <option name="updatedContent" value="import warnings&#10;import numpy as np&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.base_plotter import BasePlotter&#10;&#10;&#10;class AvgDiceByNSegmentsPlotter(BasePlotter):&#10;    &quot;&quot;&quot;&#10;    Plot average of one metric (e.g., Dice) as a function of another (e.g., N Segments GT) per experiment.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, experiments, metrics, directory='avg_dice_by_nsegments'):  # metrics: [x_metric, y_metric]&#10;        super().__init__(experiments, metrics, directory)&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        if len(self.metrics) != 2:&#10;            warnings.warn('AvgDiceByNSegmentsPlotter needs exactly 2 metrics: [x_metric, y_metric].')&#10;            return&#10;        metric_x, metric_y = self.metrics&#10;        if not data_frames:&#10;            warnings.warn('No data to plot.')&#10;            return&#10;        # collect all unique x values across experiments&#10;        x_vals_all = set()&#10;        for df in data_frames:&#10;            if metric_x in df.columns:&#10;                x_vals_all.update(df[metric_x].dropna().unique())&#10;        if not x_vals_all:&#10;            warnings.warn(f&quot;Metric '{metric_x}' not found in any data.&quot;)&#10;            return&#10;        sorted_x = sorted(x_vals_all)&#10;        # prepare plot&#10;        fig, ax = plt.subplots(figsize=(max(3, int((len(sorted_x)+1)*0.3)), 6))&#10;        # plot per experiment&#10;        for df, name in zip(data_frames, self.experiments):&#10;            if metric_x not in df.columns or metric_y not in df.columns:&#10;                warnings.warn(f&quot;Skipping experiment '{name}': missing '{metric_x}' or '{metric_y}'.&quot;)&#10;                continue&#10;            grouped = df.groupby(metric_x)[metric_y].mean()&#10;            y_vals = [grouped.get(x, np.nan) for x in sorted_x]&#10;            ax.plot(sorted_x, y_vals, marker='o', label=name)&#10;        ax.set_title(f&quot;Average {metric_y} by {metric_x} per experiment&quot;)&#10;        ax.set_xlabel(metric_x)&#10;        ax.set_ylabel(f&quot;Average {metric_y}&quot;)&#10;        ax.set_xticks(sorted_x)&#10;        # place legend below plot with multiple columns like scatter plotter&#10;        ncol = min(len(self.experiments), 3)&#10;        ax.legend(ncol=ncol, loc='upper center', bbox_to_anchor=(0.5, -0.15), fontsize='small', frameon=False)&#10;        ax.xaxis.grid(True, which='major', linestyle='--', alpha=0.5)&#10;        ax.yaxis.grid(True, which='major', linestyle='--', alpha=0.5)&#10;        fig.tight_layout()&#10;        fig.subplots_adjust(bottom=0.3)&#10;        # filename&#10;        fn = f&quot;avg_{metric_y.replace(' ', '_')}_by_{metric_x.replace(' ', '_')}.png&quot;&#10;        self.save_plot(fig, output_dir, filename=fn)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/visualization/bar_plotter.csv.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/visualization/bar_plotter.csv.py" />
              <option name="originalContent" value="import os&#10;&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.plotter import Plotter&#10;&#10;&#10;class BarPlotter(Plotter):&#10;    def __init__(self, columns):&#10;        self.columns = columns&#10;&#10;    def plot(self, dfs, exp_names, output_dir):&#10;        for col in self.columns:&#10;            values = []&#10;            for df in dfs:&#10;                if col in df.columns:&#10;                    # Use explicit value from the 'All Datasets' row instead of column mean&#10;                    if 'All Datasets' in df.index:&#10;                        values.append(df.loc['All Datasets', col])&#10;                    else:&#10;                        raise ValueError(f&quot;Row 'All Datasets' not found in DataFrame for experiment.&quot;)&#10;                else:&#10;                    raise ValueError(f&quot;Column '{col}' not found in DataFrame for experiment.&quot;)&#10;            plt.figure()&#10;            plt.bar(exp_names, values)&#10;            plt.title(f&quot;{col} across experiments&quot;)&#10;            plt.ylabel(col)&#10;            plt.xticks(rotation=45, ha='right')&#10;            plt.tight_layout()&#10;            out_path = os.path.join(output_dir, f&quot;{col}_bar_chart.png&quot;)&#10;            plt.savefig(out_path)&#10;            plt.close()&#10;            print(f&quot;Saved bar chart for '{col}' to {out_path}&quot;)" />
              <option name="updatedContent" value="import os&#10;&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.plotter import Plotter&#10;&#10;&#10;class BarPlotter(Plotter):&#10;    def __init__(self, columns):&#10;        self.columns = columns&#10;&#10;    def plot(self, dfs, exp_names, output_dir):&#10;        for col in self.columns:&#10;            values = []&#10;            for df in dfs:&#10;                if col in df.columns:&#10;                    # Use explicit value from the 'All Datasets' row instead of column mean&#10;                    if 'All Datasets' in df.index:&#10;                        values.append(df.loc['All Datasets', col])&#10;                    else:&#10;                        raise ValueError(f&quot;Row 'All Datasets' not found in DataFrame for experiment.&quot;)&#10;                else:&#10;                    raise ValueError(f&quot;Column '{col}' not found in DataFrame for experiment.&quot;)&#10;            plt.figure()&#10;            plt.bar(exp_names, values)&#10;            plt.title(f&quot;{col} across experiments&quot;)&#10;            plt.ylabel(col)&#10;            plt.xticks(rotation=45, ha='right')&#10;            plt.tight_layout()&#10;            out_path = os.path.join(output_dir, f&quot;{col}_bar_chart.png&quot;)&#10;            plt.savefig(out_path)&#10;            plt.close()&#10;            print(f&quot;Saved bar chart for '{col}' to {out_path}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/visualization/base_plotter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/visualization/base_plotter.py" />
              <option name="originalContent" value="import os&#10;&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.iplotter import IPlotter&#10;&#10;&#10;class BasePlotter(IPlotter):&#10;    def __init__(self, experiments, metrics, directory):&#10;        self.experiments = experiments&#10;        self.metrics = metrics&#10;        self.directory = directory&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        raise NotImplementedError(&quot;Subclasses must implement the plot method.&quot;)&#10;&#10;    def save_plot(self, fig, output_dir, filename=None):&#10;        &quot;&quot;&quot;Save figure in the configured directory, using optional filename.&quot;&quot;&quot;&#10;        dir_path = os.path.join(output_dir, self.directory)&#10;        os.makedirs(dir_path, exist_ok=True)&#10;        if filename is None:&#10;            filename = '_'.join([m.replace(' ', '_') for m in self.metrics]) + '.png'&#10;        # remove any path separators from filename to prevent unintended directories&#10;        filename = filename.replace('/', '_').replace('\\', '_')&#10;        out_path = os.path.join(str(dir_path), filename)&#10;        plt.savefig(out_path, bbox_inches='tight')&#10;        plt.close(fig)&#10;        print(f&quot;Saved chart to {out_path}&quot;)&#10;" />
              <option name="updatedContent" value="import os&#10;&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.iplotter import IPlotter&#10;&#10;&#10;class BasePlotter(IPlotter):&#10;    def __init__(self, experiments, metrics, directory):&#10;        self.experiments = experiments&#10;        self.metrics = metrics&#10;        self.directory = directory&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        raise NotImplementedError(&quot;Subclasses must implement the plot method.&quot;)&#10;&#10;    def save_plot(self, fig, output_dir, filename=None):&#10;        &quot;&quot;&quot;Save figure in the configured directory, using optional filename.&quot;&quot;&quot;&#10;        dir_path = os.path.join(output_dir, self.directory)&#10;        os.makedirs(dir_path, exist_ok=True)&#10;        if filename is None:&#10;            filename = '_'.join([m.replace(' ', '_') for m in self.metrics]) + '.png'&#10;        # remove any path separators from filename to prevent unintended directories&#10;        filename = filename.replace('/', '_').replace('\\', '_')&#10;        out_path = os.path.join(str(dir_path), filename)&#10;        plt.savefig(out_path, bbox_inches='tight')&#10;        plt.close(fig)&#10;        print(f&quot;Saved chart to {out_path}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/visualization/line_plotter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/visualization/line_plotter.py" />
              <option name="originalContent" value="import warnings&#10;&#10;import numpy as np&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.base_plotter import BasePlotter&#10;&#10;&#10;class LinePlotter(BasePlotter):&#10;    def __init__(self, experiments, metrics, directory='line_plots'):&#10;        super().__init__(experiments, metrics, directory)&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        # number of experiments and metrics&#10;        n_exp = len(self.experiments)&#10;        n_met = len(self.metrics)&#10;        if n_exp == 0 or n_met == 0:&#10;            warnings.warn('No experiments or metrics to plot.')&#10;            return&#10;&#10;       # numeric x positions for experiments&#10;        x = np.arange(len(self.experiments))&#10;&#10;        # derive group labels from first DataFrame&#10;        df0 = data_frames[0]&#10;        all_labels = list(df0['Dataset'].unique())&#10;&#10;        # include All Datasets and dataset names, exclude only status labels&#10;        dataset_groups = [d for d in all_labels if d not in ('Healthy', 'Sick')]&#10;&#10;        # include All Datasets, Healthy, Sick if present&#10;        status_groups = [g for g in ('All Datasets', 'Healthy', 'Sick') if g in all_labels]&#10;&#10;        # iterate metrics&#10;        for metric in self.metrics:&#10;            # prepare and save by-dataset plot&#10;            fig = self.create_by_dataset_plots(data_frames, dataset_groups, metric, n_exp, x)&#10;            filename = f&quot;{metric.replace(' ','_')}_by_dataset_line.png&quot;&#10;            self.save_plot(fig, output_dir, filename=filename)&#10;&#10;            # prepare and save by-health-status plot&#10;            fig = self.create_bay_health_status_plots(data_frames, metric, n_exp, status_groups, x)&#10;            filename = f&quot;{metric.replace(' ','_')}_by_status_line.png&quot;&#10;            self.save_plot(fig, output_dir, filename=filename)&#10;&#10;    def create_by_dataset_plots(self, data_frames, dataset_groups, metric, n_exp, x):&#10;        fig, ax = plt.subplots(figsize=(max(3, int((n_exp + 1) * 0.3)), 6))&#10;        for grp in dataset_groups:&#10;            y = []&#10;            for df in data_frames:&#10;                if metric in df.columns and grp in df['Dataset'].values:&#10;                    val = df.loc[df['Dataset'] == grp, metric].iloc[0]&#10;                else:&#10;                    val = np.nan&#10;                y.append(val)&#10;            ax.plot(x, y, marker='o', label=grp, linestyle='dashed')&#10;        ax.set_title(f&quot;{metric} by dataset&quot;)&#10;        ax.set_ylabel(metric)&#10;        ax.set_xticks(x)&#10;        ax.set_xticklabels(self.experiments, rotation=45, ha='right')&#10;        ax.legend()&#10;        # draw horizontal grid lines at each y-tick for better readability&#10;        ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        # if ratio metric, add horizontal grid lines every 0.1&#10;        if metric.lower() in ['dice', 'precision', 'recall']:&#10;            ax.set_ylim(0, 1)&#10;            ax.set_yticks(np.arange(0, 1.0001, 0.1))&#10;            ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        plt.tight_layout()&#10;        return fig&#10;&#10;    def create_bay_health_status_plots(self, data_frames, metric, n_exp, status_groups, x):&#10;        fig, ax = plt.subplots(figsize=(max(3, int((n_exp + 1) * 0.3)), 6))&#10;        for grp in status_groups:&#10;            y = []&#10;            for df in data_frames:&#10;                if metric in df.columns and grp in df['Dataset'].values:&#10;                    val = df.loc[df['Dataset'] == grp, metric].iloc[0]&#10;                else:&#10;                    val = np.nan&#10;                y.append(val)&#10;            ax.plot(x, y, marker='o', label=grp, linestyle='dashed')&#10;        ax.set_title(f&quot;{metric} by health status&quot;)&#10;        ax.set_ylabel(metric)&#10;        ax.set_xticks(x)&#10;        ax.set_xticklabels(self.experiments, rotation=45, ha='right')&#10;        ax.legend()&#10;        # draw horizontal grid lines at each y-tick for better readability&#10;        ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        # if ratio metric, add horizontal grid lines every 0.1&#10;        if metric.lower() in ['dice', 'precision', 'recall']:&#10;            ax.set_ylim(0, 1)&#10;            ax.set_yticks(np.arange(0, 1.0001, 0.1))&#10;            ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        plt.tight_layout()&#10;        return fig&#10;" />
              <option name="updatedContent" value="import warnings&#10;&#10;import numpy as np&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.base_plotter import BasePlotter&#10;&#10;&#10;class LinePlotter(BasePlotter):&#10;    def __init__(self, experiments, metrics, directory='line_plots'):&#10;        super().__init__(experiments, metrics, directory)&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        # number of experiments and metrics&#10;        n_exp = len(self.experiments)&#10;        n_met = len(self.metrics)&#10;        if n_exp == 0 or n_met == 0:&#10;            warnings.warn('No experiments or metrics to plot.')&#10;            return&#10;&#10;       # numeric x positions for experiments&#10;        x = np.arange(len(self.experiments))&#10;&#10;        # derive group labels from first DataFrame&#10;        df0 = data_frames[0]&#10;        all_labels = list(df0['Dataset'].unique())&#10;&#10;        # include All Datasets and dataset names, exclude only status labels&#10;        dataset_groups = [d for d in all_labels if d not in ('Healthy', 'Sick')]&#10;&#10;        # include All Datasets, Healthy, Sick if present&#10;        status_groups = [g for g in ('All Datasets', 'Healthy', 'Sick') if g in all_labels]&#10;&#10;        # iterate metrics&#10;        for metric in self.metrics:&#10;            # prepare and save by-dataset plot&#10;            fig = self.create_by_dataset_plots(data_frames, dataset_groups, metric, n_exp, x)&#10;            filename = f&quot;{metric.replace(' ','_')}_by_dataset_line.png&quot;&#10;            self.save_plot(fig, output_dir, filename=filename)&#10;&#10;            # prepare and save by-health-status plot&#10;            fig = self.create_bay_health_status_plots(data_frames, metric, n_exp, status_groups, x)&#10;            filename = f&quot;{metric.replace(' ','_')}_by_status_line.png&quot;&#10;            self.save_plot(fig, output_dir, filename=filename)&#10;&#10;    def create_by_dataset_plots(self, data_frames, dataset_groups, metric, n_exp, x):&#10;        fig, ax = plt.subplots(figsize=(max(3, int((n_exp + 1) * 0.3)), 6))&#10;        for grp in dataset_groups:&#10;            y = []&#10;            for df in data_frames:&#10;                if metric in df.columns and grp in df['Dataset'].values:&#10;                    val = df.loc[df['Dataset'] == grp, metric].iloc[0]&#10;                else:&#10;                    val = np.nan&#10;                y.append(val)&#10;            ax.plot(x, y, marker='o', label=grp, linestyle='dashed')&#10;        ax.set_title(f&quot;{metric} by dataset&quot;)&#10;        ax.set_ylabel(metric)&#10;        ax.set_xticks(x)&#10;        ax.set_xticklabels(self.experiments, rotation=45, ha='right')&#10;        ax.legend()&#10;        # draw horizontal grid lines at each y-tick for better readability&#10;        ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        # if ratio metric, add horizontal grid lines every 0.1&#10;        if metric.lower() in ['dice', 'precision', 'recall']:&#10;            ax.set_ylim(0, 1)&#10;            ax.set_yticks(np.arange(0, 1.0001, 0.1))&#10;            ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        plt.tight_layout()&#10;        return fig&#10;&#10;    def create_bay_health_status_plots(self, data_frames, metric, n_exp, status_groups, x):&#10;        fig, ax = plt.subplots(figsize=(max(3, int((n_exp + 1) * 0.3)), 6))&#10;        for grp in status_groups:&#10;            y = []&#10;            for df in data_frames:&#10;                if metric in df.columns and grp in df['Dataset'].values:&#10;                    val = df.loc[df['Dataset'] == grp, metric].iloc[0]&#10;                else:&#10;                    val = np.nan&#10;                y.append(val)&#10;            ax.plot(x, y, marker='o', label=grp, linestyle='dashed')&#10;        ax.set_title(f&quot;{metric} by health status&quot;)&#10;        ax.set_ylabel(metric)&#10;        ax.set_xticks(x)&#10;        ax.set_xticklabels(self.experiments, rotation=45, ha='right')&#10;        ax.legend()&#10;        # draw horizontal grid lines at each y-tick for better readability&#10;        ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        # if ratio metric, add horizontal grid lines every 0.1&#10;        if metric.lower() in ['dice', 'precision', 'recall']:&#10;            ax.set_ylim(0, 1)&#10;            ax.set_yticks(np.arange(0, 1.0001, 0.1))&#10;            ax.yaxis.grid(True, which='major', color='lightgrey', linestyle='-', linewidth=0.5)&#10;        plt.tight_layout()&#10;        return fig" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/visualization/metric_count_line_plotter.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/visualization/metric_count_line_plotter.py" />
              <option name="originalContent" value="import warnings&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.base_plotter import BasePlotter&#10;&#10;&#10;class MetricCountLinePlotter(BasePlotter):&#10;    def __init__(self, experiments, metrics, directory='metric_count_line_plots'):&#10;        super().__init__(experiments, metrics, directory)&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        # expect a single experiment's full data ('_all.csv')&#10;        if not data_frames:&#10;            warnings.warn('No data to plot.')&#10;            return&#10;        df = data_frames[0]&#10;        # determine health status based on 'Sick' flag column&#10;        if 'Sick' not in df.columns:&#10;            warnings.warn(&quot;Column 'Sick' not found. Cannot group by health status.&quot;)&#10;            return&#10;        healthy_df = df[df['Sick'] == 0]&#10;        sick_df = df[df['Sick'] == 1]&#10;        status_data = [('Healthy', healthy_df), ('Sick', sick_df)]&#10;        for metric in self.metrics:&#10;            if metric not in df.columns:&#10;                warnings.warn(f&quot;Column '{metric}' not found. Skipping MetricCountLinePlotter for this column.&quot;)&#10;                continue&#10;            # determine sorted unique metric values&#10;            all_vals = df[metric].dropna().unique()&#10;            sorted_vals = sorted(all_vals)&#10;            # prepare figure&#10;            width = max(3, int(len(sorted_vals) * 0.3))&#10;            fig, ax = plt.subplots(figsize=(width, 6))&#10;            # plot counts per status&#10;            for status, sub_df in status_data:&#10;                vals_status = sub_df[metric].dropna()&#10;                counts = vals_status.value_counts().reindex(sorted_vals, fill_value=0)&#10;                ax.plot(sorted_vals, counts.values, marker='o', label=status)&#10;            ax.set_title(f&quot;Count of images by {metric} and health status&quot;)&#10;            ax.set_xlabel(metric)&#10;            ax.set_ylabel('Count of images')&#10;            ax.legend()&#10;            plt.xticks(rotation=45, ha='right')&#10;            plt.tight_layout()&#10;            filename = f&quot;{metric.replace(' ', '_')}_count_line.png&quot;&#10;            self.save_plot(fig, output_dir, filename=filename)&#10;" />
              <option name="updatedContent" value="import warnings&#10;from matplotlib import pyplot as plt&#10;&#10;from visualization.base_plotter import BasePlotter&#10;&#10;&#10;class MetricCountLinePlotter(BasePlotter):&#10;    def __init__(self, experiments, metrics, directory='metric_count_line_plots'):&#10;        super().__init__(experiments, metrics, directory)&#10;&#10;    def plot(self, data_frames, output_dir):&#10;        # expect a single experiment's full data ('_all.csv')&#10;        if not data_frames:&#10;            warnings.warn('No data to plot.')&#10;            return&#10;        df = data_frames[0]&#10;        # determine health status based on 'Sick' flag column&#10;        if 'Sick' not in df.columns:&#10;            warnings.warn(&quot;Column 'Sick' not found. Cannot group by health status.&quot;)&#10;            return&#10;        healthy_df = df[df['Sick'] == 0]&#10;        sick_df = df[df['Sick'] == 1]&#10;        status_data = [('Healthy', healthy_df), ('Sick', sick_df)]&#10;        for metric in self.metrics:&#10;            if metric not in df.columns:&#10;                warnings.warn(f&quot;Column '{metric}' not found. Skipping MetricCountLinePlotter for this column.&quot;)&#10;                continue&#10;            # determine sorted unique metric values&#10;            all_vals = df[metric].dropna().unique()&#10;            sorted_vals = sorted(all_vals)&#10;            # prepare figure&#10;            width = max(3, int(len(sorted_vals) * 0.3))&#10;            fig, ax = plt.subplots(figsize=(width, 6))&#10;            # plot counts per status&#10;            for status, sub_df in status_data:&#10;                vals_status = sub_df[metric].dropna()&#10;                counts = vals_status.value_counts().reindex(sorted_vals, fill_value=0)&#10;                ax.plot(sorted_vals, counts.values, marker='o', label=status)&#10;            ax.set_title(f&quot;Count of images by {metric} and health status&quot;)&#10;            ax.set_xlabel(metric)&#10;            ax.set_ylabel('Count of images')&#10;            ax.legend()&#10;            plt.xticks(rotation=45, ha='right')&#10;            plt.tight_layout()&#10;            filename = f&quot;{metric.replace(' ', '_')}_count_line.png&quot;&#10;            self.save_plot(fig, output_dir, filename=filename)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/visualization/visualizer.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/visualization/visualizer.py" />
              <option name="originalContent" value="import os&#10;&#10;from pandas import read_csv&#10;&#10;from visualization.box_plotter import BoxPlotter&#10;from visualization.heatmap_plotter import HeatmapPlotter&#10;from visualization.scatter_plotter import ScatterPlotter&#10;from visualization.metric_count_bar_plotter import MetricCountBarPlotter&#10;from visualization.bubble_plotter import BubblePlotter&#10;from visualization.avg_dice_by_nsegments_plotter import AvgDiceByNSegmentsPlotter&#10;from visualization.box_dice_by_nsegments_plotter import BoxDiceByNSegmentsPlotter&#10;&#10;&#10;class Visualizer:&#10;    def __init__(self, base_validation_path, plotters, output_dir):&#10;        self.base_validation_path = base_validation_path&#10;        self.plotters = plotters&#10;        self.output_dir = output_dir&#10;&#10;    def visualize(self):&#10;        os.makedirs(self.output_dir, exist_ok=True)&#10;        for plotter in self.plotters:&#10;            if isinstance(plotter, (BoxPlotter, HeatmapPlotter, ScatterPlotter, MetricCountBarPlotter, BubblePlotter, AvgDiceByNSegmentsPlotter, BoxDiceByNSegmentsPlotter)):&#10;                suffix = '_all.csv'&#10;            else:&#10;                suffix = '_mean.csv'&#10;            paths = [os.path.join(self.base_validation_path, name + suffix) for name in plotter.experiments]&#10;            dfs = [read_csv(str(p)) for p in paths]&#10;            # use experiment_names as labels&#10;            plotter.plot(dfs, self.output_dir)" />
              <option name="updatedContent" value="import os&#10;&#10;from pandas import read_csv&#10;&#10;from visualization.box_plotter import BoxPlotter&#10;from visualization.heatmap_plotter import HeatmapPlotter&#10;from visualization.scatter_plotter import ScatterPlotter&#10;from visualization.metric_count_bar_plotter import MetricCountBarPlotter&#10;from visualization.bubble_plotter import BubblePlotter&#10;from visualization.avg_dice_by_nsegments_plotter import AvgDiceByNSegmentsPlotter&#10;from visualization.box_dice_by_nsegments_plotter import BoxDiceByNSegmentsPlotter&#10;&#10;&#10;class Visualizer:&#10;    def __init__(self, base_validation_path, plotters, output_dir):&#10;        self.base_validation_path = base_validation_path&#10;        self.plotters = plotters&#10;        self.output_dir = output_dir&#10;&#10;    def visualize(self):&#10;        os.makedirs(self.output_dir, exist_ok=True)&#10;        for plotter in self.plotters:&#10;            if isinstance(plotter, (BoxPlotter, HeatmapPlotter, ScatterPlotter, MetricCountBarPlotter, BubblePlotter, AvgDiceByNSegmentsPlotter, BoxDiceByNSegmentsPlotter)):&#10;                suffix = '_all.csv'&#10;            else:&#10;                suffix = '_mean.csv'&#10;            paths = [os.path.join(self.base_validation_path, name + suffix) for name in plotter.experiments]&#10;            dfs = [read_csv(str(p)) for p in paths]&#10;            # use experiment_names as labels&#10;            plotter.plot(dfs, self.output_dir)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>